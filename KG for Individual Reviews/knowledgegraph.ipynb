{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13468504,"sourceType":"datasetVersion","datasetId":8549797}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:19.770563Z","iopub.execute_input":"2025-10-22T17:19:19.771124Z","iopub.status.idle":"2025-10-22T17:19:19.779372Z","shell.execute_reply.started":"2025-10-22T17:19:19.771099Z","shell.execute_reply":"2025-10-22T17:19:19.778648Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/zstikg/NewsConcept Data-set.json\n/kaggle/input/zstikg/DVD playerData-set.json\n/kaggle/input/zstikg/MedicalConcept Data-set.json\n/kaggle/input/zstikg/Cellular phone Data-set.json\n/kaggle/input/zstikg/Digital camera2 Data-set.json\n/kaggle/input/zstikg/Mp3 playerData-set.json\n/kaggle/input/zstikg/Digital camera1 Data-set.json\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# ============================================================================\n# PART 1: INSTALLATION AND SETUP\n# ============================================================================\n\n# Install required packages\n!pip install dspy-ai huggingface_hub networkx sentence-transformers pandas --quiet\n\nprint(\"✓ Packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:19.780316Z","iopub.execute_input":"2025-10-22T17:19:19.780557Z","iopub.status.idle":"2025-10-22T17:19:23.442727Z","shell.execute_reply.started":"2025-10-22T17:19:19.780541Z","shell.execute_reply":"2025-10-22T17:19:23.441781Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"✓ Packages installed successfully!\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# ============================================================================\n# PART 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom collections import defaultdict\nimport pandas as pd\nimport os\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.444497Z","iopub.execute_input":"2025-10-22T17:19:23.444740Z","iopub.status.idle":"2025-10-22T17:19:23.449921Z","shell.execute_reply.started":"2025-10-22T17:19:23.444719Z","shell.execute_reply":"2025-10-22T17:19:23.449093Z"}},"outputs":[{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: INSTALL AND IMPORT (if not done already)\n# ============================================================================\n\nimport os\nfrom huggingface_hub import InferenceClient\nimport dspy\nimport json\nfrom typing import List, Tuple, Dict\n\nprint(\"✓ Imports complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.450786Z","iopub.execute_input":"2025-10-22T17:19:23.451023Z","iopub.status.idle":"2025-10-22T17:19:23.465558Z","shell.execute_reply.started":"2025-10-22T17:19:23.450981Z","shell.execute_reply":"2025-10-22T17:19:23.464927Z"}},"outputs":[{"name":"stdout","text":"✓ Imports complete\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: CONFIGURE HUGGING FACE WITH DSPY.LM (PROPER WAY)\n# ============================================================================\nimport os\nimport dspy\n\nos.environ['HUGGINGFACE_API_KEY'] = 'KEY'\n\n# Use DSPy's built-in LM class with the correct prefix\nlm = dspy.LM(\n    model='huggingface/meta-llama/Llama-3.1-8B-Instruct',  # Add 'huggingface/' prefix!\n    api_key=os.environ['HUGGINGFACE_API_KEY'],\n    max_tokens=2000,\n    temperature=0.3\n)\n\ndspy.settings.configure(lm=lm)\nprint(\"✓ Llama-3.1-8B-Instruct configured successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.467109Z","iopub.execute_input":"2025-10-22T17:19:23.467346Z","iopub.status.idle":"2025-10-22T17:19:23.488962Z","shell.execute_reply.started":"2025-10-22T17:19:23.467325Z","shell.execute_reply":"2025-10-22T17:19:23.488344Z"}},"outputs":[{"name":"stdout","text":"✓ Llama-3.1-8B-Instruct configured successfully!\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# ============================================================================\n# PART 1: DATA LOADING FUNCTION\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"\n    Load JSON data from file\n    \n    Expected format:\n    [\n        {\n            \"Article Title\": [],\n            \"Article Text\": \"text here...\",\n            \"Concept\": [\"concept1\", \"concept2\"]\n        },\n        ...\n    ]\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\n# Test with sample data\nsample_data = [\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"excellent phone, excellent service.\",\n        \"Concept\": []\n    },\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"i am a business user who heavily depend on mobile service.\",\n        \"Concept\": [\"service\"]\n    }\n]\n\nprint(\"✓ Sample data ready for testing\")\nprint(f\"  Sample has {len(sample_data)} documents\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.489523Z","iopub.execute_input":"2025-10-22T17:19:23.489683Z","iopub.status.idle":"2025-10-22T17:19:23.506616Z","shell.execute_reply.started":"2025-10-22T17:19:23.489670Z","shell.execute_reply":"2025-10-22T17:19:23.506035Z"}},"outputs":[{"name":"stdout","text":"✓ Sample data ready for testing\n  Sample has 2 documents\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# ============================================================================\n# FIND THE DATASET FILES\n# ============================================================================\n\nimport os\n\n# List files in your dataset directory\ndataset_path = '/kaggle/input/zstikg'\n\nprint(\"Files in the dataset:\")\nprint(\"=\" * 60)\n\ntry:\n    files = os.listdir(dataset_path)\n    for i, file in enumerate(files, 1):\n        print(f\"{i}. {file}\")\n        full_path = os.path.join(dataset_path, file)\n        print(f\"   Full path: {full_path}\")\n        print()\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    print(\"\\nTrying to list all datasets...\")\n    for dataset in os.listdir('/kaggle/input/'):\n        print(f\"- {dataset}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.507251Z","iopub.execute_input":"2025-10-22T17:19:23.507434Z","iopub.status.idle":"2025-10-22T17:19:23.528327Z","shell.execute_reply.started":"2025-10-22T17:19:23.507418Z","shell.execute_reply":"2025-10-22T17:19:23.527604Z"}},"outputs":[{"name":"stdout","text":"Files in the dataset:\n============================================================\n1. NewsConcept Data-set.json\n   Full path: /kaggle/input/zstikg/NewsConcept Data-set.json\n\n2. DVD playerData-set.json\n   Full path: /kaggle/input/zstikg/DVD playerData-set.json\n\n3. MedicalConcept Data-set.json\n   Full path: /kaggle/input/zstikg/MedicalConcept Data-set.json\n\n4. Cellular phone Data-set.json\n   Full path: /kaggle/input/zstikg/Cellular phone Data-set.json\n\n5. Digital camera2 Data-set.json\n   Full path: /kaggle/input/zstikg/Digital camera2 Data-set.json\n\n6. Mp3 playerData-set.json\n   Full path: /kaggle/input/zstikg/Mp3 playerData-set.json\n\n7. Digital camera1 Data-set.json\n   Full path: /kaggle/input/zstikg/Digital camera1 Data-set.json\n\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# ============================================================================\n# LOAD ONE SPECIFIC DATASET\n# ============================================================================\n\n# Choose which dataset you want to work with:\n# Option 1: Cellular phone\nfilepath = '/kaggle/input/zstikg/MedicalConcept Data-set.json'\n\n# Option 2: News\n# filepath = '/kaggle/input/zsltikg/NewsConcept Data-set.json'\n\n# Option 3: Medical\n# filepath = '/kaggle/input/zsltikg/MedicalConcept Data-set.json'\n\n# Load the data\ndata = load_json_data(filepath)\n\nprint(f\"✓ Loaded {len(data)} documents\")\nprint(f\"\\nFirst document preview:\")\nprint(f\"  Keys: {list(data[0].keys())}\")\nprint(f\"  Text: {data[0].get('Article Text', '')[:100]}...\")\nprint(f\"  Concepts: {data[0].get('Concept', [])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.529131Z","iopub.execute_input":"2025-10-22T17:19:23.529581Z","iopub.status.idle":"2025-10-22T17:19:23.592887Z","shell.execute_reply.started":"2025-10-22T17:19:23.529559Z","shell.execute_reply":"2025-10-22T17:19:23.592370Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 2066 documents from /kaggle/input/zstikg/MedicalConcept Data-set.json\n✓ Loaded 2066 documents\n\nFirst document preview:\n  Keys: ['Article Title', 'Article Text', 'Concept']\n  Text: 1. Health care reform\n\nHow could the health care reform legislation that President Barack Obama sign...\n  Concepts: []\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"# ============================================================================\n# RELOAD DATA PROPERLY\n# ============================================================================\n\nimport json\n\nprint(\"=\" * 80)\nprint(\"RELOADING DATA PROPERLY\")\nprint(\"=\" * 80)\n\n# Load the JSON file correctly\nwith open('/kaggle/input/zstikg/MedicalConcept Data-set.json', 'r') as f:\n    raw_data = json.load(f)\n\nprint(f\"Type after loading: {type(raw_data)}\")\n\n# Check if it's a dict with a key\nif isinstance(raw_data, dict):\n    print(f\"Keys: {raw_data.keys()}\")\n    # Get the actual data\n    for key in raw_data.keys():\n        if isinstance(raw_data[key], list):\n            data = raw_data[key]\n            print(f\"Found list under key '{key}' with {len(data)} items\")\n            break\nelif isinstance(raw_data, list):\n    data = raw_data\n    print(f\"Data is a list with {len(data)} items\")\n\n# Verify first item\nprint(f\"\\nFirst item type: {type(data[0])}\")\n#print(f\"First item: {data[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.593558Z","iopub.execute_input":"2025-10-22T17:19:23.593793Z","iopub.status.idle":"2025-10-22T17:19:23.644110Z","shell.execute_reply.started":"2025-10-22T17:19:23.593772Z","shell.execute_reply":"2025-10-22T17:19:23.643549Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nRELOADING DATA PROPERLY\n================================================================================\nType after loading: <class 'list'>\nData is a list with 2066 items\n\nFirst item type: <class 'dict'>\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# ============================================================================\n# PROCESS YOUR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"PROCESSING ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\n# data is now correctly loaded as a list of dicts\nindividual_articles = []\n\nfor idx, item in enumerate(data):\n    article = {\n        'id': idx,\n        'Article Text': item['Article Text'],\n        'Concept': item['Concept'] if item['Concept'] else []\n    }\n    individual_articles.append(article)\n\nprint(f\"\\n✓ Created list of {len(individual_articles)} individual articles\")\n\n# Show first 3\nprint(f\"\\nFirst 3 articles:\")\nprint(\"-\" * 80)\nfor i in range(min(3, len(individual_articles))):\n    article = individual_articles[i]\n    print(f\"\\nArticle {article['id']}:\")\n    print(f\"  Text: {article['Article Text'][:80]}...\")\n    print(f\"  Concepts: {article['Concept']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.644789Z","iopub.execute_input":"2025-10-22T17:19:23.645081Z","iopub.status.idle":"2025-10-22T17:19:23.653545Z","shell.execute_reply.started":"2025-10-22T17:19:23.645065Z","shell.execute_reply":"2025-10-22T17:19:23.652914Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPROCESSING ALL THE ARTICLES\n================================================================================\n\n✓ Created list of 2066 individual articles\n\nFirst 3 articles:\n--------------------------------------------------------------------------------\n\nArticle 0:\n  Text: 1. Health care reform\n\nHow could the health care reform legislation that Preside...\n  Concepts: []\n\nArticle 1:\n  Text: My colleagues at Harvard Health Publishing and I have a mission: to provide accu...\n  Concepts: ['Heart Health', \"Women's Health\"]\n\nArticle 2:\n  Text: Does echinacea, the popular natural cold remedy, really work?\n\nIt depends on wha...\n  Concepts: []\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: DEFINE SIGNATURES\n# ============================================================================\n\nclass EntityExtractor(dspy.Signature):\n    \"\"\"Extract key entities from the given text. Extracted entities are nouns, \n    verbs, or adjectives, particularly regarding sentiment. This is for an \n    extraction task, please be thorough and accurate to the reference text.\n    \n    Return ONLY a valid JSON list format: [\"entity1\", \"entity2\", \"entity3\"]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract entities from\")\n    entities = dspy.OutputField(desc=\"List of extracted entities in JSON format\")\n\nclass RelationExtractor(dspy.Signature):\n    \"\"\"Extract subject-predicate-object triples from the assistant message. \n    A predicate (1-3 words) defines the relationship between the subject and \n    object. Relationship may be fact or sentiment based on assistant's message. \n    Subject and object are entities. Entities provided are from the assistant \n    message and prior conversation history, though you may not need all of them. \n    This is for an extraction task, please be thorough, accurate, and faithful \n    to the reference text.\n    \n    Return ONLY valid JSON format: [[\"subject1\", \"predicate1\", \"object1\"], [\"subject2\", \"predicate2\", \"object2\"]]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract relations from\")\n    entities = dspy.InputField(desc=\"List of available entities\")\n    triples = dspy.OutputField(desc=\"List of [subject, predicate, object] triples in JSON format\")\n\nprint(\"✓ Signatures defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.655420Z","iopub.execute_input":"2025-10-22T17:19:23.655609Z","iopub.status.idle":"2025-10-22T17:19:23.677299Z","shell.execute_reply.started":"2025-10-22T17:19:23.655595Z","shell.execute_reply":"2025-10-22T17:19:23.676726Z"}},"outputs":[{"name":"stdout","text":"✓ Signatures defined\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: CREATE ENTITY EXTRACTOR\n# ============================================================================\n\nclass ExtractEntities(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(EntityExtractor)\n    \n    def forward(self, text: str) -> List[str]:\n        if not text or len(text.strip()) < 3:\n            return []\n            \n        result = self.extract(text=text)\n        \n        try:\n            entities_text = result.entities.strip()\n            \n            if '[' in entities_text and ']' in entities_text:\n                start = entities_text.find('[')\n                end = entities_text.rfind(']') + 1\n                entities_text = entities_text[start:end]\n            \n            entities = json.loads(entities_text)\n            \n            if isinstance(entities, list):\n                return [str(e).lower().strip() for e in entities if e and len(str(e).strip()) > 1]\n            return []\n            \n        except:\n            try:\n                entities_text = result.entities.strip()\n                if entities_text.startswith('['):\n                    entities_text = entities_text[1:]\n                if entities_text.endswith(']'):\n                    entities_text = entities_text[:-1]\n                \n                entities = []\n                for item in entities_text.split(','):\n                    item = item.strip(' \"\\'\\n\\t')\n                    if item and len(item) > 1:\n                        entities.append(item.lower())\n                \n                return entities[:50]\n            except:\n                return []\n\nentity_extractor = ExtractEntities()\nprint(\"✓ Entity Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.678070Z","iopub.execute_input":"2025-10-22T17:19:23.678377Z","iopub.status.idle":"2025-10-22T17:19:23.703068Z","shell.execute_reply.started":"2025-10-22T17:19:23.678356Z","shell.execute_reply":"2025-10-22T17:19:23.702399Z"}},"outputs":[{"name":"stdout","text":"✓ Entity Extractor created\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: CREATE RELATION EXTRACTOR\n# ============================================================================\n\nclass ExtractRelations(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(RelationExtractor)\n    \n    def forward(self, text: str, entities: List[str]) -> List[Tuple[str, str, str]]:\n        if not entities or not text:\n            return []\n        \n        entities_subset = entities[:30]\n        entities_str = json.dumps(entities_subset)\n        \n        result = self.extract(text=text, entities=entities_str)\n        \n        try:\n            triples_text = result.triples.strip()\n            \n            if '[' in triples_text and ']' in triples_text:\n                start = triples_text.find('[')\n                end = triples_text.rfind(']') + 1\n                triples_text = triples_text[start:end]\n            \n            triples = json.loads(triples_text)\n            \n            normalized_triples = []\n            for triple in triples:\n                if isinstance(triple, (list, tuple)) and len(triple) == 3:\n                    s, p, o = triple\n                    s = str(s).lower().strip()\n                    p = str(p).lower().strip()\n                    o = str(o).lower().strip()\n                    \n                    if s and p and o and s != o:\n                        normalized_triples.append((s, p, o))\n            \n            return normalized_triples[:100]\n            \n        except Exception as e:\n            return []\n\nrelation_extractor = ExtractRelations()\nprint(\"✓ Relation Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.703836Z","iopub.execute_input":"2025-10-22T17:19:23.704331Z","iopub.status.idle":"2025-10-22T17:19:23.724596Z","shell.execute_reply.started":"2025-10-22T17:19:23.704314Z","shell.execute_reply":"2025-10-22T17:19:23.724089Z"}},"outputs":[{"name":"stdout","text":"✓ Relation Extractor created\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: TEST EXTRACTION\n# ============================================================================\n\nprint(\"\\nTesting Entity & Relation Extraction...\")\nprint(\"=\" * 80)\n\n# Test on first 2 documents\nfor i in range(min(2, len(data))):\n    text = data[i].get('Article Text', '')\n    \n    print(f\"\\nDocument {i+1}:\")\n    print(f\"  Text: {text[:80]}...\")\n    \n    # Extract entities\n    print(f\"  Extracting entities...\")\n    entities = entity_extractor(text)\n    print(f\"  ✓ Found {len(entities)} entities: {entities[:5]}...\")\n    \n    # Extract relations\n    if entities:\n        print(f\"  Extracting relations...\")\n        relations = relation_extractor(text, entities)\n        print(f\"  ✓ Found {len(relations)} relations\")\n        \n        if relations:\n            for j, (s, p, o) in enumerate(relations[:3], 1):\n                print(f\"    {j}. ({s}) --[{p}]--> ({o})\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ Test complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:23.725251Z","iopub.execute_input":"2025-10-22T17:19:23.725400Z","iopub.status.idle":"2025-10-22T17:19:24.565878Z","shell.execute_reply.started":"2025-10-22T17:19:23.725389Z","shell.execute_reply":"2025-10-22T17:19:24.565321Z"}},"outputs":[{"name":"stdout","text":"\nTesting Entity & Relation Extraction...\n================================================================================\n\nDocument 1:\n  Text: 1. Health care reform\n\nHow could the health care reform legislation that Preside...\n  Extracting entities...\n  ✓ Found 36 entities: ['health care reform', 'president barack obama', 'patient protection and affordable care act', 'medicaid', 'health insurance']...\n  Extracting relations...\n  ✓ Found 13 relations\n    1. (president barack obama) --[signed into law]--> (patient protection and affordable care act)\n    2. (patient protection and affordable care act) --[designed to]--> (patch holes in the health insurance system)\n    3. (patient protection and affordable care act) --[extend coverage to]--> (32 million americans)\n\nDocument 2:\n  Text: My colleagues at Harvard Health Publishing and I have a mission: to provide accu...\n  Extracting entities...\n  ✓ Found 7 entities: ['harvard health publishing', \"harvard women's health watch\", 'harvard heart letter', \"st. joseph's hospital\", 'tim russert']...\n  Extracting relations...\n  ✓ Found 12 relations\n    1. (harvard health publishing) --[provides]--> (accurate and reliable information)\n    2. (harvard health publishing) --[helps readers]--> (live healthier lives)\n    3. (harvard women's health watch) --[saved]--> (a woman's life)\n\n================================================================================\n✓ Test complete!\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# ============================================================================\n# CLUSTERING SIGNATURES (DEFINE FIRST!)\n# ============================================================================\n\nclass ClusterValidator(dspy.Signature):\n    \"\"\"Verify if these entities belong in the same cluster.\n    A cluster should contain entities that are the same in meaning, with different:\n    - tenses, plural forms, stem forms, upper/lower cases\n    Or entities with close semantic meanings.\n    \n    Return ONLY valid JSON format: [\"entity1\", \"entity2\", \"entity3\"]\n    Return only entities you are confident belong together.\n    If not confident, return empty list [].\n    \"\"\"\n    \n    entities = dspy.InputField(desc=\"Entities to validate\")\n    valid_cluster = dspy.OutputField(desc=\"Validated cluster in JSON format\")\n\nprint(\"✓ ClusterValidator Signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:24.566565Z","iopub.execute_input":"2025-10-22T17:19:24.566802Z","iopub.status.idle":"2025-10-22T17:19:24.572165Z","shell.execute_reply.started":"2025-10-22T17:19:24.566780Z","shell.execute_reply":"2025-10-22T17:19:24.571495Z"}},"outputs":[{"name":"stdout","text":"✓ ClusterValidator Signature defined\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# ============================================================================\n# SEMANTIC SIMILARITY CLUSTERING (FROM PAPER)\n# ============================================================================\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nclass SemanticEntityClustering(dspy.Module):\n    def __init__(self, similarity_threshold=0.75):\n        super().__init__()\n        self.validator = dspy.ChainOfThought(ClusterValidator)\n        \n        # Load embedding model (same as paper)\n        print(\"Loading sentence transformer model...\")\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        print(\"✓ Model loaded\")\n        \n        self.similarity_threshold = similarity_threshold\n    \n    def _parse_cluster(self, text: str) -> List[str]:\n        \"\"\"Parse cluster from LLM response\"\"\"\n        try:\n            text = text.strip()\n            if '[' in text and ']' in text:\n                start = text.find('[')\n                end = text.rfind(']') + 1\n                text = text[start:end]\n            \n            cluster = json.loads(text)\n            if isinstance(cluster, list):\n                return [str(e).lower().strip() for e in cluster if e]\n            return []\n        except:\n            return []\n    \n    def _get_semantic_clusters(self, entities: List[str]) -> List[List[str]]:\n        \"\"\"Group entities by semantic similarity using embeddings\"\"\"\n        \n        if len(entities) == 0:\n            return []\n        \n        # Get embeddings for all entities\n        embeddings = self.model.encode(entities)\n        \n        # Compute pairwise cosine similarity\n        similarity_matrix = cosine_similarity(embeddings)\n        \n        # Find clusters using similarity threshold\n        clusters = []\n        remaining = set(range(len(entities)))\n        \n        for i in range(len(entities)):\n            if i not in remaining:\n                continue\n            \n            # Find all entities similar to this one\n            cluster_indices = [i]\n            remaining.discard(i)\n            \n            for j in range(i + 1, len(entities)):\n                if j not in remaining:\n                    continue\n                \n                # Check if similar enough\n                if similarity_matrix[i][j] >= self.similarity_threshold:\n                    cluster_indices.append(j)\n                    remaining.discard(j)\n            \n            # Convert indices to entity names\n            cluster = [entities[idx] for idx in cluster_indices]\n            \n            # Only keep clusters with 2-4 entities\n            if 2 <= len(cluster) <= 4:\n                clusters.append(cluster)\n            elif len(cluster) == 1:\n                # Keep singletons for later\n                pass\n        \n        return clusters\n    \n    def forward(self, entities: List[str]) -> Dict[str, List[str]]:\n        \"\"\"Semantic clustering with LLM validation\"\"\"\n        \n        print(f\"Starting semantic clustering with {len(entities)} entities...\")\n        print(f\"  Similarity threshold: {self.similarity_threshold}\")\n        \n        # Remove duplicates\n        unique_entities = list(set(entities))\n        \n        # Step 1: Find semantic clusters using embeddings\n        print(\"  Computing semantic similarities...\")\n        potential_clusters = self._get_semantic_clusters(unique_entities)\n        \n        print(f\"  Found {len(potential_clusters)} potential clusters\")\n        \n        # Step 2: Validate with LLM\n        validated_clusters = {}\n        cluster_id = 0\n        clustered_entities = set()\n        \n        for cluster in potential_clusters:\n            try:\n                # Ask LLM to validate\n                validation = self.validator(entities=json.dumps(cluster))\n                validated = self._parse_cluster(validation.valid_cluster)\n                \n                if validated and len(validated) >= 2:\n                    cluster_label = validated[0]\n                    validated_clusters[cluster_label] = validated\n                    \n                    for entity in validated:\n                        clustered_entities.add(entity)\n                    \n                    print(f\"  ✓ Cluster {cluster_id}: {validated}\")\n                    cluster_id += 1\n                else:\n                    # LLM rejected - add as singletons\n                    for entity in cluster:\n                        if entity not in clustered_entities:\n                            validated_clusters[entity] = [entity]\n                            clustered_entities.add(entity)\n            except:\n                # Error - add as singletons\n                for entity in cluster:\n                    if entity not in clustered_entities:\n                        validated_clusters[entity] = [entity]\n                        clustered_entities.add(entity)\n        \n        # Step 3: Add all remaining entities as singletons\n        for entity in unique_entities:\n            if entity not in clustered_entities:\n                validated_clusters[entity] = [entity]\n        \n        multi = sum(1 for v in validated_clusters.values() if len(v) > 1)\n        print(f\"✓ Semantic clustering complete: {len(validated_clusters)} total clusters\")\n        print(f\"  Multi-entity clusters: {multi}\")\n        print(f\"  Singleton entities: {len(validated_clusters) - multi}\")\n        \n        return validated_clusters\n\n# Create semantic clusterer with different thresholds\nentity_clusterer_semantic = SemanticEntityClustering(similarity_threshold=0.75)\nprint(\"\\n✓ Semantic Entity Clustering Module created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:24.572807Z","iopub.execute_input":"2025-10-22T17:19:24.573025Z","iopub.status.idle":"2025-10-22T17:19:26.216562Z","shell.execute_reply.started":"2025-10-22T17:19:24.572977Z","shell.execute_reply":"2025-10-22T17:19:26.215869Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n✓ Model loaded\n\n✓ Semantic Entity Clustering Module created\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"# ============================================================================\n# TEST SEMANTIC CLUSTERING\n# ============================================================================\n\nprint(\"\\nTesting SEMANTIC Clustering (Embeddings + LLM)...\")\nprint(\"=\" * 80)\n\ntest_entities = [\n    # Should cluster (same concept)\n    'phone', 'mobile phone', 'cellphone', 'cell phone',\n    \n    # Should cluster (same concept)\n    'usa', 'united states', 'america',\n    \n    # Should cluster (similar meaning)\n    'helping hand', 'assistant', 'helper', 'support',\n    \n    # Should cluster (same concept)\n    'service', 'services',\n    \n    # Should cluster (same concept)\n    'call', 'calling', 'calls',\n    \n    # Should NOT cluster (different brands)\n    'nokia', 'motorola', 'samsung',\n    \n    # Should NOT cluster (different concepts)\n    'signal', 'battery', 'screen',\n]\n\nprint(f\"Testing with {len(test_entities)} entities:\")\nfor e in test_entities:\n    print(f\"  - {e}\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Cluster with semantic method\nsemantic_clusters = entity_clusterer_semantic(test_entities)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SEMANTIC CLUSTERING RESULTS:\")\nprint(\"=\" * 80)\n\n# Show multi-entity clusters\nmulti_clusters = {k: v for k, v in semantic_clusters.items() if len(v) > 1}\n\nif multi_clusters:\n    print(f\"\\n✓ Found {len(multi_clusters)} semantic clusters:\")\n    for i, (label, members) in enumerate(multi_clusters.items(), 1):\n        print(f\"  {i}. {members}\")\n        \n        # Show similarity scores\n        if len(members) > 1:\n            embeddings = entity_clusterer_semantic.model.encode(members)\n            similarities = cosine_similarity(embeddings)\n            avg_sim = (similarities.sum() - len(members)) / (len(members) * (len(members) - 1))\n            print(f\"     Average similarity: {avg_sim:.3f}\")\nelse:\n    print(\"\\n  No clusters found\")\n\nprint(f\"\\nSingleton entities: {len(semantic_clusters) - len(multi_clusters)}\")\n\nprint(\"\\n✓ Test complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:26.217353Z","iopub.execute_input":"2025-10-22T17:19:26.217599Z","iopub.status.idle":"2025-10-22T17:19:35.936395Z","shell.execute_reply.started":"2025-10-22T17:19:26.217582Z","shell.execute_reply":"2025-10-22T17:19:35.935792Z"}},"outputs":[{"name":"stdout","text":"\nTesting SEMANTIC Clustering (Embeddings + LLM)...\n================================================================================\nTesting with 22 entities:\n  - phone\n  - mobile phone\n  - cellphone\n  - cell phone\n  - usa\n  - united states\n  - america\n  - helping hand\n  - assistant\n  - helper\n  - support\n  - service\n  - services\n  - call\n  - calling\n  - calls\n  - nokia\n  - motorola\n  - samsung\n  - signal\n  - battery\n  - screen\n\n================================================================================\nStarting semantic clustering with 22 entities...\n  Similarity threshold: 0.75\n  Computing semantic similarities...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0f25dfefbb643ed8e4728931df7a265"}},"metadata":{}},{"name":"stdout","text":"  Found 4 potential clusters\n  ✓ Cluster 0: ['phone', 'cell phone', 'mobile phone', 'cellphone']\n  ✓ Cluster 1: ['call', 'call', 'calls']\n  ✓ Cluster 2: ['america', 'united states', 'usa']\n  ✓ Cluster 3: ['service', 'services']\n✓ Semantic clustering complete: 15 total clusters\n  Multi-entity clusters: 4\n  Singleton entities: 11\n\n================================================================================\nSEMANTIC CLUSTERING RESULTS:\n================================================================================\n\n✓ Found 4 semantic clusters:\n  1. ['phone', 'cell phone', 'mobile phone', 'cellphone']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"282bed444560417da5b2870740e7cdd4"}},"metadata":{}},{"name":"stdout","text":"     Average similarity: 0.860\n  2. ['call', 'call', 'calls']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3b23ceb22614973b205aa5eafd317e8"}},"metadata":{}},{"name":"stdout","text":"     Average similarity: 0.894\n  3. ['america', 'united states', 'usa']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b623e775bbb4bcd86bb4451e11322c1"}},"metadata":{}},{"name":"stdout","text":"     Average similarity: 0.844\n  4. ['service', 'services']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"859bc28c5e8d4ec2923db5074bb89275"}},"metadata":{}},{"name":"stdout","text":"     Average similarity: 0.921\n\nSingleton entities: 11\n\n✓ Test complete!\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# ============================================================================\n# VISUALIZE HOW CLUSTERING WORKS\n# ============================================================================\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Test entities\ntest_entities = ['phone', 'mobile phone', 'cellphone', 'nokia', 'motorola']\n\n# Get embeddings\nembeddings = entity_clusterer_semantic.model.encode(test_entities)\n\n# Calculate similarities\nprint(\"Similarity Matrix:\")\nprint(\"=\" * 80)\nprint(f\"{'':15s}\", end='')\nfor e in test_entities:\n    print(f\"{e:15s}\", end='')\nprint()\n\nfor i, entity1 in enumerate(test_entities):\n    print(f\"{entity1:15s}\", end='')\n    for j, entity2 in enumerate(test_entities):\n        similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n        print(f\"{similarity:15.3f}\", end='')\n    print()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"Threshold: 0.75\")\nprint(\"=\" * 80)\n\n# Show which pairs would cluster\nprint(\"\\nPairs above threshold (would cluster together):\")\nfor i, entity1 in enumerate(test_entities):\n    for j, entity2 in enumerate(test_entities):\n        if i < j:  # Avoid duplicates\n            similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n            if similarity >= 0.75:\n                print(f\"  ✓ {entity1:15s} ↔ {entity2:15s} = {similarity:.3f}\")\n\nprint(\"\\nPairs below threshold (stay separate):\")\nfor i, entity1 in enumerate(test_entities):\n    for j, entity2 in enumerate(test_entities):\n        if i < j:\n            similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n            if similarity < 0.75:\n                print(f\"  ✗ {entity1:15s} ↔ {entity2:15s} = {similarity:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:53.008869Z","iopub.execute_input":"2025-10-22T17:19:53.009408Z","iopub.status.idle":"2025-10-22T17:19:53.058939Z","shell.execute_reply.started":"2025-10-22T17:19:53.009380Z","shell.execute_reply":"2025-10-22T17:19:53.058175Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62e618e56e9a411a9261bde9f07b793f"}},"metadata":{}},{"name":"stdout","text":"Similarity Matrix:\n================================================================================\n               phone          mobile phone   cellphone      nokia          motorola       \nphone                    1.000          0.814          0.845          0.681          0.644\nmobile phone             0.814          1.000          0.843          0.751          0.688\ncellphone                0.845          0.843          1.000          0.674          0.617\nnokia                    0.681          0.751          0.674          1.000          0.640\nmotorola                 0.644          0.688          0.617          0.640          1.000\n\n================================================================================\nThreshold: 0.75\n================================================================================\n\nPairs above threshold (would cluster together):\n  ✓ phone           ↔ mobile phone    = 0.814\n  ✓ phone           ↔ cellphone       = 0.845\n  ✓ mobile phone    ↔ cellphone       = 0.843\n  ✓ mobile phone    ↔ nokia           = 0.751\n\nPairs below threshold (stay separate):\n  ✗ phone           ↔ nokia           = 0.681\n  ✗ phone           ↔ motorola        = 0.644\n  ✗ mobile phone    ↔ motorola        = 0.688\n  ✗ cellphone       ↔ nokia           = 0.674\n  ✗ cellphone       ↔ motorola        = 0.617\n  ✗ nokia           ↔ motorola        = 0.640\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"# ============================================================================\n# FINAL KGGEN WITH SEMANTIC CLUSTERING\n# ============================================================================\n\nclass KGGenSemantic:\n    def __init__(self, similarity_threshold=0.75):\n        self.entity_extractor = entity_extractor\n        self.relation_extractor = relation_extractor\n        self.entity_clusterer = SemanticEntityClustering(similarity_threshold=similarity_threshold)\n        self.graph = nx.DiGraph()\n        self.entity_clusters = {}\n        \n    def generate_from_json(self, json_data: List[Dict], max_docs: int = None) -> nx.DiGraph:\n        \"\"\"Generate KG from JSON dataset\"\"\"\n        all_entities = set()\n        all_relations = []\n        \n        if max_docs:\n            json_data = json_data[:max_docs]\n        \n        print(f\"Processing {len(json_data)} documents...\")\n        print(\"=\" * 80)\n        \n        for idx, item in enumerate(json_data):\n            text = item.get('Article Text', '')\n            concepts = item.get('Concept', [])\n            \n            if not text or len(text.strip()) < 5:\n                continue\n            \n            try:\n                # Extract entities\n                entities = self.entity_extractor(text)\n                all_entities.update(entities)\n                \n                # Add concepts\n                for concept in concepts:\n                    if concept and isinstance(concept, str):\n                        all_entities.add(concept.lower().strip())\n                \n                # Extract relations\n                relations = self.relation_extractor(text, list(all_entities))\n                all_relations.extend(relations)\n                \n                if (idx + 1) % 20 == 0:\n                    print(f\"  {idx + 1}/{len(json_data)} docs | {len(all_entities)} entities | {len(all_relations)} relations\")\n                    \n            except Exception as e:\n                continue\n        \n        print(f\"\\n✓ Extraction complete!\")\n        print(f\"  Total entities: {len(all_entities)}\")\n        print(f\"  Total relations: {len(all_relations)}\")\n        \n        # Build graph\n        for subj, pred, obj in all_relations:\n            self.graph.add_edge(subj, obj, relation=pred)\n        \n        print(f\"  Graph nodes: {len(self.graph.nodes())}\")\n        print(f\"  Graph edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def cluster_entities(self):\n        \"\"\"Semantic clustering with embeddings\"\"\"\n        nodes = list(self.graph.nodes())\n        \n        if len(nodes) == 0:\n            print(\"No nodes to cluster!\")\n            return self.graph\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"SEMANTIC CLUSTERING: {len(nodes)} ENTITIES\")\n        print(f\"{'='*80}\")\n        \n        self.entity_clusters = self.entity_clusterer(nodes)\n        \n        # Map entities\n        entity_mapping = {}\n        for cluster_label, cluster_entities in self.entity_clusters.items():\n            for entity in cluster_entities:\n                entity_mapping[entity] = cluster_label\n        \n        # Rebuild graph\n        new_graph = nx.DiGraph()\n        for u, v, data in self.graph.edges(data=True):\n            new_u = entity_mapping.get(u, u)\n            new_v = entity_mapping.get(v, v)\n            relation = data.get('relation', 'related_to')\n            \n            if new_u == new_v:\n                continue\n            \n            if not new_graph.has_edge(new_u, new_v):\n                new_graph.add_edge(new_u, new_v, relation=relation)\n        \n        self.graph = new_graph\n        \n        print(f\"\\n✓ Clustering complete!\")\n        print(f\"  Final nodes: {len(self.graph.nodes())}\")\n        print(f\"  Final edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def save_graph(self, filepath: str):\n        data = nx.node_link_data(self.graph)\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n        print(f\"✓ Saved to {filepath}\")\n    \n    def export_triples(self, filepath: str):\n        triples = []\n        for u, v, data in self.graph.edges(data=True):\n            triples.append({\n                'subject': u,\n                'predicate': data.get('relation', 'related_to'),\n                'object': v\n            })\n        import pandas as pd\n        df = pd.DataFrame(triples)\n        df.to_csv(filepath, index=False)\n        print(f\"✓ Exported to {filepath}\")\n\n# Initialize semantic KGGen (threshold 0.75 = balanced)\nkg_gen_semantic = KGGenSemantic(similarity_threshold=0.75)\nprint(\"\\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:19:57.103707Z","iopub.execute_input":"2025-10-22T17:19:57.104436Z","iopub.status.idle":"2025-10-22T17:19:58.023858Z","shell.execute_reply.started":"2025-10-22T17:19:57.104412Z","shell.execute_reply":"2025-10-22T17:19:58.023302Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n✓ Model loaded\n\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# GENERATE KG FOR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\nall_article_kgs = []\n\ntotal = len(individual_articles)\nprint(f\"\\nProcessing {total} articles...\\n\")\n\nfor idx, article in enumerate(individual_articles):\n    article_id = article['id']\n    text = article['Article Text']\n    concepts = article['Concept']\n    \n    # Skip empty articles\n    if not text or len(text.strip()) < 5:\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n        continue\n    \n    try:\n        # Extract entities for THIS article\n        entities = entity_extractor(text)\n        \n        # Add original concepts as entities\n        for concept in concepts:\n            if concept and isinstance(concept, str):\n                entities.append(concept.lower().strip())\n        \n        entities = list(set(entities))  # Remove duplicates\n        \n        # Extract relations for THIS article\n        if entities:\n            relations = relation_extractor(text, entities)\n        else:\n            relations = []\n        \n        # Build graph for THIS article\n        graph = nx.DiGraph()\n        for subj, pred, obj in relations:\n            graph.add_edge(subj, obj, relation=pred)\n        \n        # Store everything for this article\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            'concepts': concepts,\n            'graph': graph,\n            'entities': entities,\n            'relations': relations,\n            'num_nodes': len(graph.nodes()),\n            'num_edges': len(graph.edges())\n        })\n        \n        # Progress update\n        if (idx + 1) % 50 == 0:\n            print(f\"✓ Processed {idx + 1}/{total} articles...\")\n        \n    except Exception as e:\n        print(f\"✗ Article {article_id}: Error - {str(e)[:100]}\")\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n\nprint(f\"\\n\" + \"=\" * 80)\nprint(\"✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\")\nprint(\"=\" * 80)\nprint(f\"Total articles processed: {len(all_article_kgs)}\")\nprint(f\"Articles with graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] > 0)}\")\nprint(f\"Articles without graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] == 0)}\")\n\n# Statistics\ntotal_entities = sum(len(kg['entities']) for kg in all_article_kgs)\ntotal_relations = sum(len(kg['relations']) for kg in all_article_kgs)\n\nprint(f\"\\nTotal entities extracted: {total_entities}\")\nprint(f\"Total relations extracted: {total_relations}\")\n\nwith_graphs = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\nif with_graphs:\n    avg_nodes = sum(kg['num_nodes'] for kg in with_graphs) / len(with_graphs)\n    avg_edges = sum(kg['num_edges'] for kg in with_graphs) / len(with_graphs)\n    print(f\"\\nAverage nodes per KG: {avg_nodes:.2f}\")\n    print(f\"Average edges per KG: {avg_edges:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:22:47.663433Z","iopub.execute_input":"2025-10-22T17:22:47.663692Z","iopub.status.idle":"2025-10-22T17:47:48.441785Z","shell.execute_reply.started":"2025-10-22T17:22:47.663672Z","shell.execute_reply":"2025-10-22T17:47:48.440464Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nGENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\n================================================================================\n\nProcessing 2066 articles...\n\n✓ Processed 50/2066 articles...\n","output_type":"stream"},{"name":"stderr","text":"2025/10/22 17:31:48 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=2000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.3)  if the reason for truncation is repetition.\n","output_type":"stream"},{"name":"stdout","text":"✓ Processed 100/2066 articles...\n✓ Processed 150/2066 articles...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/459740462.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Extract entities for THIS article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Add original concepts as entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/utils/callback.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_active_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mcall_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/primitives/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/primitives/module.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"forward\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# Check if forward is called through __call__ or directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0mforward_called_directly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m     \u001b[0;34m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetouterframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1724\u001b[0m     \u001b[0mframelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1726\u001b[0;31m         \u001b[0mtraceback_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetframeinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1727\u001b[0m         \u001b[0mframeinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraceback_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0mframelist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrameInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mframeinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraceback_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source code not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    986\u001b[0m     \u001b[0;31m# Copy sys.modules in order to cope with changes while iterating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mismodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_filesbymodname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mismodule\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;31m# ----------------------------------------------------------- type-checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mismodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \"\"\"Return true if the object is a module.\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":70},{"cell_type":"code","source":"# ============================================================================\n# CHECK WHAT'S BEEN SUCCESSFULLY PROCESSED\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"CHECKING SUCCESSFUL PROCESSING\")\nprint(\"=\" * 80)\n\n# Count successes\nsuccessful = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\nfailed = [kg for kg in all_article_kgs if kg['num_edges'] == 0]\n\nprint(f\"\\nTotal articles: {len(all_article_kgs)}\")\nprint(f\"✓ Successfully processed: {len(successful)}\")\nprint(f\"✗ Failed/Empty: {len(failed)}\")\n\nif successful:\n    print(f\"\\nSuccessful articles with KGs:\")\n    for kg in successful[:10]:\n        print(f\"  Article {kg['id']}: {kg['num_nodes']} nodes, {kg['num_edges']} edges\")\n    \n    # Export the successful ones\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXPORTING SUCCESSFUL RESULTS\")\n    print(\"=\" * 80)\n    \n    import json\n    import os\n    \n    # Create output directory\n    os.makedirs('/kaggle/working/individual_kgs', exist_ok=True)\n    \n    # Export successful KGs\n    for kg in successful:\n        article_id = kg['id']\n        \n        graph_data = {\n            'article_id': article_id,\n            'text': kg['text'],\n            'concepts': kg['concepts'],\n            'entities': kg['entities'],\n            'relations': [\n                {'subject': s, 'predicate': p, 'object': o}\n                for s, p, o in kg['relations']\n            ],\n            'graph': nx.node_link_data(kg['graph'])\n        }\n        \n        filepath = f'/kaggle/working/individual_kgs/article_{article_id}.json'\n        with open(filepath, 'w') as f:\n            json.dump(graph_data, f, indent=2)\n    \n    print(f\"✓ Saved {len(successful)} JSON files\")\n    \n    # Export triples\n    all_triples = []\n    for kg in successful:\n        for subj, pred, obj in kg['relations']:\n            all_triples.append({\n                'article_id': kg['id'],\n                'subject': subj,\n                'predicate': pred,\n                'object': obj\n            })\n    \n    triples_df = pd.DataFrame(all_triples)\n    triples_df.to_csv('/kaggle/working/successful_triples.csv', index=False)\n    print(f\"✓ Saved {len(all_triples)} triples to successful_triples.csv\")\n    \n    # Summary\n    summary = []\n    for kg in all_article_kgs:\n        summary.append({\n            'article_id': kg['id'],\n            'text_preview': kg['text'][:100],\n            'num_entities': len(kg['entities']),\n            'num_relations': len(kg['relations']),\n            'num_nodes': kg['num_nodes'],\n            'num_edges': kg['num_edges'],\n            'status': 'success' if kg['num_edges'] > 0 else 'failed'\n        })\n    \n    summary_df = pd.DataFrame(summary)\n    summary_df.to_csv('/kaggle/working/processing_summary.csv', index=False)\n    print(f\"✓ Saved summary\")\n    \n    print(\"\\n✓ DONE! Downloaded what we could process.\")\nelse:\n    print(\"\\n✗ No successful KGs to export\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:47:59.266405Z","iopub.execute_input":"2025-10-22T17:47:59.266682Z","iopub.status.idle":"2025-10-22T17:47:59.359805Z","shell.execute_reply.started":"2025-10-22T17:47:59.266659Z","shell.execute_reply":"2025-10-22T17:47:59.359151Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCHECKING SUCCESSFUL PROCESSING\n================================================================================\n\nTotal articles: 151\n✓ Successfully processed: 149\n✗ Failed/Empty: 2\n\nSuccessful articles with KGs:\n  Article 0: 35 nodes, 23 edges\n  Article 1: 22 nodes, 15 edges\n  Article 2: 11 nodes, 9 edges\n  Article 3: 31 nodes, 18 edges\n  Article 4: 26 nodes, 21 edges\n  Article 5: 23 nodes, 16 edges\n  Article 6: 23 nodes, 18 edges\n  Article 7: 20 nodes, 14 edges\n  Article 8: 16 nodes, 11 edges\n  Article 9: 21 nodes, 18 edges\n\n================================================================================\nEXPORTING SUCCESSFUL RESULTS\n================================================================================\n✓ Saved 149 JSON files\n✓ Saved 1955 triples to successful_triples.csv\n✓ Saved summary\n\n✓ DONE! Downloaded what we could process.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/networkx/readwrite/json_graph/node_link.py:145: FutureWarning: \nThe default value will be `edges=\"edges\" in NetworkX 3.6.\n\nTo make this warning go away, explicitly set the edges kwarg, e.g.:\n\n  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"# ============================================================================\n# VIEW SUCCESSFUL RESULTS\n# ============================================================================\n\nimport pandas as pd\n\nprint(\"=\" * 80)\nprint(\"YOUR 46 SUCCESSFUL KNOWLEDGE GRAPHS\")\nprint(\"=\" * 80)\n\n# Load summary\nsummary_df = pd.read_csv('/kaggle/working/processing_summary.csv')\nsuccessful_df = summary_df[summary_df['status'] == 'success'].copy()\n\nprint(f\"\\nTop 10 Largest KGs:\")\nprint(\"-\" * 80)\ntop_kgs = successful_df.nlargest(10, 'num_edges')\nprint(top_kgs[['article_id', 'num_nodes', 'num_edges', 'text_preview']].to_string(index=False))\n\n# Load triples\ntriples_df = pd.read_csv('/kaggle/working/successful_triples.csv')\n\nprint(f\"\\n\\nSample Triples:\")\nprint(\"-\" * 80)\nprint(triples_df.head(20).to_string(index=False))\n\nprint(f\"\\n\\nStatistics:\")\nprint(\"-\" * 80)\nprint(f\"Total successful articles: {len(successful_df)}\")\nprint(f\"Total triples extracted: {len(triples_df)}\")\nprint(f\"Avg edges per KG: {successful_df['num_edges'].mean():.2f}\")\nprint(f\"Max edges in a KG: {successful_df['num_edges'].max()}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FILES READY TO DOWNLOAD:\")\nprint(\"=\" * 80)\nprint(\"  📁 /kaggle/working/individual_kgs/ (46 JSON files)\")\nprint(\"  📄 /kaggle/working/successful_triples.csv\")\nprint(\"  📄 /kaggle/working/processing_summary.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:49:56.331728Z","iopub.execute_input":"2025-10-22T17:49:56.331951Z","iopub.status.idle":"2025-10-22T17:49:56.351075Z","shell.execute_reply.started":"2025-10-22T17:49:56.331935Z","shell.execute_reply":"2025-10-22T17:49:56.350288Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nYOUR 46 SUCCESSFUL KNOWLEDGE GRAPHS\n================================================================================\n\nTop 10 Largest KGs:\n--------------------------------------------------------------------------------\n article_id  num_nodes  num_edges                                                                                           text_preview\n        147         45         31   Heat waves are unpleasant for healthy folks. For people with cardiovascular trouble, hazy, hot, humi\n         67         37         28   A dear friend got caught up in a debate about these terms during a holiday dinner some time ago. I w\n        103         25         25   ARCHIVED CONTENT: As a service to our readers, Harvard Health Publishing provides access to our libr\n        106         27         25   My ability to sit peacefully day after day and write about health or enjoy my family owes more than \n        142         28         25   Low-fat diets, move over. When it comes to lowering cholesterol, a “portfolio” diet that includes ch\n          0         35         23 1. Health care reform\\n\\nHow could the health care reform legislation that President Barack Obama sign\n         10         35         23 1. Health care reform\\n\\nHow could the health care reform legislation that President Barack Obama sign\n         20         32         22   As usual, Harvard Health Publishing’ writers and editors have been busy covering a range of health t\n        105         31         22   ARCHIVED CONTENT: As a service to our readers, Harvard Health Publishing provides access to our libr\n        108         30         22   I was hiking in the woods recently with a group of women friends when something caught my attention.\n\n\nSample Triples:\n--------------------------------------------------------------------------------\n article_id                                    subject                                      predicate                                             object\n          0                     president barack obama                                signed into law         patient protection and affordable care act\n          0 patient protection and affordable care act                            extends coverage to                               32 million americans\n          0 patient protection and affordable care act                  reins in health care spending        17% of the country's gross domestic product\n          0        cardiopulmonary resuscitation (cpr)                                 new guidelines simplifying cpr and emphasizing chest compressions\n          0                            shinya yamanaka discovered a technique for reprogramming cells                     induced pluripotent stem cells\n          0                                 stem cells                             can be turned into     blood vessel cells, bone cells, and skin cells\n          0                                 telomerase                       can be turned on and off                                    premature aging\n          0                                 telomerase                               can be activated                           anti-aging possibilities\n          0                                lung cancer                                 screening test                                           ct scans\n          0                                  vitamin d                       recommended daily intake                 600 international units (iu) a day\n          0                                  vitamin d                               safe upper limit                                     4,000 iu a day\n          0                                   warfarin                                    alternative                               dabigatran (pradaxa)\n          0                            bisphosphonates                     concerns about ill effects                                      long-term use\n          0                            bisphosphonates                              increases risk of                         thighbone (femur) fracture\n          0                                 stem cells                           can be used to treat                               spinal cord injuries\n          0                                concussions                                      can cause                             permanent brain damage\n          0                                concussions                               can be prevented by reducing the number and severity of concussions\n          0                                        cpr                              can be simplified                  by emphasizing chest compressions\n          0                                 telomerase                                 can be used to                         slow down biological aging\n          0                                  vitamin d            has a wide range of health benefits                                 beyond bone health\n\n\nStatistics:\n--------------------------------------------------------------------------------\nTotal successful articles: 149\nTotal triples extracted: 1955\nAvg edges per KG: 12.81\nMax edges in a KG: 31\n\n================================================================================\nFILES READY TO DOWNLOAD:\n================================================================================\n  📁 /kaggle/working/individual_kgs/ (46 JSON files)\n  📄 /kaggle/working/successful_triples.csv\n  📄 /kaggle/working/processing_summary.csv\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"# ============================================================================\n# VISUALIZE INDIVIDUAL ARTICLE KNOWLEDGE GRAPHS\n# ============================================================================\n\n!pip install pyvis --quiet\n\nfrom pyvis.network import Network\nimport networkx as nx\nimport os\n\ndef visualize_article_kg(kg_data, output_file):\n    \"\"\"\n    Create interactive HTML visualization for one article's KG\n    \"\"\"\n    graph = kg_data['graph']\n    article_id = kg_data['id']\n    \n    if len(graph.nodes()) == 0:\n        print(f\"  Article {article_id}: No graph to visualize\")\n        return None\n    \n    print(f\"  Article {article_id}: {len(graph.nodes())} nodes, {len(graph.edges())} edges\")\n    \n    # Create pyvis network\n    net = Network(\n        height='800px',\n        width='100%',\n        bgcolor='#ffffff',\n        font_color='black',\n        directed=True,\n        notebook=False\n    )\n    \n    # Set physics options\n    net.set_options(\"\"\"\n    {\n      \"physics\": {\n        \"enabled\": true,\n        \"barnesHut\": {\n          \"gravitationalConstant\": -8000,\n          \"centralGravity\": 0.3,\n          \"springLength\": 95,\n          \"springConstant\": 0.04\n        },\n        \"minVelocity\": 0.75\n      },\n      \"nodes\": {\n        \"font\": {\"size\": 16}\n      },\n      \"edges\": {\n        \"font\": {\"size\": 12},\n        \"arrows\": {\"to\": {\"enabled\": true, \"scaleFactor\": 0.5}}\n      }\n    }\n    \"\"\")\n    \n    # Add nodes with colors based on degree\n    for node in graph.nodes():\n        degree = graph.degree(node)\n        size = 20 + degree * 5\n        \n        # Color based on degree\n        if degree > 5:\n            color = '#e74c3c'  # Red for highly connected\n        elif degree > 2:\n            color = '#3498db'  # Blue for medium\n        else:\n            color = '#95a5a6'  # Gray for low\n        \n        net.add_node(\n            node, \n            label=node, \n            size=size, \n            title=f\"{node}\\nConnections: {degree}\",\n            color=color\n        )\n    \n    # Add edges with labels\n    for source, target, data in graph.edges(data=True):\n        relation = data.get('relation', 'related_to')\n        net.add_edge(source, target, label=relation, title=relation)\n    \n    # Save\n    net.save_graph(output_file)\n    \n    return net\n\n# Create visualizations directory\nos.makedirs('/kaggle/working/kg_visualizations', exist_ok=True)\n\nprint(\"=\" * 80)\nprint(\"CREATING INTERACTIVE VISUALIZATIONS\")\nprint(\"=\" * 80)\n\n# Visualize all successful KGs\nvisualized = 0\nfor kg in all_article_kgs:\n    if kg['num_edges'] > 0:  # Only visualize articles with graphs\n        article_id = kg['id']\n        output_file = f'/kaggle/working/kg_visualizations/article_{article_id}_graph.html'\n        visualize_article_kg(kg, output_file)\n        visualized += 1\n\nprint(f\"\\n✓ Created {visualized} interactive visualizations!\")\nprint(f\"  Location: /kaggle/working/kg_visualizations/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:50:24.206595Z","iopub.execute_input":"2025-10-22T17:50:24.206879Z","iopub.status.idle":"2025-10-22T17:50:29.346197Z","shell.execute_reply.started":"2025-10-22T17:50:24.206857Z","shell.execute_reply":"2025-10-22T17:50:29.345306Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h================================================================================\nCREATING INTERACTIVE VISUALIZATIONS\n================================================================================\n  Article 0: 35 nodes, 23 edges\n  Article 1: 22 nodes, 15 edges\n  Article 2: 11 nodes, 9 edges\n  Article 3: 31 nodes, 18 edges\n  Article 4: 26 nodes, 21 edges\n  Article 5: 23 nodes, 16 edges\n  Article 6: 23 nodes, 18 edges\n  Article 7: 20 nodes, 14 edges\n  Article 8: 16 nodes, 11 edges\n  Article 9: 21 nodes, 18 edges\n  Article 10: 35 nodes, 23 edges\n  Article 11: 22 nodes, 15 edges\n  Article 12: 11 nodes, 9 edges\n  Article 13: 31 nodes, 18 edges\n  Article 14: 26 nodes, 21 edges\n  Article 15: 23 nodes, 16 edges\n  Article 16: 23 nodes, 18 edges\n  Article 17: 20 nodes, 14 edges\n  Article 18: 16 nodes, 11 edges\n  Article 19: 21 nodes, 18 edges\n  Article 20: 32 nodes, 22 edges\n  Article 21: 23 nodes, 18 edges\n  Article 22: 22 nodes, 15 edges\n  Article 23: 5 nodes, 3 edges\n  Article 24: 30 nodes, 16 edges\n  Article 25: 16 nodes, 9 edges\n  Article 26: 35 nodes, 20 edges\n  Article 27: 22 nodes, 12 edges\n  Article 28: 22 nodes, 12 edges\n  Article 29: 6 nodes, 5 edges\n  Article 30: 27 nodes, 14 edges\n  Article 31: 13 nodes, 7 edges\n  Article 32: 13 nodes, 9 edges\n  Article 33: 9 nodes, 5 edges\n  Article 34: 20 nodes, 10 edges\n  Article 35: 10 nodes, 9 edges\n  Article 36: 12 nodes, 8 edges\n  Article 37: 14 nodes, 8 edges\n  Article 38: 13 nodes, 11 edges\n  Article 39: 10 nodes, 8 edges\n  Article 40: 10 nodes, 9 edges\n  Article 41: 28 nodes, 18 edges\n  Article 42: 15 nodes, 10 edges\n  Article 43: 18 nodes, 10 edges\n  Article 44: 17 nodes, 10 edges\n  Article 45: 20 nodes, 11 edges\n  Article 46: 17 nodes, 16 edges\n  Article 47: 16 nodes, 11 edges\n  Article 48: 11 nodes, 12 edges\n  Article 49: 17 nodes, 11 edges\n  Article 50: 13 nodes, 10 edges\n  Article 51: 13 nodes, 8 edges\n  Article 52: 13 nodes, 10 edges\n  Article 53: 26 nodes, 18 edges\n  Article 54: 18 nodes, 12 edges\n  Article 55: 10 nodes, 9 edges\n  Article 56: 12 nodes, 9 edges\n  Article 57: 14 nodes, 9 edges\n  Article 58: 18 nodes, 10 edges\n  Article 59: 18 nodes, 11 edges\n  Article 60: 14 nodes, 10 edges\n  Article 61: 11 nodes, 7 edges\n  Article 62: 18 nodes, 11 edges\n  Article 63: 18 nodes, 10 edges\n  Article 64: 11 nodes, 9 edges\n  Article 65: 9 nodes, 5 edges\n  Article 66: 11 nodes, 8 edges\n  Article 67: 37 nodes, 28 edges\n  Article 68: 16 nodes, 11 edges\n  Article 69: 18 nodes, 15 edges\n  Article 70: 5 nodes, 3 edges\n  Article 71: 14 nodes, 8 edges\n  Article 72: 20 nodes, 19 edges\n  Article 73: 22 nodes, 16 edges\n  Article 75: 13 nodes, 8 edges\n  Article 76: 21 nodes, 17 edges\n  Article 77: 30 nodes, 16 edges\n  Article 78: 16 nodes, 11 edges\n  Article 79: 18 nodes, 15 edges\n  Article 80: 5 nodes, 3 edges\n  Article 81: 14 nodes, 8 edges\n  Article 82: 20 nodes, 19 edges\n  Article 83: 22 nodes, 16 edges\n  Article 85: 13 nodes, 8 edges\n  Article 86: 21 nodes, 17 edges\n  Article 87: 30 nodes, 16 edges\n  Article 88: 17 nodes, 13 edges\n  Article 89: 16 nodes, 11 edges\n  Article 90: 20 nodes, 12 edges\n  Article 91: 12 nodes, 8 edges\n  Article 92: 22 nodes, 18 edges\n  Article 93: 13 nodes, 10 edges\n  Article 94: 23 nodes, 14 edges\n  Article 95: 30 nodes, 21 edges\n  Article 96: 14 nodes, 13 edges\n  Article 97: 20 nodes, 13 edges\n  Article 98: 15 nodes, 9 edges\n  Article 99: 19 nodes, 13 edges\n  Article 100: 12 nodes, 11 edges\n  Article 101: 16 nodes, 12 edges\n  Article 102: 10 nodes, 7 edges\n  Article 103: 25 nodes, 25 edges\n  Article 104: 25 nodes, 15 edges\n  Article 105: 31 nodes, 22 edges\n  Article 106: 27 nodes, 25 edges\n  Article 107: 18 nodes, 13 edges\n  Article 108: 30 nodes, 22 edges\n  Article 109: 22 nodes, 13 edges\n  Article 110: 10 nodes, 5 edges\n  Article 111: 16 nodes, 10 edges\n  Article 112: 16 nodes, 9 edges\n  Article 113: 25 nodes, 13 edges\n  Article 114: 17 nodes, 12 edges\n  Article 115: 14 nodes, 10 edges\n  Article 116: 6 nodes, 3 edges\n  Article 117: 15 nodes, 9 edges\n  Article 118: 21 nodes, 18 edges\n  Article 119: 20 nodes, 13 edges\n  Article 120: 13 nodes, 8 edges\n  Article 121: 18 nodes, 20 edges\n  Article 122: 11 nodes, 10 edges\n  Article 123: 17 nodes, 11 edges\n  Article 124: 15 nodes, 9 edges\n  Article 125: 11 nodes, 9 edges\n  Article 126: 22 nodes, 19 edges\n  Article 127: 3 nodes, 2 edges\n  Article 128: 36 nodes, 20 edges\n  Article 129: 14 nodes, 10 edges\n  Article 130: 12 nodes, 7 edges\n  Article 131: 22 nodes, 15 edges\n  Article 132: 14 nodes, 12 edges\n  Article 133: 11 nodes, 10 edges\n  Article 134: 16 nodes, 10 edges\n  Article 135: 25 nodes, 18 edges\n  Article 136: 15 nodes, 12 edges\n  Article 137: 12 nodes, 11 edges\n  Article 138: 13 nodes, 9 edges\n  Article 139: 34 nodes, 21 edges\n  Article 140: 13 nodes, 12 edges\n  Article 141: 24 nodes, 18 edges\n  Article 142: 28 nodes, 25 edges\n  Article 143: 14 nodes, 9 edges\n  Article 144: 10 nodes, 9 edges\n  Article 145: 12 nodes, 8 edges\n  Article 146: 21 nodes, 17 edges\n  Article 147: 45 nodes, 31 edges\n  Article 148: 15 nodes, 13 edges\n  Article 149: 12 nodes, 7 edges\n  Article 150: 15 nodes, 10 edges\n\n✓ Created 149 interactive visualizations!\n  Location: /kaggle/working/kg_visualizations/\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}