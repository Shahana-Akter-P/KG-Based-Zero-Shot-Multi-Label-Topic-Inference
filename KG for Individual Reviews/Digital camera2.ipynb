{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13468504,"sourceType":"datasetVersion","datasetId":8549797},{"sourceId":13524188,"sourceType":"datasetVersion","datasetId":8587369}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:02.108101Z","iopub.execute_input":"2025-11-07T17:38:02.108729Z","iopub.status.idle":"2025-11-07T17:38:02.121627Z","shell.execute_reply.started":"2025-11-07T17:38:02.108711Z","shell.execute_reply":"2025-11-07T17:38:02.120421Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/keywords/Keyword_Medical.json\n/kaggle/input/keywords/Keyword_Canon.json\n/kaggle/input/keywords/Keyword_Creative.json\n/kaggle/input/keywords/Keyword_Apex.json\n/kaggle/input/keywords/Keyword_Apex 1.json\n/kaggle/input/keywords/Keyword_Nokia.json\n/kaggle/input/keywords/Keyword_News.json\n/kaggle/input/keywords/Keyword_Nikon.json\n/kaggle/input/zstikg/NewsConcept Data-set.json\n/kaggle/input/zstikg/DVD playerData-set.json\n/kaggle/input/zstikg/MedicalConcept Data-set.json\n/kaggle/input/zstikg/Cellular phone Data-set.json\n/kaggle/input/zstikg/Digital camera2 Data-set.json\n/kaggle/input/zstikg/Mp3 playerData-set.json\n/kaggle/input/zstikg/Digital camera1 Data-set.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# PART 1: INSTALLATION AND SETUP\n# ============================================================================\n\n# Install required packages\n!pip install dspy-ai huggingface_hub networkx sentence-transformers pandas --quiet\n\nprint(\"✓ Packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:02.122988Z","iopub.execute_input":"2025-11-07T17:38:02.123267Z","iopub.status.idle":"2025-11-07T17:38:05.772856Z","shell.execute_reply.started":"2025-11-07T17:38:02.123246Z","shell.execute_reply":"2025-11-07T17:38:05.771709Z"}},"outputs":[{"name":"stdout","text":"✓ Packages installed successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# PART 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom collections import defaultdict\nimport pandas as pd\nimport os\nfrom huggingface_hub import InferenceClient\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:05.773913Z","iopub.execute_input":"2025-11-07T17:38:05.774212Z","iopub.status.idle":"2025-11-07T17:38:21.948958Z","shell.execute_reply.started":"2025-11-07T17:38:05.774179Z","shell.execute_reply":"2025-11-07T17:38:21.947917Z"}},"outputs":[{"name":"stderr","text":"2025-11-07 17:38:17.240840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762537097.299570     362 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762537097.318841     362 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: CONFIGURE HUGGING FACE WITH DSPY.LM (PROPER WAY)\n# ============================================================================\n\nos.environ['HUGGINGFACE_API_KEY'] = 'KEY'\n\n# Use DSPy's built-in LM class with the correct prefix\nlm = dspy.LM(\n    model='huggingface/meta-llama/Llama-3.1-8B-Instruct',\n    api_key=os.environ['HUGGINGFACE_API_KEY'],\n    max_tokens=12000,\n    temperature=0.3\n)\n\ndspy.settings.configure(lm=lm)\nprint(\"✓ Llama-3.1-8B-Instruct configured successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:21.950979Z","iopub.execute_input":"2025-11-07T17:38:21.951886Z","iopub.status.idle":"2025-11-07T17:38:21.958165Z","shell.execute_reply.started":"2025-11-07T17:38:21.951861Z","shell.execute_reply":"2025-11-07T17:38:21.956739Z"}},"outputs":[{"name":"stdout","text":"✓ Llama-3.1-8B-Instruct configured successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================================\n# PART 1: DATA LOADING FUNCTION\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"\n    Load JSON data from file\n    \n    Expected format:\n    [\n        {\n            \"Article Title\": [],\n            \"Article Text\": \"text here...\",\n            \"Concept\": [\"concept1\", \"concept2\"]\n        },\n        ...\n    ]\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\n# Test with sample data\nsample_data = [\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"excellent phone, excellent service.\",\n        \"Concept\": []\n    },\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"i am a business user who heavily depend on mobile service.\",\n        \"Concept\": [\"service\"]\n    }\n]\n\nprint(\"✓ Sample data ready for testing\")\nprint(f\"  Sample has {len(sample_data)} documents\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:21.959343Z","iopub.execute_input":"2025-11-07T17:38:21.959699Z","iopub.status.idle":"2025-11-07T17:38:21.990858Z","shell.execute_reply.started":"2025-11-07T17:38:21.959668Z","shell.execute_reply":"2025-11-07T17:38:21.989760Z"}},"outputs":[{"name":"stdout","text":"✓ Sample data ready for testing\n  Sample has 2 documents\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# LOAD ONE SPECIFIC DATASET\n# ============================================================================\n\n# Choose which dataset you want to work with:\n# Option 1: Cellular phone\nfilepath = '/kaggle/input/zstikg/Digital camera2 Data-set.json'\n\n# Option 2: News\n# filepath = '/kaggle/input/zsltikg/NewsConcept Data-set.json'\n\n# Option 3: Medical\n# filepath = '/kaggle/input/zsltikg/MedicalConcept Data-set.json'\n\n# Load the data\ndata = load_json_data(filepath)\n\nprint(f\"✓ Loaded {len(data)} documents\")\nprint(f\"\\nFirst document preview:\")\nprint(f\"  Keys: {list(data[0].keys())}\")\nprint(f\"  Text: {data[0].get('Article Text', '')}...\")\nprint(f\"  Concepts: {data[0].get('Concept', [])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:21.992270Z","iopub.execute_input":"2025-11-07T17:38:21.992631Z","iopub.status.idle":"2025-11-07T17:38:22.018870Z","shell.execute_reply.started":"2025-11-07T17:38:21.992605Z","shell.execute_reply":"2025-11-07T17:38:22.017663Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 380 documents from /kaggle/input/zstikg/Digital camera2 Data-set.json\n✓ Loaded 380 documents\n\nFirst document preview:\n  Keys: ['Article Title', 'Article Text', 'Concept']\n  Text: the best 4mp compact digital available  \n...\n  Concepts: []\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# PROCESS YOUR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"PROCESSING ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\n# data is now correctly loaded as a list of dicts\nindividual_articles = []\n\nfor idx, item in enumerate(data):\n    article = {\n        'id': idx,\n        'Article Text': item['Article Text'],\n        #'Concept': item['Concept'] if item['Concept'] else []\n    }\n    individual_articles.append(article)\n\nprint(f\"\\n✓ Created list of {len(individual_articles)} individual articles\")\n\n# Show first 3\nprint(f\"\\nFirst 3 articles:\")\nprint(\"-\" * 80)\nfor i in range(min(3, len(individual_articles))):\n    article = individual_articles[i]\n    print(f\"\\nArticle {article['id']}:\")\n    print(f\"  Text: {article['Article Text'][:80]}...\")\n    #print(f\"  Concepts: {article['Concept']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:22.019674Z","iopub.execute_input":"2025-11-07T17:38:22.019898Z","iopub.status.idle":"2025-11-07T17:38:22.039581Z","shell.execute_reply.started":"2025-11-07T17:38:22.019882Z","shell.execute_reply":"2025-11-07T17:38:22.038599Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPROCESSING ALL THE ARTICLES\n================================================================================\n\n✓ Created list of 380 individual articles\n\nFirst 3 articles:\n--------------------------------------------------------------------------------\n\nArticle 0:\n  Text: the best 4mp compact digital available  \n...\n\nArticle 1:\n  Text: this camera is perfect for an enthusiastic amateur photographer . \n...\n\nArticle 2:\n  Text: the pictures are razor-sharp , even in macro . \n...\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: DEFINE SIGNATURES\n# ============================================================================\n\nclass EntityExtractor(dspy.Signature):\n    \"\"\"Extract key entities from the given text. Extracted entities are nouns, \n    verbs, or adjectives, particularly regarding sentiment. This is for an \n    extraction task, please be thorough and accurate to the reference text.\n    \n    Return ONLY a valid JSON list format: [\"entity1\", \"entity2\", \"entity3\"]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract entities from\")\n    entities = dspy.OutputField(desc=\"List of extracted entities in JSON format\")\n\nclass RelationExtractor(dspy.Signature):\n    \"\"\"Extract subject-predicate-object triples from the assistant message. \n    A predicate (1-3 words) defines the relationship between the subject and \n    object. Relationship may be fact or sentiment based on assistant's message. \n    Subject and object are entities. Entities provided are from the assistant \n    message and prior conversation history, though you may not need all of them. \n    This is for an extraction task, please be thorough, accurate, and faithful \n    to the reference text.\n    \n    Return ONLY valid JSON format: [[\"subject1\", \"predicate1\", \"object1\"], [\"subject2\", \"predicate2\", \"object2\"]]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract relations from\")\n    entities = dspy.InputField(desc=\"List of available entities\")\n    triples = dspy.OutputField(desc=\"List of [subject, predicate, object] triples in JSON format\")\n\nprint(\"✓ Signatures defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:22.040691Z","iopub.execute_input":"2025-11-07T17:38:22.041017Z","iopub.status.idle":"2025-11-07T17:38:22.062992Z","shell.execute_reply.started":"2025-11-07T17:38:22.040989Z","shell.execute_reply":"2025-11-07T17:38:22.061498Z"}},"outputs":[{"name":"stdout","text":"✓ Signatures defined\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: CREATE ENTITY EXTRACTOR\n# ============================================================================\n\nclass ExtractEntities(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(EntityExtractor)\n    \n    def forward(self, text: str) -> List[str]:\n        if not text or len(text.strip()) < 3:\n            return []\n            \n        result = self.extract(text=text)\n        \n        try:\n            entities_text = result.entities.strip()\n            \n            if '[' in entities_text and ']' in entities_text:\n                start = entities_text.find('[')\n                end = entities_text.rfind(']') + 1\n                entities_text = entities_text[start:end]\n            \n            entities = json.loads(entities_text)\n            \n            if isinstance(entities, list):\n                return [str(e).lower().strip() for e in entities if e and len(str(e).strip()) > 1]\n            return []\n            \n        except:\n            try:\n                entities_text = result.entities.strip()\n                if entities_text.startswith('['):\n                    entities_text = entities_text[1:]\n                if entities_text.endswith(']'):\n                    entities_text = entities_text[:-1]\n                \n                entities = []\n                for item in entities_text.split(','):\n                    item = item.strip(' \"\\'\\n\\t')\n                    if item and len(item) > 1:\n                        entities.append(item.lower())\n                \n                return entities[:50]\n            except:\n                return []\n\nentity_extractor = ExtractEntities()\nprint(\"✓ Entity Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:22.065746Z","iopub.execute_input":"2025-11-07T17:38:22.065981Z","iopub.status.idle":"2025-11-07T17:38:22.087953Z","shell.execute_reply.started":"2025-11-07T17:38:22.065965Z","shell.execute_reply":"2025-11-07T17:38:22.087062Z"}},"outputs":[{"name":"stdout","text":"✓ Entity Extractor created\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: CREATE RELATION EXTRACTOR\n# ============================================================================\n\nclass ExtractRelations(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(RelationExtractor)\n    \n    def forward(self, text: str, entities: List[str]) -> List[Tuple[str, str, str]]:\n        if not entities or not text:\n            return []\n        \n        entities_subset = entities[:30]\n        entities_str = json.dumps(entities_subset)\n        \n        result = self.extract(text=text, entities=entities_str)\n        \n        try:\n            triples_text = result.triples.strip()\n            \n            if '[' in triples_text and ']' in triples_text:\n                start = triples_text.find('[')\n                end = triples_text.rfind(']') + 1\n                triples_text = triples_text[start:end]\n            \n            triples = json.loads(triples_text)\n            \n            normalized_triples = []\n            for triple in triples:\n                if isinstance(triple, (list, tuple)) and len(triple) == 3:\n                    s, p, o = triple\n                    s = str(s).lower().strip()\n                    p = str(p).lower().strip()\n                    o = str(o).lower().strip()\n                    \n                    if s and p and o and s != o:\n                        normalized_triples.append((s, p, o))\n            \n            return normalized_triples\n            \n        except Exception as e:\n            return []\n\nrelation_extractor = ExtractRelations()\nprint(\"✓ Relation Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:22.088708Z","iopub.execute_input":"2025-11-07T17:38:22.088928Z","iopub.status.idle":"2025-11-07T17:38:22.117425Z","shell.execute_reply.started":"2025-11-07T17:38:22.088913Z","shell.execute_reply":"2025-11-07T17:38:22.116389Z"}},"outputs":[{"name":"stdout","text":"✓ Relation Extractor created\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# CLUSTERING SIGNATURES (DEFINE FIRST!)\n# ============================================================================\n\nclass ClusterValidator(dspy.Signature):\n    \"\"\"Verify if these entities belong in the same cluster.\n    A cluster should contain entities that are the same in meaning, with different:\n    - tenses, plural forms, stem forms, upper/lower cases\n    Or entities with close semantic meanings.\n    \n    Return ONLY valid JSON format: [\"entity1\", \"entity2\", \"entity3\"]\n    Return only entities you are confident belong together.\n    If not confident, return empty list [].\n    \"\"\"\n    \n    entities = dspy.InputField(desc=\"Entities to validate\")\n    valid_cluster = dspy.OutputField(desc=\"Validated cluster in JSON format\")\n\nprint(\"✓ ClusterValidator Signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:22.118640Z","iopub.execute_input":"2025-11-07T17:38:22.119553Z","iopub.status.idle":"2025-11-07T17:38:22.145315Z","shell.execute_reply.started":"2025-11-07T17:38:22.119520Z","shell.execute_reply":"2025-11-07T17:38:22.143870Z"}},"outputs":[{"name":"stdout","text":"✓ ClusterValidator Signature defined\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================================\n# SEMANTIC SIMILARITY CLUSTERING (FROM PAPER)\n# ============================================================================\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nclass SemanticEntityClustering(dspy.Module):\n    def __init__(self, similarity_threshold=0.75):\n        super().__init__()\n        self.validator = dspy.ChainOfThought(ClusterValidator)\n        \n        # Load embedding model (same as paper)\n        print(\"Loading sentence transformer model...\")\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        print(\"✓ Model loaded\")\n        \n        self.similarity_threshold = similarity_threshold\n    \n    def _parse_cluster(self, text: str) -> List[str]:\n        \"\"\"Parse cluster from LLM response\"\"\"\n        try:\n            text = text.strip()\n            if '[' in text and ']' in text:\n                start = text.find('[')\n                end = text.rfind(']') + 1\n                text = text[start:end]\n            \n            cluster = json.loads(text)\n            if isinstance(cluster, list):\n                return [str(e).lower().strip() for e in cluster if e]\n            return []\n        except:\n            return []\n    \n    def _get_semantic_clusters(self, entities: List[str]) -> List[List[str]]:\n        \"\"\"Group entities by semantic similarity using embeddings\"\"\"\n        \n        if len(entities) == 0:\n            return []\n        \n        # Get embeddings for all entities\n        embeddings = self.model.encode(entities)\n        \n        # Compute pairwise cosine similarity\n        similarity_matrix = cosine_similarity(embeddings)\n        \n        # Find clusters using similarity threshold\n        clusters = []\n        remaining = set(range(len(entities)))\n        \n        for i in range(len(entities)):\n            if i not in remaining:\n                continue\n            \n            # Find all entities similar to this one\n            cluster_indices = [i]\n            remaining.discard(i)\n            \n            for j in range(i + 1, len(entities)):\n                if j not in remaining:\n                    continue\n                \n                # Check if similar enough\n                if similarity_matrix[i][j] >= self.similarity_threshold:\n                    cluster_indices.append(j)\n                    remaining.discard(j)\n            \n            # Convert indices to entity names\n            cluster = [entities[idx] for idx in cluster_indices]\n            \n            # Only keep clusters with 2-4 entities\n            if 2 <= len(cluster) <= 4:\n                clusters.append(cluster)\n            elif len(cluster) == 1:\n                # Keep singletons for later\n                pass\n        \n        return clusters\n    \n    def forward(self, entities: List[str]) -> Dict[str, List[str]]:\n        \"\"\"Semantic clustering with LLM validation\"\"\"\n        \n        print(f\"Starting semantic clustering with {len(entities)} entities...\")\n        print(f\"  Similarity threshold: {self.similarity_threshold}\")\n        \n        # Remove duplicates\n        unique_entities = list(set(entities))\n        \n        # Step 1: Find semantic clusters using embeddings\n        print(\"  Computing semantic similarities...\")\n        potential_clusters = self._get_semantic_clusters(unique_entities)\n        \n        print(f\"  Found {len(potential_clusters)} potential clusters\")\n        \n        # Step 2: Validate with LLM\n        validated_clusters = {}\n        cluster_id = 0\n        clustered_entities = set()\n        \n        for cluster in potential_clusters:\n            try:\n                # Ask LLM to validate\n                validation = self.validator(entities=json.dumps(cluster))\n                validated = self._parse_cluster(validation.valid_cluster)\n                \n                if validated and len(validated) >= 2:\n                    cluster_label = validated[0]\n                    validated_clusters[cluster_label] = validated\n                    \n                    for entity in validated:\n                        clustered_entities.add(entity)\n                    \n                    print(f\"  ✓ Cluster {cluster_id}: {validated}\")\n                    cluster_id += 1\n                else:\n                    # LLM rejected - add as singletons\n                    for entity in cluster:\n                        if entity not in clustered_entities:\n                            validated_clusters[entity] = [entity]\n                            clustered_entities.add(entity)\n            except:\n                # Error - add as singletons\n                for entity in cluster:\n                    if entity not in clustered_entities:\n                        validated_clusters[entity] = [entity]\n                        clustered_entities.add(entity)\n        \n        # Step 3: Add all remaining entities as singletons\n        for entity in unique_entities:\n            if entity not in clustered_entities:\n                validated_clusters[entity] = [entity]\n        \n        multi = sum(1 for v in validated_clusters.values() if len(v) > 1)\n        print(f\"✓ Semantic clustering complete: {len(validated_clusters)} total clusters\")\n        print(f\"  Multi-entity clusters: {multi}\")\n        print(f\"  Singleton entities: {len(validated_clusters) - multi}\")\n        \n        return validated_clusters\n\n# Create semantic clusterer with different thresholds\nentity_clusterer_semantic = SemanticEntityClustering(similarity_threshold=0.75)\nprint(\"\\n✓ Semantic Entity Clustering Module created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:22.146536Z","iopub.execute_input":"2025-11-07T17:38:22.146811Z","iopub.status.idle":"2025-11-07T17:38:26.986046Z","shell.execute_reply.started":"2025-11-07T17:38:22.146793Z","shell.execute_reply":"2025-11-07T17:38:26.984725Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✓ Model loaded\n\n✓ Semantic Entity Clustering Module created\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================================\n# FINAL KGGEN WITH SEMANTIC CLUSTERING\n# ============================================================================\n\nclass KGGenSemantic:\n    def __init__(self, similarity_threshold=0.75):\n        self.entity_extractor = entity_extractor\n        self.relation_extractor = relation_extractor\n        self.entity_clusterer = SemanticEntityClustering(similarity_threshold=similarity_threshold)\n        self.graph = nx.DiGraph()\n        self.entity_clusters = {}\n        \n    def generate_from_json(self, json_data: List[Dict], max_docs: int = None) -> nx.DiGraph:\n        \"\"\"Generate KG from JSON dataset\"\"\"\n        all_entities = set()\n        all_relations = []\n        \n        if max_docs:\n            json_data = json_data[:max_docs]\n        \n        print(f\"Processing {len(json_data)} documents...\")\n        print(\"=\" * 80)\n        \n        for idx, item in enumerate(json_data):\n            text = item.get('Article Text', '')\n            concepts = item.get('Concept', [])\n            \n            if not text or len(text.strip()) < 5:\n                continue\n            \n            try:\n                # Extract entities\n                entities = self.entity_extractor(text)\n                all_entities.update(entities)\n                \n                # Add concepts\n                #for concept in concepts:\n                    #if concept and isinstance(concept, str):\n                        #all_entities.add(concept.lower().strip())\n                \n                # Extract relations\n                relations = self.relation_extractor(text, list(all_entities))\n                all_relations.extend(relations)\n                \n                if (idx + 1) % 20 == 0:\n                    print(f\"  {idx + 1}/{len(json_data)} docs | {len(all_entities)} entities | {len(all_relations)} relations\")\n                    \n            except Exception as e:\n                continue\n        \n        print(f\"\\n✓ Extraction complete!\")\n        print(f\"  Total entities: {len(all_entities)}\")\n        print(f\"  Total relations: {len(all_relations)}\")\n        \n        # Build graph\n        for subj, pred, obj in all_relations:\n            self.graph.add_edge(subj, obj, relation=pred)\n        \n        print(f\"  Graph nodes: {len(self.graph.nodes())}\")\n        print(f\"  Graph edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def cluster_entities(self):\n        \"\"\"Semantic clustering with embeddings\"\"\"\n        nodes = list(self.graph.nodes())\n        \n        if len(nodes) == 0:\n            print(\"No nodes to cluster!\")\n            return self.graph\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"SEMANTIC CLUSTERING: {len(nodes)} ENTITIES\")\n        print(f\"{'='*80}\")\n        \n        self.entity_clusters = self.entity_clusterer(nodes)\n        \n        # Map entities\n        entity_mapping = {}\n        for cluster_label, cluster_entities in self.entity_clusters.items():\n            for entity in cluster_entities:\n                entity_mapping[entity] = cluster_label\n        \n        # Rebuild graph\n        new_graph = nx.DiGraph()\n        for u, v, data in self.graph.edges(data=True):\n            new_u = entity_mapping.get(u, u)\n            new_v = entity_mapping.get(v, v)\n            relation = data.get('relation', 'related_to')\n            \n            if new_u == new_v:\n                continue\n            \n            if not new_graph.has_edge(new_u, new_v):\n                new_graph.add_edge(new_u, new_v, relation=relation)\n        \n        self.graph = new_graph\n        \n        print(f\"\\n✓ Clustering complete!\")\n        print(f\"  Final nodes: {len(self.graph.nodes())}\")\n        print(f\"  Final edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def save_graph(self, filepath: str):\n        data = nx.node_link_data(self.graph)\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n        print(f\"✓ Saved to {filepath}\")\n    \n    def export_triples(self, filepath: str):\n        triples = []\n        for u, v, data in self.graph.edges(data=True):\n            triples.append({\n                'subject': u,\n                'predicate': data.get('relation', 'related_to'),\n                'object': v\n            })\n        import pandas as pd\n        df = pd.DataFrame(triples)\n        df.to_csv(filepath, index=False)\n        print(f\"✓ Exported to {filepath}\")\n\n# Initialize semantic KGGen (threshold 0.75 = balanced)\nkg_gen_semantic = KGGenSemantic(similarity_threshold=0.75)\nprint(\"\\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:26.987206Z","iopub.execute_input":"2025-11-07T17:38:26.987584Z","iopub.status.idle":"2025-11-07T17:38:29.344312Z","shell.execute_reply.started":"2025-11-07T17:38:26.987554Z","shell.execute_reply":"2025-11-07T17:38:29.343334Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n✓ Model loaded\n\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================================================\n# GENERATE KG FOR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\nall_article_kgs = []\n\ntotal = len(individual_articles)\nprint(f\"\\nProcessing {total} articles...\\n\")\n\nfor idx, article in enumerate(individual_articles):\n    article_id = article['id']\n    text = article['Article Text']\n    #concepts = article['Concept']\n    \n    # Skip empty articles\n    if not text or len(text.strip()) < 5:\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n        continue\n    \n    try:\n        # Extract entities for THIS article\n        entities = entity_extractor(text)\n        \n        # Add original concepts as entities\n        #for concept in concepts:\n            #if concept and isinstance(concept, str):\n                #entities.append(concept.lower().strip())\n        \n        entities = list(set(entities))  # Remove duplicates\n        \n        # Extract relations for THIS article\n        if entities:\n            relations = relation_extractor(text, entities)\n        else:\n            relations = []\n        \n        # Build graph for THIS article\n        graph = nx.DiGraph()\n        for subj, pred, obj in relations:\n            graph.add_edge(subj, obj, relation=pred)\n        \n        # Store everything for this article\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': graph,\n            'entities': entities,\n            'relations': relations,\n            'num_nodes': len(graph.nodes()),\n            'num_edges': len(graph.edges())\n        })\n        \n        # Progress update\n        if (idx + 1) % 50 == 0:\n            print(f\"✓ Processed {idx + 1}/{total} articles...\")\n        \n    except Exception as e:\n        print(f\"✗ Article {article_id}: Error - {str(e)}\")\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n\nprint(f\"\\n\" + \"=\" * 80)\nprint(\"✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\")\nprint(\"=\" * 80)\nprint(f\"Total articles processed: {len(all_article_kgs)}\")\nprint(f\"Articles with graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] > 0)}\")\nprint(f\"Articles without graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] == 0)}\")\n\n# Statistics\ntotal_entities = sum(len(kg['entities']) for kg in all_article_kgs)\ntotal_relations = sum(len(kg['relations']) for kg in all_article_kgs)\n\nprint(f\"\\nTotal entities extracted: {total_entities}\")\nprint(f\"Total relations extracted: {total_relations}\")\n\nwith_graphs = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\nif with_graphs:\n    avg_nodes = sum(kg['num_nodes'] for kg in with_graphs) / len(with_graphs)\n    avg_edges = sum(kg['num_edges'] for kg in with_graphs) / len(with_graphs)\n    #print(f\"\\nAverage nodes per KG: {avg_nodes:.2f}\")\n    #print(f\"Average edges per KG: {avg_edges:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:38:29.345223Z","iopub.execute_input":"2025-11-07T17:38:29.345493Z","iopub.status.idle":"2025-11-07T18:00:08.109214Z","shell.execute_reply.started":"2025-11-07T17:38:29.345475Z","shell.execute_reply":"2025-11-07T18:00:08.108105Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nGENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\n================================================================================\n\nProcessing 380 articles...\n\n✓ Processed 50/380 articles...\n✓ Processed 100/380 articles...\n✓ Processed 150/380 articles...\n✓ Processed 200/380 articles...\n✓ Processed 250/380 articles...\n✓ Processed 300/380 articles...\n✓ Processed 350/380 articles...\n\n================================================================================\n✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\n================================================================================\nTotal articles processed: 380\nArticles with graphs: 363\nArticles without graphs: 17\n\nTotal entities extracted: 1321\nTotal relations extracted: 1068\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom dspy.signatures import Signature\nfrom dspy import OutputField, InputField\nimport pandas as pd\nimport os\nfrom collections import Counter\nimport numpy as np\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:00:08.110594Z","iopub.execute_input":"2025-11-07T18:00:08.110961Z","iopub.status.idle":"2025-11-07T18:00:08.116874Z","shell.execute_reply.started":"2025-11-07T18:00:08.110932Z","shell.execute_reply":"2025-11-07T18:00:08.115964Z"}},"outputs":[{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ============================================================================\n# SAVE KNOWLEDGE GRAPHS TO JSON FILE\n# ============================================================================\n\nimport json\nimport networkx as nx\n\ndef save_knowledge_graphs(all_article_kgs, filepath='/kaggle/working/article_knowledge_graphs.json'):\n    \"\"\"\n    Save knowledge graphs to JSON file.\n    \n    Args:\n        all_article_kgs: List of dictionaries containing KG information\n        filepath: Path to save the JSON file\n    \"\"\"\n    # Convert NetworkX graphs to serializable format\n    serializable_kgs = []\n    \n    for kg_data in all_article_kgs:\n        # Convert NetworkX graph to node-link format\n        graph = kg_data['graph']\n        graph_dict = nx.node_link_data(graph) if graph.number_of_nodes() > 0 else None\n        \n        serializable_kg = {\n            'id': kg_data['id'],\n            'text': kg_data['text'],\n            'entities': kg_data['entities'],\n            'relations': kg_data['relations'],\n            'num_nodes': kg_data['num_nodes'],\n            'num_edges': kg_data['num_edges'],\n            'graph_data': graph_dict  # Serialized graph\n        }\n        \n        serializable_kgs.append(serializable_kg)\n    \n    # Save to JSON\n    with open(filepath, 'w', encoding='utf-8') as f:\n        json.dump(serializable_kgs, f, indent=2, ensure_ascii=False)\n    \n    print(f\"\\n{'='*80}\")\n    print(\"KNOWLEDGE GRAPHS SAVED\")\n    print(f\"{'='*80}\")\n    print(f\"Saved {len(serializable_kgs)} knowledge graphs to:\")\n    print(f\"  {filepath}\")\n    print(f\"\\nFile size: {len(json.dumps(serializable_kgs)) / (1024*1024):.2f} MB\")\n    print(f\"Articles with graphs: {sum(1 for kg in serializable_kgs if kg['num_edges'] > 0)}\")\n    print(f\"Articles without graphs: {sum(1 for kg in serializable_kgs if kg['num_edges'] == 0)}\")\n\n# Save the knowledge graphs\nsave_knowledge_graphs(all_article_kgs, '/kaggle/working/article_knowledge_graphs.json')\n\nprint(\"\\n✓ Knowledge graphs saved successfully!\")\nprint(\"You can now proceed to the topic classification step.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:00:08.117748Z","iopub.execute_input":"2025-11-07T18:00:08.118023Z","iopub.status.idle":"2025-11-07T18:00:08.176696Z","shell.execute_reply.started":"2025-11-07T18:00:08.118003Z","shell.execute_reply":"2025-11-07T18:00:08.175480Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nKNOWLEDGE GRAPHS SAVED\n================================================================================\nSaved 380 knowledge graphs to:\n  /kaggle/working/article_knowledge_graphs.json\n\nFile size: 0.27 MB\nArticles with graphs: 363\nArticles without graphs: 17\n\n✓ Knowledge graphs saved successfully!\nYou can now proceed to the topic classification step.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: EXTRACT ALL UNIQUE CONCEPTS FROM DATASET (FIXED)\n# ============================================================================\n\ndef extract_unique_concepts(articles: List[Dict]) -> List[str]:\n    \"\"\"\n    Extract all unique concepts from the dataset's 'Concept' field.\n    \n    Args:\n        articles: List of article dictionaries\n    \n    Returns:\n        Sorted list of unique concepts\n    \"\"\"\n    all_concepts = set()\n    \n    for article in articles:\n        # Get concepts from the Concept field\n        if 'Concept' in article and article['Concept']:\n            if isinstance(article['Concept'], list):\n                all_concepts.update([c.lower().strip() for c in article['Concept'] if c])\n            elif isinstance(article['Concept'], str):\n                all_concepts.add(article['Concept'].lower().strip())\n    \n    # Convert to sorted list\n    unique_concepts = sorted(list(all_concepts))\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"UNIQUE CONCEPTS EXTRACTED FROM DATASET\")\n    print(f\"{'='*80}\")\n    print(f\"Total unique concepts: {len(unique_concepts)}\")\n    print(f\"\\nFirst 20 concepts: {unique_concepts[:20]}\")\n    print(f\"\\nLast 20 concepts: {unique_concepts[-20:]}\")\n    \n    return unique_concepts\n\n\n# FIX: Use 'data' instead of 'articles' (or rename 'data' to 'articles')\n# Option 1: Extract concepts from 'data'\nall_concepts = extract_unique_concepts(data)\n\n# Option 2: If you want to use 'articles', just rename 'data':\n# articles = data\n# all_concepts = extract_unique_concepts(articles)\n\n# Save concept list for reference\nwith open('/kaggle/working/all_concepts.json', 'w') as f:\n    json.dump(all_concepts, f, indent=2)\n\nprint(f\"\\n✓ Saved concept list to /kaggle/working/all_concepts.json\")\nprint(f\"✓ Ready for topic classification with {len(all_concepts)} concepts\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:00:08.177798Z","iopub.execute_input":"2025-11-07T18:00:08.178036Z","iopub.status.idle":"2025-11-07T18:00:08.185610Z","shell.execute_reply.started":"2025-11-07T18:00:08.178016Z","shell.execute_reply":"2025-11-07T18:00:08.184498Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nUNIQUE CONCEPTS EXTRACTED FROM DATASET\n================================================================================\nTotal unique concepts: 21\n\nFirst 20 concepts: ['battery', 'camera', 'control', 'design', 'feature', 'focus', 'lens', 'manual', 'memory', 'menu', 'mode', 'movie', 'picture', 'price', 'print', 'service', 'size', 'software', 'transfer', 'use']\n\nLast 20 concepts: ['camera', 'control', 'design', 'feature', 'focus', 'lens', 'manual', 'memory', 'menu', 'mode', 'movie', 'picture', 'price', 'print', 'service', 'size', 'software', 'transfer', 'use', 'weight']\n\n✓ Saved concept list to /kaggle/working/all_concepts.json\n✓ Ready for topic classification with 21 concepts\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: LOAD DATA AND EXISTING KNOWLEDGE GRAPHS\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"Load JSON data from file\"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\ndef load_knowledge_graphs(filepath: str) -> List[Dict]:\n    \"\"\"Load previously created knowledge graphs\"\"\"\n    if not os.path.exists(filepath):\n        print(f\"\\n⚠️  WARNING: Knowledge graph file not found at {filepath}\")\n        return None\n    \n    with open(filepath, 'r', encoding='utf-8') as f:\n        kgs = json.load(f)\n    print(f\"✓ Loaded {len(kgs)} knowledge graphs from {filepath}\")\n    return kgs\n\n# Load your data\narticles = load_json_data('/kaggle/input/zstikg/Digital camera2 Data-set.json')\nknowledge_graphs = load_knowledge_graphs('/kaggle/working/article_knowledge_graphs.json')\n\n# Only classify articles that have KGs\narticles_with_kgs = articles[:len(knowledge_graphs)] \n\nif knowledge_graphs is None:\n    print(\"\\n❌ Cannot proceed without knowledge graphs. Please save them first.\")\nelse:\n    print(f\"\\n✓ Successfully loaded {len(articles)} articles and {len(knowledge_graphs)} KGs\")\n    print(\"✓ Ready to proceed with topic classification!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:00:08.186598Z","iopub.execute_input":"2025-11-07T18:00:08.186848Z","iopub.status.idle":"2025-11-07T18:00:08.222134Z","shell.execute_reply.started":"2025-11-07T18:00:08.186830Z","shell.execute_reply":"2025-11-07T18:00:08.220244Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 380 documents from /kaggle/input/zstikg/Digital camera2 Data-set.json\n✓ Loaded 380 knowledge graphs from /kaggle/working/article_knowledge_graphs.json\n\n✓ Successfully loaded 380 articles and 380 KGs\n✓ Ready to proceed with topic classification!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: KG FORMATTING FUNCTION\n# ============================================================================\n\ndef format_kg_for_llm(kg_data: Dict) -> str:\n    \"\"\"Format knowledge graph into readable text for the LLM.\"\"\"\n    if not kg_data or kg_data.get('num_edges', 0) == 0:\n        return \"No knowledge graph available.\"\n    \n    # Format entities\n    entities = kg_data.get('entities', [])\n    entities_str = \", \".join(entities[:30]) if entities else \"None\"\n    if len(entities) > 30:\n        entities_str += f\"... ({len(entities) - 30} more)\"\n    \n    # Format relationships\n    relations = kg_data.get('relations', [])\n    if relations:\n        relationships = []\n        for relation in relations[:20]:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                relationships.append(f\"{source} --[{rel_type}]--> {target}\")\n        relationships_str = \"\\n\".join(relationships)\n        if len(relations) > 20:\n            relationships_str += f\"\\n... ({len(relations) - 20} more relationships)\"\n    else:\n        relationships_str = \"None\"\n    \n    kg_summary = f\"\"\"Knowledge Graph Summary:\nEntities: {entities_str}\n\nKey Relationships:\n{relationships_str}\"\"\"\n    \n    return kg_summary\n\nprint(\"✓ KG formatting function ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:00:08.223070Z","iopub.execute_input":"2025-11-07T18:00:08.223465Z","iopub.status.idle":"2025-11-07T18:00:08.230798Z","shell.execute_reply.started":"2025-11-07T18:00:08.223443Z","shell.execute_reply":"2025-11-07T18:00:08.229773Z"}},"outputs":[{"name":"stdout","text":"✓ KG formatting function ready\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: DEFINE DSPY SIGNATURE FOR TOPIC CLASSIFICATION\n# ============================================================================\n\nclass TopicClassification(Signature):\n    \"\"\"Given an article text, its knowledge graph, and a list of possible topics,\n    determine which topics (can be multiple) are most relevant to this article.\n    Return only topic names that are actually present in the available_topics list.\"\"\"\n    \n    article_text: str = InputField(\n        desc=\"The text content of the article\"\n    )\n    knowledge_graph: str = InputField(\n        desc=\"Knowledge graph extracted from the article showing entities and relationships\"\n    )\n    available_topics: str = InputField(\n        desc=\"Comma-separated list of all possible topic/concept names\"\n    )\n    \n    predicted_topics: str = OutputField(\n        desc=\"Comma-separated list of relevant topics from the available_topics list. \"\n             \"Only return topics that actually appear in the available_topics list. \"\n             \"If multiple topics apply, list them all (maximum 5). If no topics match well, return 'none'.\"\n    )\n    #confidence: str = OutputField(\n        #desc=\"Confidence level: high, medium, or low\"\n    #)\n    #reasoning: str = OutputField(\n        #desc=\"Brief explanation of why these topics were chosen based on the article and knowledge graph\"\n    #)\n\nprint(\"✓ Topic classification signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:00:08.231663Z","iopub.execute_input":"2025-11-07T18:00:08.231874Z","iopub.status.idle":"2025-11-07T18:00:08.255121Z","shell.execute_reply.started":"2025-11-07T18:00:08.231856Z","shell.execute_reply":"2025-11-07T18:00:08.254393Z"}},"outputs":[{"name":"stdout","text":"✓ Topic classification signature defined\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ============================================================================\n# STEP 7: CREATE TOPIC CLASSIFIER CLASS\n# ============================================================================\n\nclass ArticleTopicClassifier:\n    \"\"\"Multi-label topic classifier using article text, KG, and available topics.\"\"\"\n    \n    def __init__(self, available_topics: List[str]):\n        self.available_topics = available_topics\n        self.available_topics_str = \", \".join(available_topics)\n        self.classifier = dspy.ChainOfThought(TopicClassification)\n        print(f\"✓ Classifier initialized with {len(available_topics)} possible topics\")\n    \n    def classify_article(self, article_text: str, kg_data: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        kg_summary = format_kg_for_llm(kg_data)\n        \n        # Truncate article if too long\n        #max_text_length = 500\n        #if len(article_text) > max_text_length:\n            #article_text = article_text[:max_text_length] + \"...\"\n        \n        try:\n            result = self.classifier(\n                article_text=article_text,\n                knowledge_graph=kg_summary,\n                available_topics=self.available_topics_str\n            )\n            \n            predicted_topics_raw = result.predicted_topics\n            \n            if predicted_topics_raw.lower() == 'none':\n                predicted_topics = []\n            else:\n                predicted_topics = [\n                    t.strip().lower() \n                    for t in predicted_topics_raw.split(',')\n                    if t.strip()\n                ]\n                predicted_topics = [\n                    t for t in predicted_topics \n                    if t in [at.lower() for at in self.available_topics]\n                ]\n            \n            return {\n                'predicted_topics': predicted_topics,\n                'num_topics': len(predicted_topics),\n                #'confidence': result.confidence,\n                #'reasoning': result.reasoning\n            }\n            \n        except Exception as e:\n            print(f\"  Error: {str(e)}\")\n            return {\n                'predicted_topics': [],\n                'num_topics': 0,\n                #'confidence': 'error',\n                #'reasoning': f'Classification failed: {str(e)}'\n            }\n    \n    def classify_dataset(self, articles: List[Dict], knowledge_graphs: List[Dict],\n                        max_articles: int = None) -> List[Dict]:\n        \"\"\"Classify multiple articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        if max_articles:\n            articles = articles[:max_articles]\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(articles)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx, article in enumerate(articles):\n            print(f\"Processing article {idx}...\", end=\" \")\n            \n            article_text = article.get('Article Text', '')\n            kg_data = kg_dict.get(idx, {'num_edges': 0})\n            classification = self.classify_article(article_text, kg_data)\n            \n            result = {\n                'article_id': idx,\n                'article_text': article_text + \"...\",\n                'true_concepts': article.get('Concept', []),\n                'predicted_topics': classification['predicted_topics'],\n                'num_predicted': classification['num_topics'],\n                #'confidence': classification['confidence'],\n                #'reasoning': classification['reasoning']\n            }\n            \n            results.append(result)\n            print(f\"✓ Found {len(classification['predicted_topics'])} topics\")\n        \n        return results\n\nprint(\"✓ Classifier class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:00:08.255913Z","iopub.execute_input":"2025-11-07T18:00:08.256154Z","iopub.status.idle":"2025-11-07T18:00:08.276851Z","shell.execute_reply.started":"2025-11-07T18:00:08.256132Z","shell.execute_reply":"2025-11-07T18:00:08.275796Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier class defined\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ============================================================================\n# STEP 8: ANALYSIS FUNCTIONS\n# ============================================================================\n\ndef analyze_classification_results(results: List[Dict], available_topics: List[str]) -> Dict:\n    \"\"\"Analyze classification results and generate statistics.\"\"\"\n    from collections import Counter\n    \n    topic_counts = Counter()\n    #confidence_dist = Counter()\n    \n    multi_label_count = 0\n    no_label_count = 0\n    \n    for result in results:\n        predicted = result['predicted_topics']\n        \n        if len(predicted) == 0:\n            no_label_count += 1\n        elif len(predicted) > 1:\n            multi_label_count += 1\n        \n        for topic in predicted:\n            topic_counts[topic] += 1\n        \n        #confidence_dist[result['confidence']] += 1\n    \n    stats = {\n        'total_articles': len(results),\n        'articles_with_topics': len(results) - no_label_count,\n        'articles_without_topics': no_label_count,\n        'multi_label_articles': multi_label_count,\n        #'avg_topics_per_article': sum(len(r['predicted_topics']) for r in results) / len(results),\n        #'most_common_topics': topic_counts.most_common(10)\n    }\n    \n    print(f\"\\n{'='*80}\")\n    print(\"CLASSIFICATION RESULTS SUMMARY\")\n    print(f\"{'='*80}\")\n    print(f\"\\nTotal articles: {stats['total_articles']}\")\n    print(f\"Articles with topics: {stats['articles_with_topics']} ({stats['articles_with_topics']/stats['total_articles']*100:.1f}%)\")\n    print(f\"Multi-label articles: {stats['multi_label_articles']} ({stats['multi_label_articles']/stats['total_articles']*100:.1f}%)\")\n    #print(f\"Average topics per article: {stats['avg_topics_per_article']:.2f}\")\n    \n    print(f\"\\n{'='*80}\")\n    print(\"TOP 10 MOST FREQUENT TOPICS\")\n    print(f\"{'='*80}\")\n    #for topic, count in stats['most_common_topics']:\n        #print(f\"  {topic}: {count} articles ({count/stats['total_articles']*100:.1f}%)\")\n    \n    return stats\n\ndef save_results(results: List[Dict], output_path: str):\n    \"\"\"Save classification results to JSON file.\"\"\"\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(results, f, indent=2, ensure_ascii=False)\n    print(f\"\\n✓ Results saved to {output_path}\")\n\ndef export_to_csv(results: List[Dict], output_path: str):\n    \"\"\"Export results to CSV for easy analysis.\"\"\"\n    rows = []\n    for r in results:\n        rows.append({\n            'article_id': r['article_id'],\n            #'article_preview': r['article_text'],\n            'true_concepts': '|'.join(r['true_concepts']) if r['true_concepts'] else '',\n            'predicted_topics': '|'.join(r['predicted_topics']),\n            'num_predicted': len(r['predicted_topics']),\n            #'confidence': r['confidence'],\n            #'reasoning': r['reasoning']\n        })\n    \n    df = pd.DataFrame(rows)\n    df.to_csv(output_path, index=False, encoding='utf-8')\n    print(f\"✓ CSV exported to {output_path}\")\n\nprint(\"✓ Analysis functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:00:08.277739Z","iopub.execute_input":"2025-11-07T18:00:08.277967Z","iopub.status.idle":"2025-11-07T18:00:08.304711Z","shell.execute_reply.started":"2025-11-07T18:00:08.277953Z","shell.execute_reply":"2025-11-07T18:00:08.303933Z"}},"outputs":[{"name":"stdout","text":"✓ Analysis functions defined\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ============================================================================\n# RUN CLASSIFICATION ON ALL THE ARTICLES\n# ============================================================================\n\n# Initialize classifier\nclassifier = ArticleTopicClassifier(all_concepts)\n\n# Classify all the articles with KGs\nresults = classifier.classify_dataset(\n    articles=articles,\n    knowledge_graphs=knowledge_graphs,\n    max_articles=10000\n)\n\n# Analyze results\nstats = analyze_classification_results(results, all_concepts)\n\n# Save results\nsave_results(results, '/kaggle/working/topic_classification_results.json')\nexport_to_csv(results, '/kaggle/working/topic_classification_results.csv')\n\n# Show sample results\nprint(f\"\\n{'='*80}\")\nprint(\"SAMPLE RESULTS (First 3)\")\nprint(f\"{'='*80}\\n\")\n\nfor result in results[:3]:\n    print(f\"Article {result['article_id']}:\")\n    print(f\"  Preview: {result['article_text']}\")\n    print(f\"  True: {result['true_concepts']}\")\n    print(f\"  Predicted: {result['predicted_topics']}\")\n    #print(f\"  Confidence: {result['confidence']}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:00:08.305368Z","iopub.execute_input":"2025-11-07T18:00:08.305557Z","iopub.status.idle":"2025-11-07T18:21:24.057629Z","shell.execute_reply.started":"2025-11-07T18:00:08.305539Z","shell.execute_reply":"2025-11-07T18:21:24.056804Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier initialized with 21 possible topics\n\n================================================================================\nCLASSIFYING 380 ARTICLES\n================================================================================\n\nProcessing article 0... ✓ Found 2 topics\nProcessing article 1... ✓ Found 5 topics\nProcessing article 2... ✓ Found 5 topics\nProcessing article 3... ✓ Found 1 topics\nProcessing article 4... ✓ Found 2 topics\nProcessing article 5... ✓ Found 3 topics\nProcessing article 6... ✓ Found 1 topics\nProcessing article 7... ✓ Found 3 topics\nProcessing article 8... ✓ Found 4 topics\nProcessing article 9... ✓ Found 2 topics\nProcessing article 10... ✓ Found 1 topics\nProcessing article 11... ✓ Found 0 topics\nProcessing article 12... ✓ Found 1 topics\nProcessing article 13... ✓ Found 1 topics\nProcessing article 14... ✓ Found 2 topics\nProcessing article 15... ✓ Found 1 topics\nProcessing article 16... ✓ Found 2 topics\nProcessing article 17... ✓ Found 3 topics\nProcessing article 18... ✓ Found 2 topics\nProcessing article 19... ✓ Found 3 topics\nProcessing article 20... ✓ Found 3 topics\nProcessing article 21... ✓ Found 2 topics\nProcessing article 22... ✓ Found 1 topics\nProcessing article 23... ✓ Found 2 topics\nProcessing article 24... ✓ Found 2 topics\nProcessing article 25... ✓ Found 4 topics\nProcessing article 26... ✓ Found 2 topics\nProcessing article 27... ✓ Found 1 topics\nProcessing article 28... ✓ Found 3 topics\nProcessing article 29... ✓ Found 7 topics\nProcessing article 30... ✓ Found 0 topics\nProcessing article 31... ✓ Found 4 topics\nProcessing article 32... ✓ Found 5 topics\nProcessing article 33... ✓ Found 2 topics\nProcessing article 34... ✓ Found 3 topics\nProcessing article 35... ✓ Found 1 topics\nProcessing article 36... ✓ Found 3 topics\nProcessing article 37... ✓ Found 1 topics\nProcessing article 38... ✓ Found 2 topics\nProcessing article 39... ✓ Found 2 topics\nProcessing article 40... ✓ Found 1 topics\nProcessing article 41... ✓ Found 10 topics\nProcessing article 42... ✓ Found 1 topics\nProcessing article 43... ✓ Found 0 topics\nProcessing article 44... ✓ Found 3 topics\nProcessing article 45... ✓ Found 2 topics\nProcessing article 46... ✓ Found 1 topics\nProcessing article 47... ✓ Found 1 topics\nProcessing article 48... ✓ Found 1 topics\nProcessing article 49... ✓ Found 4 topics\nProcessing article 50... ✓ Found 5 topics\nProcessing article 51... ✓ Found 5 topics\nProcessing article 52... ✓ Found 2 topics\nProcessing article 53... ✓ Found 2 topics\nProcessing article 54... ✓ Found 2 topics\nProcessing article 55... ✓ Found 1 topics\nProcessing article 56... ✓ Found 1 topics\nProcessing article 57... ✓ Found 2 topics\nProcessing article 58... ✓ Found 1 topics\nProcessing article 59... ✓ Found 1 topics\nProcessing article 60... ✓ Found 1 topics\nProcessing article 61... ✓ Found 2 topics\nProcessing article 62... ✓ Found 4 topics\nProcessing article 63... ✓ Found 3 topics\nProcessing article 64... ✓ Found 2 topics\nProcessing article 65... ✓ Found 5 topics\nProcessing article 66... ✓ Found 0 topics\nProcessing article 67... ✓ Found 1 topics\nProcessing article 68... ✓ Found 2 topics\nProcessing article 69... ✓ Found 1 topics\nProcessing article 70... ✓ Found 1 topics\nProcessing article 71... ✓ Found 1 topics\nProcessing article 72... ✓ Found 1 topics\nProcessing article 73... ✓ Found 5 topics\nProcessing article 74... ✓ Found 4 topics\nProcessing article 75... ✓ Found 3 topics\nProcessing article 76... ✓ Found 5 topics\nProcessing article 77... ✓ Found 3 topics\nProcessing article 78... ✓ Found 2 topics\nProcessing article 79... ✓ Found 3 topics\nProcessing article 80... ✓ Found 3 topics\nProcessing article 81... ✓ Found 1 topics\nProcessing article 82... ✓ Found 5 topics\nProcessing article 83... ✓ Found 6 topics\nProcessing article 84... ✓ Found 0 topics\nProcessing article 85... ✓ Found 4 topics\nProcessing article 86... ✓ Found 5 topics\nProcessing article 87... ✓ Found 1 topics\nProcessing article 88... ✓ Found 3 topics\nProcessing article 89... ✓ Found 3 topics\nProcessing article 90... ✓ Found 3 topics\nProcessing article 91... ✓ Found 0 topics\nProcessing article 92... ✓ Found 3 topics\nProcessing article 93... ✓ Found 3 topics\nProcessing article 94... ✓ Found 2 topics\nProcessing article 95... ✓ Found 5 topics\nProcessing article 96... ✓ Found 2 topics\nProcessing article 97... ✓ Found 1 topics\nProcessing article 98... ✓ Found 1 topics\nProcessing article 99... ✓ Found 1 topics\nProcessing article 100... ✓ Found 2 topics\nProcessing article 101... ✓ Found 2 topics\nProcessing article 102... ✓ Found 2 topics\nProcessing article 103... ✓ Found 2 topics\nProcessing article 104... ✓ Found 2 topics\nProcessing article 105... ✓ Found 1 topics\nProcessing article 106... ✓ Found 3 topics\nProcessing article 107... ✓ Found 2 topics\nProcessing article 108... ✓ Found 1 topics\nProcessing article 109... ✓ Found 5 topics\nProcessing article 110... ✓ Found 1 topics\nProcessing article 111... ✓ Found 3 topics\nProcessing article 112... ✓ Found 1 topics\nProcessing article 113... ✓ Found 1 topics\nProcessing article 114... ✓ Found 1 topics\nProcessing article 115... ✓ Found 4 topics\nProcessing article 116... ✓ Found 5 topics\nProcessing article 117... ✓ Found 5 topics\nProcessing article 118... ✓ Found 3 topics\nProcessing article 119... ✓ Found 3 topics\nProcessing article 120... ✓ Found 1 topics\nProcessing article 121... ✓ Found 2 topics\nProcessing article 122... ✓ Found 3 topics\nProcessing article 123... ✓ Found 2 topics\nProcessing article 124... ✓ Found 5 topics\nProcessing article 125... ✓ Found 5 topics\nProcessing article 126... ✓ Found 4 topics\nProcessing article 127... ✓ Found 3 topics\nProcessing article 128... ✓ Found 2 topics\nProcessing article 129... ✓ Found 1 topics\nProcessing article 130... ✓ Found 1 topics\nProcessing article 131... ✓ Found 1 topics\nProcessing article 132... ✓ Found 3 topics\nProcessing article 133... ✓ Found 2 topics\nProcessing article 134... ✓ Found 4 topics\nProcessing article 135... ✓ Found 1 topics\nProcessing article 136... ✓ Found 3 topics\nProcessing article 137... ✓ Found 2 topics\nProcessing article 138... ✓ Found 2 topics\nProcessing article 139... ✓ Found 6 topics\nProcessing article 140... ✓ Found 1 topics\nProcessing article 141... ✓ Found 0 topics\nProcessing article 142... ✓ Found 1 topics\nProcessing article 143... ✓ Found 2 topics\nProcessing article 144... ✓ Found 2 topics\nProcessing article 145... ✓ Found 5 topics\nProcessing article 146... ✓ Found 2 topics\nProcessing article 147... ✓ Found 1 topics\nProcessing article 148... ✓ Found 2 topics\nProcessing article 149... ✓ Found 0 topics\nProcessing article 150... ✓ Found 5 topics\nProcessing article 151... ✓ Found 1 topics\nProcessing article 152... ✓ Found 3 topics\nProcessing article 153... ✓ Found 2 topics\nProcessing article 154... ✓ Found 5 topics\nProcessing article 155... ✓ Found 3 topics\nProcessing article 156... ✓ Found 2 topics\nProcessing article 157... ✓ Found 3 topics\nProcessing article 158... ✓ Found 2 topics\nProcessing article 159... ✓ Found 1 topics\nProcessing article 160... ✓ Found 3 topics\nProcessing article 161... ✓ Found 2 topics\nProcessing article 162... ✓ Found 2 topics\nProcessing article 163... ✓ Found 3 topics\nProcessing article 164... ✓ Found 3 topics\nProcessing article 165... ✓ Found 1 topics\nProcessing article 166... ✓ Found 1 topics\nProcessing article 167... ✓ Found 5 topics\nProcessing article 168... ✓ Found 1 topics\nProcessing article 169... ✓ Found 1 topics\nProcessing article 170... ✓ Found 2 topics\nProcessing article 171... ✓ Found 5 topics\nProcessing article 172... ✓ Found 2 topics\nProcessing article 173... ✓ Found 5 topics\nProcessing article 174... ✓ Found 2 topics\nProcessing article 175... ✓ Found 2 topics\nProcessing article 176... ✓ Found 1 topics\nProcessing article 177... ✓ Found 3 topics\nProcessing article 178... ✓ Found 5 topics\nProcessing article 179... ✓ Found 5 topics\nProcessing article 180... ✓ Found 3 topics\nProcessing article 181... ✓ Found 2 topics\nProcessing article 182... ✓ Found 5 topics\nProcessing article 183... ✓ Found 4 topics\nProcessing article 184... ✓ Found 2 topics\nProcessing article 185... ✓ Found 2 topics\nProcessing article 186... ✓ Found 3 topics\nProcessing article 187... ✓ Found 5 topics\nProcessing article 188... ✓ Found 5 topics\nProcessing article 189... ✓ Found 2 topics\nProcessing article 190... ✓ Found 1 topics\nProcessing article 191... ✓ Found 1 topics\nProcessing article 192... ✓ Found 2 topics\nProcessing article 193... ✓ Found 4 topics\nProcessing article 194... ✓ Found 2 topics\nProcessing article 195... ✓ Found 3 topics\nProcessing article 196... ✓ Found 2 topics\nProcessing article 197... ✓ Found 1 topics\nProcessing article 198... ✓ Found 1 topics\nProcessing article 199... ✓ Found 5 topics\nProcessing article 200... ✓ Found 2 topics\nProcessing article 201... ✓ Found 2 topics\nProcessing article 202... ✓ Found 3 topics\nProcessing article 203... ✓ Found 5 topics\nProcessing article 204... ✓ Found 1 topics\nProcessing article 205... ✓ Found 1 topics\nProcessing article 206... ✓ Found 2 topics\nProcessing article 207... ✓ Found 5 topics\nProcessing article 208... ✓ Found 0 topics\nProcessing article 209... ✓ Found 5 topics\nProcessing article 210... ✓ Found 5 topics\nProcessing article 211... ✓ Found 2 topics\nProcessing article 212... ✓ Found 1 topics\nProcessing article 213... ✓ Found 2 topics\nProcessing article 214... ✓ Found 0 topics\nProcessing article 215... ✓ Found 4 topics\nProcessing article 216... ✓ Found 2 topics\nProcessing article 217... ✓ Found 2 topics\nProcessing article 218... ✓ Found 3 topics\nProcessing article 219... ✓ Found 1 topics\nProcessing article 220... ✓ Found 4 topics\nProcessing article 221... ✓ Found 5 topics\nProcessing article 222... ✓ Found 2 topics\nProcessing article 223... ✓ Found 2 topics\nProcessing article 224... ✓ Found 1 topics\nProcessing article 225... ✓ Found 4 topics\nProcessing article 226... ✓ Found 3 topics\nProcessing article 227... ✓ Found 5 topics\nProcessing article 228... ✓ Found 2 topics\nProcessing article 229... ✓ Found 1 topics\nProcessing article 230... ✓ Found 0 topics\nProcessing article 231... ✓ Found 1 topics\nProcessing article 232... ✓ Found 2 topics\nProcessing article 233... ✓ Found 2 topics\nProcessing article 234... ✓ Found 4 topics\nProcessing article 235... ✓ Found 4 topics\nProcessing article 236... ✓ Found 2 topics\nProcessing article 237... ✓ Found 4 topics\nProcessing article 238... ✓ Found 2 topics\nProcessing article 239... ✓ Found 5 topics\nProcessing article 240... ✓ Found 5 topics\nProcessing article 241... ✓ Found 3 topics\nProcessing article 242... ✓ Found 2 topics\nProcessing article 243... ✓ Found 5 topics\nProcessing article 244... ✓ Found 1 topics\nProcessing article 245... ✓ Found 0 topics\nProcessing article 246... ✓ Found 3 topics\nProcessing article 247... ✓ Found 2 topics\nProcessing article 248... ✓ Found 2 topics\nProcessing article 249... ✓ Found 5 topics\nProcessing article 250... ✓ Found 3 topics\nProcessing article 251... ✓ Found 3 topics\nProcessing article 252... ✓ Found 1 topics\nProcessing article 253... ✓ Found 5 topics\nProcessing article 254... ✓ Found 2 topics\nProcessing article 255... ✓ Found 4 topics\nProcessing article 256... ✓ Found 3 topics\nProcessing article 257... ✓ Found 5 topics\nProcessing article 258... ✓ Found 3 topics\nProcessing article 259... ✓ Found 1 topics\nProcessing article 260... ✓ Found 2 topics\nProcessing article 261... ✓ Found 1 topics\nProcessing article 262... ✓ Found 2 topics\nProcessing article 263... ✓ Found 2 topics\nProcessing article 264... ✓ Found 2 topics\nProcessing article 265... ✓ Found 3 topics\nProcessing article 266... ✓ Found 2 topics\nProcessing article 267... ✓ Found 1 topics\nProcessing article 268... ✓ Found 3 topics\nProcessing article 269... ✓ Found 3 topics\nProcessing article 270... ✓ Found 5 topics\nProcessing article 271... ✓ Found 2 topics\nProcessing article 272... ✓ Found 2 topics\nProcessing article 273... ✓ Found 4 topics\nProcessing article 274... ✓ Found 3 topics\nProcessing article 275... ✓ Found 3 topics\nProcessing article 276... ✓ Found 5 topics\nProcessing article 277... ✓ Found 1 topics\nProcessing article 278... ✓ Found 1 topics\nProcessing article 279... ✓ Found 3 topics\nProcessing article 280... ✓ Found 1 topics\nProcessing article 281... ✓ Found 1 topics\nProcessing article 282... ✓ Found 3 topics\nProcessing article 283... ✓ Found 1 topics\nProcessing article 284... ✓ Found 3 topics\nProcessing article 285... ✓ Found 4 topics\nProcessing article 286... ✓ Found 2 topics\nProcessing article 287... ✓ Found 3 topics\nProcessing article 288... ✓ Found 2 topics\nProcessing article 289... ✓ Found 2 topics\nProcessing article 290... ✓ Found 4 topics\nProcessing article 291... ✓ Found 2 topics\nProcessing article 292... ✓ Found 2 topics\nProcessing article 293... ✓ Found 5 topics\nProcessing article 294... ✓ Found 1 topics\nProcessing article 295... ✓ Found 0 topics\nProcessing article 296... ✓ Found 5 topics\nProcessing article 297... ✓ Found 1 topics\nProcessing article 298... ✓ Found 2 topics\nProcessing article 299... ✓ Found 5 topics\nProcessing article 300... ✓ Found 1 topics\nProcessing article 301... ✓ Found 1 topics\nProcessing article 302... ✓ Found 1 topics\nProcessing article 303... ✓ Found 2 topics\nProcessing article 304... ✓ Found 4 topics\nProcessing article 305... ✓ Found 2 topics\nProcessing article 306... ✓ Found 10 topics\nProcessing article 307... ✓ Found 2 topics\nProcessing article 308... ✓ Found 1 topics\nProcessing article 309... ✓ Found 4 topics\nProcessing article 310... ✓ Found 4 topics\nProcessing article 311... ✓ Found 5 topics\nProcessing article 312... ✓ Found 0 topics\nProcessing article 313... ✓ Found 5 topics\nProcessing article 314... ✓ Found 4 topics\nProcessing article 315... ✓ Found 2 topics\nProcessing article 316... ✓ Found 2 topics\nProcessing article 317... ✓ Found 4 topics\nProcessing article 318... ✓ Found 3 topics\nProcessing article 319... ✓ Found 1 topics\nProcessing article 320... ✓ Found 5 topics\nProcessing article 321... ✓ Found 1 topics\nProcessing article 322... ✓ Found 2 topics\nProcessing article 323... ✓ Found 5 topics\nProcessing article 324... ✓ Found 0 topics\nProcessing article 325... ✓ Found 3 topics\nProcessing article 326... ✓ Found 2 topics\nProcessing article 327... ✓ Found 0 topics\nProcessing article 328... ✓ Found 1 topics\nProcessing article 329... ✓ Found 2 topics\nProcessing article 330... ✓ Found 1 topics\nProcessing article 331... ✓ Found 3 topics\nProcessing article 332... ✓ Found 5 topics\nProcessing article 333... ✓ Found 2 topics\nProcessing article 334... ✓ Found 1 topics\nProcessing article 335... ✓ Found 2 topics\nProcessing article 336... ✓ Found 3 topics\nProcessing article 337... ✓ Found 1 topics\nProcessing article 338... ✓ Found 3 topics\nProcessing article 339... ✓ Found 5 topics\nProcessing article 340... ✓ Found 1 topics\nProcessing article 341... ✓ Found 2 topics\nProcessing article 342... ✓ Found 2 topics\nProcessing article 343... ✓ Found 2 topics\nProcessing article 344... ✓ Found 1 topics\nProcessing article 345... ✓ Found 1 topics\nProcessing article 346... ✓ Found 4 topics\nProcessing article 347... ✓ Found 2 topics\nProcessing article 348... ✓ Found 3 topics\nProcessing article 349... ✓ Found 2 topics\nProcessing article 350... ✓ Found 1 topics\nProcessing article 351... ✓ Found 2 topics\nProcessing article 352... ✓ Found 2 topics\nProcessing article 353... ✓ Found 5 topics\nProcessing article 354... ✓ Found 3 topics\nProcessing article 355... ✓ Found 1 topics\nProcessing article 356... ✓ Found 1 topics\nProcessing article 357... ✓ Found 1 topics\nProcessing article 358... ✓ Found 2 topics\nProcessing article 359... ✓ Found 1 topics\nProcessing article 360... ✓ Found 3 topics\nProcessing article 361... ✓ Found 5 topics\nProcessing article 362... ✓ Found 3 topics\nProcessing article 363... ✓ Found 1 topics\nProcessing article 364... ✓ Found 1 topics\nProcessing article 365... ✓ Found 3 topics\nProcessing article 366... ✓ Found 1 topics\nProcessing article 367... ✓ Found 2 topics\nProcessing article 368... ✓ Found 1 topics\nProcessing article 369... ✓ Found 2 topics\nProcessing article 370... ✓ Found 1 topics\nProcessing article 371... ✓ Found 1 topics\nProcessing article 372... ✓ Found 2 topics\nProcessing article 373... ✓ Found 3 topics\nProcessing article 374... ✓ Found 1 topics\nProcessing article 375... ✓ Found 5 topics\nProcessing article 376... ✓ Found 5 topics\nProcessing article 377... ✓ Found 2 topics\nProcessing article 378... ✓ Found 4 topics\nProcessing article 379... ✓ Found 4 topics\n\n================================================================================\nCLASSIFICATION RESULTS SUMMARY\n================================================================================\n\nTotal articles: 380\nArticles with topics: 364 (95.8%)\nMulti-label articles: 265 (69.7%)\n\n================================================================================\nTOP 10 MOST FREQUENT TOPICS\n================================================================================\n\n✓ Results saved to /kaggle/working/topic_classification_results.json\n✓ CSV exported to /kaggle/working/topic_classification_results.csv\n\n================================================================================\nSAMPLE RESULTS (First 3)\n================================================================================\n\nArticle 0:\n  Preview: the best 4mp compact digital available  \n...\n  True: []\n  Predicted: ['camera', 'design']\n\nArticle 1:\n  Preview: this camera is perfect for an enthusiastic amateur photographer . \n...\n  True: ['camera']\n  Predicted: ['camera', 'feature', 'focus', 'lens', 'picture']\n\nArticle 2:\n  Preview: the pictures are razor-sharp , even in macro . \n...\n  True: ['lens']\n  Predicted: ['picture', 'camera', 'feature', 'focus', 'lens']\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION: Compare Predicted vs True Concepts\n# ============================================================================\n\ndef evaluate_classification(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate accuracy metrics by comparing predicted topics with true concepts.\n    \"\"\"\n    total = len(results)\n    exact_match = 0  # All predicted topics match exactly with true concepts\n    partial_match = 0  # At least one topic matches\n    no_match = 0  # No topics match\n    no_true_labels = 0  # Articles with no true concepts\n    \n    all_precisions = []\n    all_recalls = []\n    all_f1s = []\n    \n    correct_predictions = []\n    incorrect_predictions = []\n    \n    for result in results:\n        true_set = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_set = set(result['predicted_topics'])\n        \n        # Skip articles with no true labels\n        if len(true_set) == 0:\n            no_true_labels += 1\n            continue\n        \n        # Find intersection\n        intersection = true_set & pred_set\n        \n        # Exact match\n        if true_set == pred_set and len(true_set) > 0:\n            exact_match += 1\n            correct_predictions.append(result)\n        elif len(intersection) > 0:\n            partial_match += 1\n        else:\n            no_match += 1\n            incorrect_predictions.append(result)\n        \n        # Calculate precision, recall, F1\n        if len(pred_set) > 0:\n            precision = len(intersection) / len(pred_set)\n            all_precisions.append(precision)\n        else:\n            all_precisions.append(0)\n        \n        if len(true_set) > 0:\n            recall = len(intersection) / len(true_set)\n            all_recalls.append(recall)\n        else:\n            all_recalls.append(0)\n        \n        if all_precisions[-1] + all_recalls[-1] > 0:\n            f1 = 2 * (all_precisions[-1] * all_recalls[-1]) / (all_precisions[-1] + all_recalls[-1])\n            all_f1s.append(f1)\n        else:\n            all_f1s.append(0)\n    \n    articles_with_true_labels = total - no_true_labels\n    \n    metrics = {\n        'total_articles': total,\n        'articles_with_true_labels': articles_with_true_labels,\n        'articles_without_true_labels': no_true_labels,\n        'exact_matches': exact_match,\n        'partial_matches': partial_match,\n        'no_matches': no_match,\n        'exact_match_rate': exact_match / articles_with_true_labels if articles_with_true_labels > 0 else 0,\n        'partial_match_rate': partial_match / articles_with_true_labels if articles_with_true_labels > 0 else 0,\n        'avg_precision': np.mean(all_precisions) if all_precisions else 0,\n        'avg_recall': np.mean(all_recalls) if all_recalls else 0,\n        'avg_f1': np.mean(all_f1s) if all_f1s else 0,\n        'correct_examples': correct_predictions[:3],\n        'incorrect_examples': incorrect_predictions[:3]\n    }\n    \n    # Print results\n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS\")\n    print(f\"{'='*80}\")\n    print(f\"\\nTotal articles evaluated: {articles_with_true_labels}\")\n    print(f\"Articles without labels (skipped): {no_true_labels}\")\n    print(f\"\\n📊 ACCURACY:\")\n    print(f\"  ✓ Exact matches: {exact_match} ({metrics['exact_match_rate']*100:.1f}%)\")\n    print(f\"  ~ Partial matches: {partial_match} ({metrics['partial_match_rate']*100:.1f}%)\")\n    print(f\"  ✗ No matches: {no_match} ({no_match/articles_with_true_labels*100:.1f}%)\")\n    print(f\"\\n📈 PERFORMANCE METRICS:\")\n    print(f\"  Precision: {metrics['avg_precision']:.3f} (how many predicted topics were correct)\")\n    print(f\"  Recall: {metrics['avg_recall']:.3f} (how many true topics were found)\")\n    print(f\"  F1 Score: {metrics['avg_f1']:.3f} (overall accuracy)\")\n    \n    return metrics\n\n\n# Run evaluation\nprint(\"\\nEvaluating classification accuracy...\")\neval_metrics = evaluate_classification(results)\n\n# Show correct examples\nprint(f\"\\n{'='*80}\")\nprint(\"✓ CORRECTLY CLASSIFIED EXAMPLES\")\nprint(f\"{'='*80}\")\nfor ex in eval_metrics['correct_examples']:\n    print(f\"\\nArticle {ex['article_id']}:\")\n    print(f\"  True: {ex['true_concepts']}\")\n    print(f\"  Predicted: {ex['predicted_topics']}\")\n    print(f\"  ✓ EXACT MATCH!\")\n\n# Show incorrect examples\nprint(f\"\\n{'='*80}\")\nprint(\"✗ INCORRECTLY CLASSIFIED EXAMPLES\")\nprint(f\"{'='*80}\")\nfor ex in eval_metrics['incorrect_examples']:\n    print(f\"\\nArticle {ex['article_id']}:\")\n    print(f\"  True: {ex['true_concepts']}\")\n    print(f\"  Predicted: {ex['predicted_topics']}\")\n    #print(f\"  Reasoning: {ex['reasoning'][:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:21:24.061234Z","iopub.execute_input":"2025-11-07T18:21:24.061481Z","iopub.status.idle":"2025-11-07T18:21:24.080039Z","shell.execute_reply.started":"2025-11-07T18:21:24.061463Z","shell.execute_reply":"2025-11-07T18:21:24.076693Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating classification accuracy...\n\n================================================================================\nEVALUATION METRICS\n================================================================================\n\nTotal articles evaluated: 267\nArticles without labels (skipped): 113\n\n📊 ACCURACY:\n  ✓ Exact matches: 85 (31.8%)\n  ~ Partial matches: 152 (56.9%)\n  ✗ No matches: 30 (11.2%)\n\n📈 PERFORMANCE METRICS:\n  Precision: 0.608 (how many predicted topics were correct)\n  Recall: 0.806 (how many true topics were found)\n  F1 Score: 0.645 (overall accuracy)\n\n================================================================================\n✓ CORRECTLY CLASSIFIED EXAMPLES\n================================================================================\n\nArticle 3:\n  True: ['size']\n  Predicted: ['size']\n  ✓ EXACT MATCH!\n\nArticle 6:\n  True: ['manual']\n  Predicted: ['manual']\n  ✓ EXACT MATCH!\n\nArticle 10:\n  True: ['camera']\n  Predicted: ['camera']\n  ✓ EXACT MATCH!\n\n================================================================================\n✗ INCORRECTLY CLASSIFIED EXAMPLES\n================================================================================\n\nArticle 36:\n  True: ['memory']\n  Predicted: ['camera', 'print', 'use']\n\nArticle 39:\n  True: ['control']\n  Predicted: ['memory', 'size']\n\nArticle 66:\n  True: ['camera']\n  Predicted: []\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION METRICS (Following the Paper's Methodology)\n# ============================================================================\n\ndef calculate_metrics_paper_style(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate evaluation metrics following the paper's approach:\n    \"For each article, the model inferred topic(s) were compared against \n    the list of 'gold' topic(s) to compute the true positive, false positive, \n    and false negative statistics for that article. Then, all such statistics \n    for all the articles in a dataset were aggregated and used to compute \n    the final Precision, Recall, and micro-averaged F1 score.\"\n    \n    Reference: Section 5.4 of the paper\n    \"\"\"\n    # Aggregate statistics across all articles\n    total_tp = 0  # True Positives\n    total_fp = 0  # False Positives\n    total_fn = 0  # False Negatives\n    \n    articles_evaluated = 0\n    articles_skipped = 0\n    \n    for result in results:\n        # Get true and predicted topics (lowercased and stripped)\n        true_topics = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_topics = set([t.lower().strip() for t in result['predicted_topics']])\n        \n        # Skip articles with no ground truth labels\n        if len(true_topics) == 0:\n            articles_skipped += 1\n            continue\n        \n        articles_evaluated += 1\n        \n        # Calculate TP, FP, FN for this article\n        tp = len(true_topics & pred_topics)  # Intersection\n        fp = len(pred_topics - true_topics)  # Predicted but not true\n        fn = len(true_topics - pred_topics)  # True but not predicted\n        \n        # Aggregate\n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n    \n    # Calculate micro-averaged metrics\n    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    metrics = {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'total_tp': total_tp,\n        'total_fp': total_fp,\n        'total_fn': total_fn,\n        'articles_evaluated': articles_evaluated,\n        'articles_skipped': articles_skipped\n    }\n    \n    # Print results\n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS (Paper's Methodology)\")\n    print(f\"{'='*80}\")\n    print(f\"\\nArticles evaluated: {articles_evaluated}\")\n    print(f\"Articles skipped (no ground truth): {articles_skipped}\")\n    print(f\"\\nAggregated Statistics:\")\n    print(f\"  True Positives (TP): {total_tp}\")\n    print(f\"  False Positives (FP): {total_fp}\")\n    print(f\"  False Negatives (FN): {total_fn}\")\n    print(f\"\\n📊 MICRO-AVERAGED METRICS:\")\n    print(f\"  Precision: {precision:.3f}\")\n    print(f\"  Recall: {recall:.3f}\")\n    print(f\"  F1 Score: {f1_score:.3f}\")\n    \n    return metrics\n\n\n# Run evaluation using the paper's methodology\nprint(\"\\nEvaluating using paper's methodology...\")\npaper_metrics = calculate_metrics_paper_style(results)\n\n# Compare with their baselines (from Table 6 in the paper)\nprint(f\"\\n{'='*80}\")\nprint(\"COMPARISON WITH PAPER'S BASELINES\")\nprint(f\"{'='*80}\")\nprint(f\"This model's F1 Score: {paper_metrics['f1_score']:.3f}\")\nprint(f\"\\nPaper's Results:\")\nprint(f\"  GFLM-S baseline: 0.532\")\nprint(f\"  GFLM-W baseline: 0.530\")\nprint(f\"  SBERT (best mid encoder): 0.594\")\nprint(f\"  ChatGPT-3.5 (best overall): 0.606\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:21:24.081308Z","iopub.execute_input":"2025-11-07T18:21:24.082211Z","iopub.status.idle":"2025-11-07T18:21:24.102037Z","shell.execute_reply.started":"2025-11-07T18:21:24.082182Z","shell.execute_reply":"2025-11-07T18:21:24.101237Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating using paper's methodology...\n\n================================================================================\nEVALUATION METRICS (Paper's Methodology)\n================================================================================\n\nArticles evaluated: 267\nArticles skipped (no ground truth): 113\n\nAggregated Statistics:\n  True Positives (TP): 313\n  False Positives (FP): 325\n  False Negatives (FN): 82\n\n📊 MICRO-AVERAGED METRICS:\n  Precision: 0.491\n  Recall: 0.792\n  F1 Score: 0.606\n\n================================================================================\nCOMPARISON WITH PAPER'S BASELINES\n================================================================================\nThis model's F1 Score: 0.606\n\nPaper's Results:\n  GFLM-S baseline: 0.532\n  GFLM-W baseline: 0.530\n  SBERT (best mid encoder): 0.594\n  ChatGPT-3.5 (best overall): 0.606\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: LOAD KEYWORDS\n# ============================================================================\n\n# Load the keyword file\nwith open('/kaggle/input/keywords/Keyword_Canon.json', 'r') as f:\n    keyword_data = json.load(f)\n\n# Parse into dictionary: {topic_name: [keywords]}\nconcept_keywords = {}\n\nfor item in keyword_data:\n    keywords_list = item['Keyword']\n    topic_name = keywords_list[0].lower()  # First item is topic name\n    related_keywords = [kw.lower() for kw in keywords_list[1:]]  # Rest are keywords\n    concept_keywords[topic_name] = related_keywords\n\nprint(f\"\\n{'='*80}\")\nprint(\"LOADED CONCEPT KEYWORDS\")\nprint(f\"{'='*80}\")\nprint(f\"Total concepts: {len(concept_keywords)}\\n\")\n\n# Show first 3 examples\nfor i, (topic, keywords) in enumerate(list(concept_keywords.items())[:3]):\n    print(f\"{i+1}. {topic}:\")\n    print(f\"   Keywords: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:21:24.102866Z","iopub.execute_input":"2025-11-07T18:21:24.103050Z","iopub.status.idle":"2025-11-07T18:21:24.131206Z","shell.execute_reply.started":"2025-11-07T18:21:24.103031Z","shell.execute_reply":"2025-11-07T18:21:24.130063Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nLOADED CONCEPT KEYWORDS\n================================================================================\nTotal concepts: 24\n\n1. camera:\n   Keywords: canon\n2. picture:\n   Keywords: image, shoot\n3. use:\n   Keywords: function\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: BUILD CONCEPT KNOWLEDGE GRAPHS\n# ============================================================================\n\nclass ConceptKGBuilder:\n    \"\"\"Build knowledge graphs for concepts using topic names and keywords.\"\"\"\n    \n    def __init__(self):\n        self.concept_kgs = {}\n    \n    def build_concept_kg(self, topic_name: str, keywords: List[str]) -> nx.DiGraph:\n        \"\"\"Build a KG for a single concept.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add topic as central node\n        G.add_node(topic_name, node_type='topic')\n        \n        # Add keywords and connect to topic\n        for keyword in keywords:\n            G.add_node(keyword, node_type='keyword')\n            G.add_edge(topic_name, keyword, relation='has_keyword')\n        \n        # Connect keywords to each other (co-occurrence)\n        for i, kw1 in enumerate(keywords):\n            for kw2 in keywords[i+1:]:\n                G.add_edge(kw1, kw2, relation='co_occurs')\n        \n        return G\n    \n    def build_all_concept_kgs(self, concept_keywords: Dict[str, List[str]]) -> Dict[str, nx.DiGraph]:\n        \"\"\"Build KGs for all concepts.\"\"\"\n        for topic, keywords in concept_keywords.items():\n            self.concept_kgs[topic] = self.build_concept_kg(topic, keywords)\n        \n        print(f\"\\n{'='*80}\")\n        print(\"BUILT CONCEPT KNOWLEDGE GRAPHS\")\n        print(f\"{'='*80}\")\n        print(f\"Total concept KGs: {len(self.concept_kgs)}\")\n        \n        # Show stats\n        for topic, kg in list(self.concept_kgs.items())[:3]:\n            print(f\"  {topic}: {kg.number_of_nodes()} nodes, {kg.number_of_edges()} edges\")\n        \n        return self.concept_kgs\n\n# Build concept KGs\nkg_builder = ConceptKGBuilder()\nconcept_kgs = kg_builder.build_all_concept_kgs(concept_keywords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:21:24.132287Z","iopub.execute_input":"2025-11-07T18:21:24.132620Z","iopub.status.idle":"2025-11-07T18:21:24.143347Z","shell.execute_reply.started":"2025-11-07T18:21:24.132595Z","shell.execute_reply":"2025-11-07T18:21:24.142168Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nBUILT CONCEPT KNOWLEDGE GRAPHS\n================================================================================\nTotal concept KGs: 24\n  camera: 2 nodes, 1 edges\n  picture: 3 nodes, 3 edges\n  use: 2 nodes, 1 edges\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: GRAPH EMBEDDER\n# ============================================================================\n\nclass GraphEmbedder:\n    \"\"\"Embed graphs using Graph2Vec.\"\"\"\n    \n    def __init__(self, dimensions=128, wl_iterations=3):\n        \"\"\"\n        Args:\n            dimensions: Embedding dimension\n            wl_iterations: Weisfeiler-Lehman iterations\n        \"\"\"\n        self.dimensions = dimensions\n        self.model = Graph2Vec(\n            dimensions=dimensions,\n            wl_iterations=wl_iterations,\n            epochs=100,\n            min_count=1\n        )\n        print(f\"\\n✓ Initialized Graph2Vec (dim={dimensions}, WL={wl_iterations})\")\n    \n    def embed_graphs(self, graphs: List[nx.Graph]) -> np.ndarray:\n        \"\"\"\n        Embed multiple graphs.\n        \n        Args:\n            graphs: List of NetworkX graphs\n        \n        Returns:\n            Embedding matrix (n_graphs × dimensions)\n        \"\"\"\n        # Convert to undirected (Graph2Vec requires undirected)\n        undirected = [G.to_undirected() if G.is_directed() else G for G in graphs]\n        \n        # Fit and get embeddings\n        self.model.fit(undirected)\n        embeddings = self.model.get_embedding()\n        \n        return embeddings\n    \n    def embed_single_graph(self, G: nx.Graph, reference_graphs: List[nx.Graph]) -> np.ndarray:\n        \"\"\"\n        Embed a single new graph using reference graphs.\n        \n        Args:\n            G: Graph to embed\n            reference_graphs: Graphs to fit model on\n        \n        Returns:\n            Single embedding vector\n        \"\"\"\n        # Combine reference graphs with new graph\n        all_graphs = reference_graphs + [G]\n        embeddings = self.embed_graphs(all_graphs)\n        \n        # Return embedding of the last graph (the new one)\n        return embeddings[-1]\n\nprint(\"✓ GraphEmbedder class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:21:24.144103Z","iopub.execute_input":"2025-11-07T18:21:24.144345Z","iopub.status.idle":"2025-11-07T18:21:24.165372Z","shell.execute_reply.started":"2025-11-07T18:21:24.144330Z","shell.execute_reply":"2025-11-07T18:21:24.163981Z"}},"outputs":[{"name":"stdout","text":"✓ GraphEmbedder class defined\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: GRAPH-BASED TOPIC CLASSIFIER\n# ============================================================================\n\nclass GraphTopicClassifier:\n    \"\"\"Classify articles using graph embeddings.\"\"\"\n    \n    def __init__(self, \n                 concept_kgs: Dict[str, nx.DiGraph],\n                 embedding_dim: int = 128,\n                 threshold: float = 0.3):\n        \"\"\"\n        Initialize classifier.\n        \n        Args:\n            concept_kgs: Dictionary of concept KGs\n            embedding_dim: Embedding dimension\n            threshold: Similarity threshold for topic assignment\n        \"\"\"\n        self.concept_kgs = concept_kgs\n        self.threshold = threshold\n        \n        print(f\"\\n{'='*80}\")\n        print(\"INITIALIZING GRAPH-BASED CLASSIFIER\")\n        print(f\"{'='*80}\")\n        \n        # Initialize embedder\n        self.embedder = GraphEmbedder(dimensions=embedding_dim)\n        \n        # Prepare concept graphs\n        self.concept_names = list(concept_kgs.keys())\n        concept_graphs = [concept_kgs[name] for name in self.concept_names]\n        \n        # Embed all concept graphs\n        print(\"\\nEmbedding concept knowledge graphs...\")\n        self.concept_embeddings = self.embedder.embed_graphs(concept_graphs)\n        \n        print(f\"✓ Embedded {len(self.concept_names)} concept graphs\")\n        print(f\"  Embedding shape: {self.concept_embeddings.shape}\")\n        print(f\"  Similarity threshold: {threshold}\")\n        \n        # Store as dictionary\n        self.concept_embedding_dict = {\n            name: self.concept_embeddings[i] \n            for i, name in enumerate(self.concept_names)\n        }\n    \n    def _dict_to_graph(self, kg_dict: Dict) -> nx.DiGraph:\n        \"\"\"Convert KG dictionary to NetworkX graph.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add nodes from entities\n        entities = kg_dict.get('entities', [])\n        for entity in entities:\n            G.add_node(entity)\n        \n        # Add edges from relations\n        relations = kg_dict.get('relations', [])\n        for relation in relations:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                G.add_edge(source, target, relation=rel_type)\n        \n        return G\n    \n    def classify_article(self, article_kg: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        # Convert to graph\n        G_article = self._dict_to_graph(article_kg)\n        \n        # Skip if empty\n        if G_article.number_of_nodes() == 0:\n            return {\n                'predicted_topics': [],\n                'similarities': {},\n                'max_similarity': 0.0\n            }\n        \n        # Embed article graph\n        reference_graphs = [self.concept_kgs[name] for name in self.concept_names]\n        article_embedding = self.embedder.embed_single_graph(\n            G_article, \n            reference_graphs=reference_graphs\n        )\n        \n        # Compute similarities with all concepts\n        similarities = {}\n        for topic_name in self.concept_names:\n            concept_emb = self.concept_embedding_dict[topic_name]\n            sim = cosine_similarity([article_embedding], [concept_emb])[0][0]\n            similarities[topic_name] = float(sim)\n        \n        # Assign topics above threshold\n        predicted_topics = [\n            topic for topic, sim in similarities.items() \n            if sim >= self.threshold\n        ]\n        \n        # Sort by similarity\n        predicted_topics = sorted(\n            predicted_topics, \n            key=lambda t: similarities[t], \n            reverse=True\n        )\n        \n        return {\n            'predicted_topics': predicted_topics,\n            'similarities': similarities,\n            'max_similarity': max(similarities.values()) if similarities else 0.0\n        }\n    \n    def classify_dataset(self, \n                        articles: List[Dict],\n                        knowledge_graphs: List[Dict]) -> List[Dict]:\n        \"\"\"Classify all articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(knowledge_graphs)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx in range(len(knowledge_graphs)):\n            if idx % 20 == 0:\n                print(f\"Processing articles {idx}-{min(idx+20, len(knowledge_graphs))}...\")\n            \n            article = articles[idx] if idx < len(articles) else {}\n            article_kg = kg_dict.get(idx, {'entities': [], 'relations': []})\n            \n            # Skip if no KG\n            if article_kg.get('num_edges', 0) == 0:\n                results.append({\n                    'article_id': idx,\n                    'predicted_topics': [],\n                    'similarities': {},\n                    'max_similarity': 0.0,\n                    'true_concepts': article.get('Concept', [])\n                })\n                continue\n            \n            # Classify\n            classification = self.classify_article(article_kg)\n            \n            result = {\n                'article_id': idx,\n                'predicted_topics': classification['predicted_topics'],\n                'similarities': classification['similarities'],\n                'max_similarity': classification['max_similarity'],\n                'true_concepts': article.get('Concept', [])\n            }\n            \n            results.append(result)\n        \n        print(\"\\n✓ Classification complete!\")\n        return results\n\nprint(\"✓ GraphTopicClassifier class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:21:24.166209Z","iopub.execute_input":"2025-11-07T18:21:24.166454Z","iopub.status.idle":"2025-11-07T18:21:24.183581Z","shell.execute_reply.started":"2025-11-07T18:21:24.166437Z","shell.execute_reply":"2025-11-07T18:21:24.182856Z"}},"outputs":[{"name":"stdout","text":"✓ GraphTopicClassifier class defined\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: GRAPH-BASED TOPIC CLASSIFIER\n# ============================================================================\n\nclass GraphTopicClassifier:\n    \"\"\"Classify articles using graph embeddings.\"\"\"\n    \n    def __init__(self, \n                 concept_kgs: Dict[str, nx.DiGraph],\n                 embedding_dim: int = 128,\n                 threshold: float = 0.3):\n        \"\"\"\n        Initialize classifier.\n        \n        Args:\n            concept_kgs: Dictionary of concept KGs\n            embedding_dim: Embedding dimension\n            threshold: Similarity threshold for topic assignment\n        \"\"\"\n        self.concept_kgs = concept_kgs\n        self.threshold = threshold\n        \n        print(f\"\\n{'='*80}\")\n        print(\"INITIALIZING GRAPH-BASED CLASSIFIER\")\n        print(f\"{'='*80}\")\n        \n        # Initialize embedder\n        self.embedder = GraphEmbedder(dimensions=embedding_dim)\n        \n        # Prepare concept graphs\n        self.concept_names = list(concept_kgs.keys())\n        concept_graphs = [concept_kgs[name] for name in self.concept_names]\n        \n        # Embed all concept graphs\n        print(\"\\nEmbedding concept knowledge graphs...\")\n        self.concept_embeddings = self.embedder.embed_graphs(concept_graphs)\n        \n        print(f\"✓ Embedded {len(self.concept_names)} concept graphs\")\n        print(f\"  Embedding shape: {self.concept_embeddings.shape}\")\n        print(f\"  Similarity threshold: {threshold}\")\n        \n        # Store as dictionary\n        self.concept_embedding_dict = {\n            name: self.concept_embeddings[i] \n            for i, name in enumerate(self.concept_names)\n        }\n    \n    def _dict_to_graph(self, kg_dict: Dict) -> nx.DiGraph:\n        \"\"\"Convert KG dictionary to NetworkX graph.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add nodes from entities\n        entities = kg_dict.get('entities', [])\n        for entity in entities:\n            G.add_node(entity)\n        \n        # Add edges from relations\n        relations = kg_dict.get('relations', [])\n        for relation in relations:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                G.add_edge(source, target, relation=rel_type)\n        \n        return G\n    \n    def classify_article(self, article_kg: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        # Convert to graph\n        G_article = self._dict_to_graph(article_kg)\n        \n        # Skip if empty\n        if G_article.number_of_nodes() == 0:\n            return {\n                'predicted_topics': [],\n                'similarities': {},\n                'max_similarity': 0.0\n            }\n        \n        # Embed article graph\n        reference_graphs = [self.concept_kgs[name] for name in self.concept_names]\n        article_embedding = self.embedder.embed_single_graph(\n            G_article, \n            reference_graphs=reference_graphs\n        )\n        \n        # Compute similarities with all concepts\n        similarities = {}\n        for topic_name in self.concept_names:\n            concept_emb = self.concept_embedding_dict[topic_name]\n            sim = cosine_similarity([article_embedding], [concept_emb])[0][0]\n            similarities[topic_name] = float(sim)\n        \n        # Assign topics above threshold\n        predicted_topics = [\n            topic for topic, sim in similarities.items() \n            if sim >= self.threshold\n        ]\n        \n        # Sort by similarity\n        predicted_topics = sorted(\n            predicted_topics, \n            key=lambda t: similarities[t], \n            reverse=True\n        )\n        \n        return {\n            'predicted_topics': predicted_topics,\n            'similarities': similarities,\n            'max_similarity': max(similarities.values()) if similarities else 0.0\n        }\n    \n    def classify_dataset(self, \n                        articles: List[Dict],\n                        knowledge_graphs: List[Dict]) -> List[Dict]:\n        \"\"\"Classify all articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(knowledge_graphs)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx in range(len(knowledge_graphs)):\n            if idx % 20 == 0:\n                print(f\"Processing articles {idx}-{min(idx+20, len(knowledge_graphs))}...\")\n            \n            article = articles[idx] if idx < len(articles) else {}\n            article_kg = kg_dict.get(idx, {'entities': [], 'relations': []})\n            \n            # Skip if no KG\n            if article_kg.get('num_edges', 0) == 0:\n                results.append({\n                    'article_id': idx,\n                    'predicted_topics': [],\n                    'similarities': {},\n                    'max_similarity': 0.0,\n                    'true_concepts': article.get('Concept', [])\n                })\n                continue\n            \n            # Classify\n            classification = self.classify_article(article_kg)\n            \n            result = {\n                'article_id': idx,\n                'predicted_topics': classification['predicted_topics'],\n                'similarities': classification['similarities'],\n                'max_similarity': classification['max_similarity'],\n                'true_concepts': article.get('Concept', [])\n            }\n            \n            results.append(result)\n        \n        print(\"\\n✓ Classification complete!\")\n        return results\n\nprint(\"✓ GraphTopicClassifier class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:21:24.184208Z","iopub.execute_input":"2025-11-07T18:21:24.184400Z","iopub.status.idle":"2025-11-07T18:21:24.208584Z","shell.execute_reply.started":"2025-11-07T18:21:24.184386Z","shell.execute_reply":"2025-11-07T18:21:24.207261Z"}},"outputs":[{"name":"stdout","text":"✓ GraphTopicClassifier class defined\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y node2vec gensim smart_open\n!pip install smart_open==5.2.1 --force-reinstall\n!pip install gensim==4.3.2 --force-reinstall\n!pip install node2vec==0.4.6 --force-reinstall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:21:24.209410Z","iopub.execute_input":"2025-11-07T18:21:24.209695Z","iopub.status.idle":"2025-11-07T18:21:57.588946Z","shell.execute_reply.started":"2025-11-07T18:21:24.209678Z","shell.execute_reply":"2025-11-07T18:21:57.587873Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: node2vec 0.4.6\nUninstalling node2vec-0.4.6:\n  Successfully uninstalled node2vec-0.4.6\nFound existing installation: gensim 4.4.0\nUninstalling gensim-4.4.0:\n  Successfully uninstalled gensim-4.4.0\nFound existing installation: smart-open 5.2.1\nUninstalling smart-open-5.2.1:\n  Successfully uninstalled smart-open-5.2.1\nCollecting smart_open==5.2.1\n  Using cached smart_open-5.2.1-py3-none-any.whl.metadata (22 kB)\nUsing cached smart_open-5.2.1-py3-none-any.whl (58 kB)\nInstalling collected packages: smart_open\nSuccessfully installed smart_open-5.2.1\nCollecting gensim==4.3.2\n  Using cached gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\nCollecting numpy>=1.18.5 (from gensim==4.3.2)\n  Using cached numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\nCollecting scipy>=1.7.0 (from gensim==4.3.2)\n  Using cached scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\nCollecting smart-open>=1.8.1 (from gensim==4.3.2)\n  Using cached smart_open-7.4.4-py3-none-any.whl.metadata (24 kB)\nCollecting wrapt (from smart-open>=1.8.1->gensim==4.3.2)\n  Using cached wrapt-2.0.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\nUsing cached gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\nUsing cached numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\nUsing cached scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\nUsing cached smart_open-7.4.4-py3-none-any.whl (63 kB)\nUsing cached wrapt-2.0.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (114 kB)\nInstalling collected packages: wrapt, numpy, smart-open, scipy, gensim\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 2.0.1\n    Uninstalling wrapt-2.0.1:\n      Successfully uninstalled wrapt-2.0.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: smart-open\n    Found existing installation: smart-open 5.2.1\n    Uninstalling smart-open-5.2.1:\n      Successfully uninstalled smart-open-5.2.1\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.3\n    Uninstalling scipy-1.16.3:\n      Successfully uninstalled scipy-1.16.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.4 which is incompatible.\nydata-profiling 4.17.0 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.3 which is incompatible.\ndeprecated 1.2.18 requires wrapt<2,>=1.10, but you have wrapt 2.0.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nscikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nnx-cugraph-cu12 25.6.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gensim-4.3.2 numpy-2.3.4 scipy-1.16.3 smart-open-7.4.4 wrapt-2.0.1\nCollecting node2vec==0.4.6\n  Using cached node2vec-0.4.6-py3-none-any.whl.metadata (743 bytes)\nCollecting gensim<5.0.0,>=4.1.2 (from node2vec==0.4.6)\n  Using cached gensim-4.4.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\nCollecting joblib<2.0.0,>=1.1.0 (from node2vec==0.4.6)\n  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\nCollecting networkx<3.0,>=2.5 (from node2vec==0.4.6)\n  Using cached networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\nCollecting numpy<2.0.0,>=1.19.5 (from node2vec==0.4.6)\n  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nCollecting tqdm<5.0.0,>=4.55.1 (from node2vec==0.4.6)\n  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nCollecting scipy>=1.7.0 (from gensim<5.0.0,>=4.1.2->node2vec==0.4.6)\n  Using cached scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\nCollecting smart_open>=1.8.1 (from gensim<5.0.0,>=4.1.2->node2vec==0.4.6)\n  Using cached smart_open-7.4.4-py3-none-any.whl.metadata (24 kB)\nCollecting wrapt (from smart_open>=1.8.1->gensim<5.0.0,>=4.1.2->node2vec==0.4.6)\n  Using cached wrapt-2.0.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\nUsing cached node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nUsing cached gensim-4.4.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.8 MB)\nUsing cached joblib-1.5.2-py3-none-any.whl (308 kB)\nUsing cached networkx-2.8.8-py3-none-any.whl (2.0 MB)\nUsing cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\nUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\nUsing cached scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\nUsing cached smart_open-7.4.4-py3-none-any.whl (63 kB)\nUsing cached wrapt-2.0.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (114 kB)\nInstalling collected packages: wrapt, tqdm, numpy, networkx, joblib, smart_open, scipy, gensim, node2vec\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 2.0.1\n    Uninstalling wrapt-2.0.1:\n      Successfully uninstalled wrapt-2.0.1\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.67.1\n    Uninstalling tqdm-4.67.1:\n      Successfully uninstalled tqdm-4.67.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.3.4\n    Uninstalling numpy-2.3.4:\n      Successfully uninstalled numpy-2.3.4\n  Attempting uninstall: networkx\n    Found existing installation: networkx 2.8.8\n    Uninstalling networkx-2.8.8:\n      Successfully uninstalled networkx-2.8.8\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.5.2\n    Uninstalling joblib-1.5.2:\n      Successfully uninstalled joblib-1.5.2\n  Attempting uninstall: smart_open\n    Found existing installation: smart_open 7.4.4\n    Uninstalling smart_open-7.4.4:\n      Successfully uninstalled smart_open-7.4.4\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.3\n    Uninstalling scipy-1.16.3:\n      Successfully uninstalled scipy-1.16.3\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.3.2\n    Uninstalling gensim-4.3.2:\n      Successfully uninstalled gensim-4.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nydata-profiling 4.17.0 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.3 which is incompatible.\ndeprecated 1.2.18 requires wrapt<2,>=1.10, but you have wrapt 2.0.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nscikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nnx-cugraph-cu12 25.6.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gensim-4.4.0 joblib-1.5.2 networkx-2.8.8 node2vec-0.4.6 numpy-1.26.4 scipy-1.16.3 smart_open-7.4.4 tqdm-4.67.1 wrapt-2.0.1\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!pip install smart_open==5.2.1 --force-reinstall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:21:57.589951Z","iopub.execute_input":"2025-11-07T18:21:57.590203Z","iopub.status.idle":"2025-11-07T18:22:00.645443Z","shell.execute_reply.started":"2025-11-07T18:21:57.590182Z","shell.execute_reply":"2025-11-07T18:22:00.644338Z"}},"outputs":[{"name":"stdout","text":"Collecting smart_open==5.2.1\n  Using cached smart_open-5.2.1-py3-none-any.whl.metadata (22 kB)\nUsing cached smart_open-5.2.1-py3-none-any.whl (58 kB)\nInstalling collected packages: smart_open\n  Attempting uninstall: smart_open\n    Found existing installation: smart_open 7.4.4\n    Uninstalling smart_open-7.4.4:\n      Successfully uninstalled smart_open-7.4.4\nSuccessfully installed smart_open-5.2.1\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"#import os, sys\n#os._exit(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:22:00.646440Z","iopub.execute_input":"2025-11-07T18:22:00.646743Z","iopub.status.idle":"2025-11-07T18:22:00.650932Z","shell.execute_reply.started":"2025-11-07T18:22:00.646719Z","shell.execute_reply":"2025-11-07T18:22:00.650328Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import json\nimport numpy as np\nimport networkx as nx\nfrom typing import List, Dict\nfrom node2vec import Node2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\n\nprint(\"✓ Starting Multi-Label Article Classifier\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:22:00.651649Z","iopub.execute_input":"2025-11-07T18:22:00.651885Z","iopub.status.idle":"2025-11-07T18:22:00.670853Z","shell.execute_reply.started":"2025-11-07T18:22:00.651867Z","shell.execute_reply":"2025-11-07T18:22:00.669254Z"}},"outputs":[{"name":"stdout","text":"✓ Starting Multi-Label Article Classifier\n\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: GRAPH EMBEDDER\n# ============================================================================\n\nclass GraphEmbedder:\n    \"\"\"Embed graphs using Node2Vec.\"\"\"\n    \n    def __init__(self, dimensions=128, walk_length=30, num_walks=200):\n        self.dimensions = dimensions\n        self.walk_length = walk_length\n        self.num_walks = num_walks\n        print(f\"✓ Initialized Node2Vec embedder (dim={dimensions})\")\n    \n    def embed_graph(self, G: nx.Graph) -> np.ndarray:\n        \"\"\"Embed a single graph.\"\"\"\n        # Convert to undirected\n        G_undirected = G.to_undirected() if G.is_directed() else G\n        \n        # Skip if too small\n        if G_undirected.number_of_nodes() < 2:\n            return np.zeros(self.dimensions)\n        \n        try:\n            # Fit Node2Vec\n            node2vec = Node2Vec(\n                G_undirected, \n                dimensions=self.dimensions,\n                walk_length=self.walk_length,\n                num_walks=self.num_walks,\n                workers=1,\n                quiet=True\n            )\n            \n            model = node2vec.fit(window=10, min_count=1, batch_words=4)\n            \n            # Get embeddings for all nodes\n            node_embeddings = []\n            for node in G_undirected.nodes():\n                node_embeddings.append(model.wv[str(node)])\n            \n            # Average all node embeddings to get graph embedding\n            graph_embedding = np.mean(node_embeddings, axis=0)\n            return graph_embedding\n        except:\n            return np.zeros(self.dimensions)\n    \n    def embed_graphs(self, graphs: List[nx.Graph]) -> np.ndarray:\n        \"\"\"Embed multiple graphs.\"\"\"\n        embeddings = []\n        for i, G in enumerate(graphs):\n            if i % 5 == 0:\n                print(f\"  Embedding graph {i+1}/{len(graphs)}...\")\n            emb = self.embed_graph(G)\n            embeddings.append(emb)\n        return np.array(embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:22:00.671597Z","iopub.execute_input":"2025-11-07T18:22:00.671861Z","iopub.status.idle":"2025-11-07T18:22:00.688986Z","shell.execute_reply.started":"2025-11-07T18:22:00.671847Z","shell.execute_reply":"2025-11-07T18:22:00.687674Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: MULTI-LABEL GRAPH CLASSIFIER\n# ============================================================================\n\nclass MultiLabelGraphClassifier:\n    \"\"\"Multi-label classifier using graph embeddings.\"\"\"\n    \n    def __init__(self, \n                 concept_kgs: Dict[str, nx.DiGraph],\n                 embedding_dim: int = 128,\n                 threshold: float = 0.5,  # Increased from 0.3\n                 top_k: int = 3):  # Max labels per article\n        \"\"\"\n        Initialize classifier.\n        \n        Args:\n            concept_kgs: Dictionary of concept knowledge graphs\n            embedding_dim: Embedding dimension\n            threshold: Minimum similarity threshold (increased to 0.5)\n            top_k: Maximum number of topics to assign per article\n        \"\"\"\n        self.concept_kgs = concept_kgs\n        self.threshold = threshold\n        self.top_k = top_k\n        \n        print(f\"\\n{'='*80}\")\n        print(\"INITIALIZING MULTI-LABEL GRAPH CLASSIFIER\")\n        print(f\"{'='*80}\")\n        \n        # Initialize embedder\n        self.embedder = GraphEmbedder(dimensions=embedding_dim)\n        \n        # Prepare concept graphs\n        self.concept_names = list(concept_kgs.keys())\n        concept_graphs = [concept_kgs[name] for name in self.concept_names]\n        \n        # Embed all concept graphs\n        print(\"\\nEmbedding concept knowledge graphs...\")\n        self.concept_embeddings = self.embedder.embed_graphs(concept_graphs)\n        \n        print(f\"\\n✓ Embedded {len(self.concept_names)} concept graphs\")\n        print(f\"  Embedding shape: {self.concept_embeddings.shape}\")\n        print(f\"  Absolute threshold: {threshold}\")\n        print(f\"  Top-K limit: {self.top_k}\")\n        print(f\"  Relative gap threshold: 0.15\")\n        \n        # Store as dictionary\n        self.concept_embedding_dict = {\n            name: self.concept_embeddings[i] \n            for i, name in enumerate(self.concept_names)\n        }\n    \n    def _dict_to_graph(self, kg_dict: Dict) -> nx.DiGraph:\n        \"\"\"Convert KG dictionary to NetworkX graph.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add nodes from entities\n        entities = kg_dict.get('entities', [])\n        for entity in entities:\n            G.add_node(entity)\n        \n        # Add edges from relations\n        relations = kg_dict.get('relations', [])\n        for relation in relations:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                G.add_edge(source, target, relation=rel_type)\n        \n        return G\n    \n    def classify_article(self, article_kg: Dict, top_k: int = 3) -> Dict:\n        \"\"\"\n        Classify a single article (multi-label).\n        \n        Strategy: Use adaptive thresholding to prevent overprediction.\n        - Apply absolute threshold\n        - Take top-K only\n        - Use relative threshold (gap from max)\n        \n        Args:\n            article_kg: Article knowledge graph dictionary\n            top_k: Maximum number of labels to assign\n        \"\"\"\n        # Convert to graph\n        G_article = self._dict_to_graph(article_kg)\n        \n        # Skip if empty\n        if G_article.number_of_nodes() == 0:\n            return {\n                'predicted_topics': [],\n                'similarities': {},\n                'max_similarity': 0.0\n            }\n        \n        # Embed article graph\n        article_embedding = self.embedder.embed_graph(G_article)\n        \n        # Compute similarities with all concepts\n        similarities = {}\n        for topic_name in self.concept_names:\n            concept_emb = self.concept_embedding_dict[topic_name]\n            sim = cosine_similarity([article_embedding], [concept_emb])[0][0]\n            similarities[topic_name] = float(sim)\n        \n        # Get sorted topics by similarity\n        sorted_topics = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n        \n        # Strategy 1: Absolute threshold\n        candidates = [(topic, sim) for topic, sim in sorted_topics if sim >= self.threshold]\n        \n        if len(candidates) == 0:\n            return {\n                'predicted_topics': [],\n                'similarities': similarities,\n                'max_similarity': max(similarities.values()) if similarities else 0.0\n            }\n        \n        # Strategy 2: Top-K limit (prevent overprediction)\n        candidates = candidates[:top_k]\n        \n        # Strategy 3: Relative threshold (gap detection)\n        # Only keep topics within 0.15 similarity of the best match\n        max_sim = candidates[0][1]\n        relative_threshold = max_sim - 0.35\n        \n        predicted_topics = [\n            topic for topic, sim in candidates\n            if sim >= relative_threshold\n        ]\n        \n        return {\n            'predicted_topics': predicted_topics,\n            'similarities': similarities,\n            'max_similarity': max(similarities.values()) if similarities else 0.0\n        }\n    \n    def classify_dataset(self, \n                        articles: List[Dict],\n                        knowledge_graphs: List[Dict]) -> List[Dict]:\n        \"\"\"Classify all articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(knowledge_graphs)} ARTICLES (MULTI-LABEL)\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx in range(len(knowledge_graphs)):\n            if idx % 20 == 0:\n                print(f\"Processing articles {idx}-{min(idx+20, len(knowledge_graphs))}...\")\n            \n            article = articles[idx] if idx < len(articles) else {}\n            article_kg = kg_dict.get(idx, {'entities': [], 'relations': []})\n            \n            # Skip if no KG\n            if article_kg.get('num_edges', 0) == 0:\n                results.append({\n                    'article_id': idx,\n                    'predicted_topics': [],\n                    'similarities': {},\n                    'max_similarity': 0.0,\n                    'true_concepts': article.get('Concept', [])\n                })\n                continue\n            \n            # Classify (multi-label)\n            classification = self.classify_article(article_kg)\n            \n            result = {\n                'article_id': idx,\n                'predicted_topics': classification['predicted_topics'],\n                'num_predicted': len(classification['predicted_topics']),\n                'similarities': classification['similarities'],\n                'max_similarity': classification['max_similarity'],\n                'true_concepts': article.get('Concept', [])\n            }\n            \n            results.append(result)\n        \n        print(\"\\n✓ Classification complete!\")\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:22:00.689875Z","iopub.execute_input":"2025-11-07T18:22:00.690092Z","iopub.status.idle":"2025-11-07T18:22:00.707637Z","shell.execute_reply.started":"2025-11-07T18:22:00.690074Z","shell.execute_reply":"2025-11-07T18:22:00.706812Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: EVALUATION METRICS\n# ============================================================================\n\ndef calculate_metrics(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate multi-label classification metrics.\n    Uses micro-averaged precision, recall, F1.\n    \"\"\"\n    total_tp = 0\n    total_fp = 0\n    total_fn = 0\n    \n    articles_evaluated = 0\n    articles_skipped = 0\n    \n    multi_label_count = 0\n    single_label_count = 0\n    no_label_count = 0\n    \n    for result in results:\n        true_topics = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_topics = set([t.lower().strip() for t in result['predicted_topics']])\n        \n        if len(true_topics) == 0:\n            articles_skipped += 1\n            continue\n        \n        articles_evaluated += 1\n        \n        # Count label distribution\n        if len(pred_topics) == 0:\n            no_label_count += 1\n        elif len(pred_topics) == 1:\n            single_label_count += 1\n        else:\n            multi_label_count += 1\n        \n        tp = len(true_topics & pred_topics)\n        fp = len(pred_topics - true_topics)\n        fn = len(true_topics - pred_topics)\n        \n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n    \n    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    metrics = {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'total_tp': total_tp,\n        'total_fp': total_fp,\n        'total_fn': total_fn,\n        'articles_evaluated': articles_evaluated,\n        'articles_skipped': articles_skipped,\n        'multi_label_count': multi_label_count,\n        'single_label_count': single_label_count,\n        'no_label_count': no_label_count\n    }\n    \n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS (Micro-Averaged)\")\n    print(f\"{'='*80}\")\n    print(f\"\\nArticles evaluated: {articles_evaluated}\")\n    print(f\"Articles skipped (no ground truth): {articles_skipped}\")\n    print(f\"\\nLabel Distribution:\")\n    print(f\"  No labels: {no_label_count} ({no_label_count/articles_evaluated*100:.1f}%)\")\n    print(f\"  Single label: {single_label_count} ({single_label_count/articles_evaluated*100:.1f}%)\")\n    print(f\"  Multi-label: {multi_label_count} ({multi_label_count/articles_evaluated*100:.1f}%)\")\n    print(f\"\\nAggregated Statistics:\")\n    print(f\"  True Positives (TP): {total_tp}\")\n    print(f\"  False Positives (FP): {total_fp}\")\n    print(f\"  False Negatives (FN): {total_fn}\")\n    print(f\"\\n📊 PERFORMANCE:\")\n    print(f\"  Precision: {precision:.3f}\")\n    print(f\"  Recall: {recall:.3f}\")\n    print(f\"  F1 Score: {f1_score:.3f}\")\n    \n    # Comparison with baselines\n    print(f\"\\n{'='*80}\")\n    print(\"COMPARISON WITH PAPER BASELINES (Medical Dataset)\")\n    print(f\"{'='*80}\")\n    print(f\"  This model:           F1 = {f1_score:.3f}\")\n    print(f\"  GFLM-S baseline:      F1 = 0.532\")\n    print(f\"  GFLM-W baseline:      F1 = 0.530\")\n    print(f\"  SBERT:                F1 = 0.594\")\n    print(f\"  ChatGPT-3.5:          F1 = 0.606\")\n    \n    return metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:22:00.708426Z","iopub.execute_input":"2025-11-07T18:22:00.708659Z","iopub.status.idle":"2025-11-07T18:22:00.727245Z","shell.execute_reply.started":"2025-11-07T18:22:00.708639Z","shell.execute_reply":"2025-11-07T18:22:00.726453Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: RUN CLASSIFICATION\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"RUNNING MULTI-LABEL CLASSIFICATION\")\nprint(\"=\"*80)\n\n# Initialize classifier with concept KGs\nclassifier = MultiLabelGraphClassifier(\n    concept_kgs=concept_kgs,\n    embedding_dim=128,\n    threshold=0.5,  # Higher threshold = fewer predictions\n    top_k=5  # Max 3 labels per article\n)\n\n# Classify all articles\nresults = classifier.classify_dataset(\n    articles=articles,\n    knowledge_graphs=knowledge_graphs\n)\n\n# Calculate metrics\nmetrics = calculate_metrics(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:22:00.728087Z","iopub.execute_input":"2025-11-07T18:22:00.728352Z","iopub.status.idle":"2025-11-07T18:25:34.305609Z","shell.execute_reply.started":"2025-11-07T18:22:00.728331Z","shell.execute_reply":"2025-11-07T18:25:34.304425Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nRUNNING MULTI-LABEL CLASSIFICATION\n================================================================================\n\n================================================================================\nINITIALIZING MULTI-LABEL GRAPH CLASSIFIER\n================================================================================\n✓ Initialized Node2Vec embedder (dim=128)\n\nEmbedding concept knowledge graphs...\n  Embedding graph 1/24...\n  Embedding graph 6/24...\n  Embedding graph 11/24...\n  Embedding graph 16/24...\n  Embedding graph 21/24...\n\n✓ Embedded 24 concept graphs\n  Embedding shape: (24, 128)\n  Absolute threshold: 0.5\n  Top-K limit: 5\n  Relative gap threshold: 0.15\n\n================================================================================\nCLASSIFYING 380 ARTICLES (MULTI-LABEL)\n================================================================================\n\nProcessing articles 0-20...\nProcessing articles 20-40...\nProcessing articles 40-60...\nProcessing articles 60-80...\nProcessing articles 80-100...\nProcessing articles 100-120...\nProcessing articles 120-140...\nProcessing articles 140-160...\nProcessing articles 160-180...\nProcessing articles 180-200...\nProcessing articles 200-220...\nProcessing articles 220-240...\nProcessing articles 240-260...\nProcessing articles 260-280...\nProcessing articles 280-300...\nProcessing articles 300-320...\nProcessing articles 320-340...\nProcessing articles 340-360...\nProcessing articles 360-380...\n\n✓ Classification complete!\n\n================================================================================\nEVALUATION METRICS (Micro-Averaged)\n================================================================================\n\nArticles evaluated: 267\nArticles skipped (no ground truth): 113\n\nLabel Distribution:\n  No labels: 7 (2.6%)\n  Single label: 0 (0.0%)\n  Multi-label: 260 (97.4%)\n\nAggregated Statistics:\n  True Positives (TP): 60\n  False Positives (FP): 719\n  False Negatives (FN): 335\n\n📊 PERFORMANCE:\n  Precision: 0.077\n  Recall: 0.152\n  F1 Score: 0.102\n\n================================================================================\nCOMPARISON WITH PAPER BASELINES (Medical Dataset)\n================================================================================\n  This model:           F1 = 0.102\n  GFLM-S baseline:      F1 = 0.532\n  GFLM-W baseline:      F1 = 0.530\n  SBERT:                F1 = 0.594\n  ChatGPT-3.5:          F1 = 0.606\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# THREE APPROACHES TO TOPIC EMBEDDING (FIXED VERSION)\n# ============================================================================\n\n!pip install sentence-transformers -q\n\nimport json\nimport numpy as np\nfrom typing import List, Dict\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\n\nprint(\"✓ Imports complete\\n\")\n\n\nclass SimpleKGClassifier:\n    \"\"\"Classify articles by comparing KG embeddings to topic embeddings.\"\"\"\n    \n    def __init__(self, \n                 topic_names: List[str],\n                 topic_keywords: Dict[str, List[str]] = None,\n                 topic_embedding_method: str = 'name_only',\n                 threshold: float = 0.3):\n        \"\"\"\n        Initialize classifier.\n        \n        Args:\n            topic_names: List of topic names\n            topic_keywords: Dictionary mapping topics to keywords\n            topic_embedding_method: 'name_only', 'name_plus_keywords', or 'average_embeddings'\n            threshold: Similarity threshold\n        \"\"\"\n        self.topic_names = [t.lower() for t in topic_names]\n        self.topic_keywords = topic_keywords\n        self.method = topic_embedding_method\n        self.threshold = threshold\n        \n        # Initialize SBERT model\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"INITIALIZING CLASSIFIER - METHOD: {topic_embedding_method.upper()}\")\n        print(f\"{'='*80}\")\n        print(f\"Total topics: {len(self.topic_names)}\")\n        print(f\"Threshold: {threshold}\\n\")\n        \n        # Embed topics based on method\n        print(f\"Embedding topics using method: {topic_embedding_method}...\")\n        \n        if topic_embedding_method == 'name_only':\n            self.topic_embeddings = self._embed_method1_name_only()\n        elif topic_embedding_method == 'name_plus_keywords':\n            self.topic_embeddings = self._embed_method2_name_plus_keywords()\n        elif topic_embedding_method == 'average_embeddings':\n            self.topic_embeddings = self._embed_method3_average_embeddings()\n        else:\n            raise ValueError(f\"Unknown method: {topic_embedding_method}\")\n        \n        print(f\"✓ Embedded {len(self.topic_names)} topics\")\n        print(f\"  Embedding dimension: {self.topic_embeddings.shape[1]}\")\n    \n    def _embed_method1_name_only(self) -> np.ndarray:\n        \"\"\"Method 1: Embed only topic names.\"\"\"\n        print(\"  Method 1: Topic name only\")\n        print(f\"  Example: 'phone' → embedding\")\n        \n        embeddings = self.model.encode(self.topic_names)\n        return embeddings\n    \n    def _embed_method2_name_plus_keywords(self) -> np.ndarray:\n        \"\"\"Method 2: Embed topic + keywords as concatenated text.\"\"\"\n        print(\"  Method 2: Topic + keywords as text\")\n        \n        topic_texts = []\n        for topic in self.topic_names:\n            if self.topic_keywords and topic in self.topic_keywords and self.topic_keywords[topic]:\n                keywords = \", \".join(self.topic_keywords[topic][:10])\n                topic_text = f\"{topic}: {keywords}\"\n            else:\n                topic_text = topic\n            topic_texts.append(topic_text)\n        \n        print(f\"  Example: '{topic_texts[0][:80]}...'\")\n        \n        embeddings = self.model.encode(topic_texts)\n        return embeddings\n    \n    def _embed_method3_average_embeddings(self) -> np.ndarray:\n        \"\"\"Method 3: Average embeddings of topic name + keywords.\"\"\"\n        print(\"  Method 3: Average of topic and keyword embeddings\")\n        \n        all_embeddings = []\n        \n        for topic in self.topic_names:\n            # Embed topic name\n            topic_emb = self.model.encode(topic)\n            \n            # Embed keywords if available\n            if self.topic_keywords and topic in self.topic_keywords and self.topic_keywords[topic]:\n                keywords = self.topic_keywords[topic][:10]\n                keyword_embs = self.model.encode(keywords)\n                \n                # Average topic + keyword embeddings\n                all_embs = np.vstack([topic_emb.reshape(1, -1), keyword_embs])\n                avg_emb = np.mean(all_embs, axis=0)\n            else:\n                # No keywords - just use topic embedding\n                avg_emb = topic_emb\n            \n            all_embeddings.append(avg_emb)\n        \n        print(f\"  Example: avg(embed('phone'), embed('nokia'), embed('quality'))\")\n        \n        return np.array(all_embeddings)\n    \n    def kg_to_text(self, kg_dict: Dict) -> str:\n        \"\"\"Convert KG to text representation.\"\"\"\n        entities = kg_dict.get('entities', [])\n        \n        if not entities:\n            return \"\"\n        \n        # Just list entities\n        text = \", \".join(entities[:50])\n        return text\n    \n    def classify_article(self, article_kg: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        kg_text = self.kg_to_text(article_kg)\n        \n        if not kg_text:\n            return {\n                'predicted_topics': [],\n                'similarities': {},\n                'max_similarity': 0.0\n            }\n        \n        # Embed KG\n        kg_embedding = self.model.encode(kg_text)\n        \n        # Compute similarities\n        similarities = {}\n        for i, topic_name in enumerate(self.topic_names):\n            sim = cosine_similarity(\n                [kg_embedding], \n                [self.topic_embeddings[i]]\n            )[0][0]\n            similarities[topic_name] = float(sim)\n        \n        # Assign topics above threshold\n        predicted_topics = [\n            topic for topic, sim in similarities.items() \n            if sim >= self.threshold\n        ]\n        \n        predicted_topics = sorted(\n            predicted_topics, \n            key=lambda t: similarities[t], \n            reverse=True\n        )\n        \n        return {\n            'predicted_topics': predicted_topics,\n            'similarities': similarities,\n            'max_similarity': max(similarities.values()) if similarities else 0.0\n        }\n    \n    def classify_dataset(self, \n                        articles: List[Dict],\n                        knowledge_graphs: List[Dict]) -> List[Dict]:\n        \"\"\"Classify all articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(knowledge_graphs)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx in range(len(knowledge_graphs)):\n            if idx % 20 == 0:\n                print(f\"Processing articles {idx}-{min(idx+20, len(knowledge_graphs))}...\")\n            \n            article = articles[idx] if idx < len(articles) else {}\n            article_kg = kg_dict.get(idx, {'entities': [], 'relations': []})\n            \n            classification = self.classify_article(article_kg)\n            \n            result = {\n                'article_id': idx,\n                'predicted_topics': classification['predicted_topics'],\n                'similarities': classification['similarities'],\n                'max_similarity': classification['max_similarity'],\n                'true_concepts': article.get('Concept', [])\n            }\n            \n            results.append(result)\n        \n        print(\"\\n✓ Classification complete!\")\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T18:25:34.306542Z","iopub.execute_input":"2025-11-07T18:25:34.306770Z","iopub.status.idle":"2025-11-07T18:25:37.470181Z","shell.execute_reply.started":"2025-11-07T18:25:34.306751Z","shell.execute_reply":"2025-11-07T18:25:37.469230Z"}},"outputs":[{"name":"stdout","text":"✓ Imports complete\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# ============================================================================\n# TEST ALL THREE METHODS\n# ============================================================================\n\n# Get topic names\nall_topic_names = list(concept_keywords.keys())\n\nmethods = {\n    'name_only': 'Method 1: Topic Name Only',\n    'name_plus_keywords': 'Method 2: Topic + Keywords as Text',\n    'average_embeddings': 'Method 3: Average Embeddings'\n}\n\nresults_comparison = {}\n\nfor method_key, method_name in methods.items():\n    print(f\"\\n\\n{'#'*80}\")\n    print(f\"TESTING: {method_name}\")\n    print(f\"{'#'*80}\")\n    \n    # Initialize classifier\n    classifier = SimpleKGClassifier(\n        topic_names=all_topic_names,\n        topic_keywords=concept_keywords,\n        topic_embedding_method=method_key,\n        threshold=0.3\n    )\n    \n    # Classify\n    results = classifier.classify_dataset(\n        articles=articles,\n        knowledge_graphs=knowledge_graphs\n    )\n    \n    # Evaluate\n    metrics = calculate_metrics_paper_style(results)\n    \n    # Store results\n    results_comparison[method_key] = {\n        'metrics': metrics,\n        'results': results\n    }\n    \n    # Save\n    save_results(results, f'/kaggle/working/{method_key}_results.json')\n    export_to_csv(results, f'/kaggle/working/{method_key}_results.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:26:21.387937Z","iopub.execute_input":"2025-11-07T20:26:21.388606Z","iopub.status.idle":"2025-11-07T20:26:21.402529Z","shell.execute_reply.started":"2025-11-07T20:26:21.388570Z","shell.execute_reply":"2025-11-07T20:26:21.401431Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3925412041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get topic names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mall_topic_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m methods = {\n","\u001b[0;31mNameError\u001b[0m: name 'concept_keywords' is not defined"],"ename":"NameError","evalue":"name 'concept_keywords' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# FINAL COMPARISON OF ALL METHODS\n# ============================================================================\n\nprint(f\"\\n\\n{'='*80}\")\nprint(\"FINAL COMPARISON: ALL METHODS\")\nprint(f\"{'='*80}\\n\")\n\ncomparison_df = pd.DataFrame({\n    'Method': [\n        'Method 1: Name Only',\n        'Method 2: Name + Keywords Text',\n        'Method 3: Average Embeddings',\n        'LLM Reasoning (baseline)',\n        'Paper ChatGPT (baseline)'\n    ],\n    'F1 Score': [\n        results_comparison['name_only']['metrics']['f1_score'],\n        results_comparison['name_plus_keywords']['metrics']['f1_score'],\n        results_comparison['average_embeddings']['metrics']['f1_score'],\n        0.818,\n        0.606\n    ],\n    'Precision': [\n        results_comparison['name_only']['metrics']['precision'],\n        results_comparison['name_plus_keywords']['metrics']['precision'],\n        results_comparison['average_embeddings']['metrics']['precision'],\n        0.719,\n        '-'\n    ],\n    'Recall': [\n        results_comparison['name_only']['metrics']['recall'],\n        results_comparison['name_plus_keywords']['metrics']['recall'],\n        results_comparison['average_embeddings']['metrics']['recall'],\n        0.948,\n        '-'\n    ]\n})\n\nprint(comparison_df.to_string(index=False))\n\n# Find best method\nbest_method = max(results_comparison.items(), key=lambda x: x[1]['metrics']['f1_score'])\nprint(f\"\\n{'='*80}\")\nprint(f\"🏆 BEST METHOD: {methods[best_method[0]]}\")\nprint(f\"   F1 Score: {best_method[1]['metrics']['f1_score']:.3f}\")\nprint(f\"{'='*80}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-07T20:25:46.199Z"}},"outputs":[],"execution_count":null}]}