{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13468504,"sourceType":"datasetVersion","datasetId":8549797},{"sourceId":13524188,"sourceType":"datasetVersion","datasetId":8587369}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:36:53.710777Z","iopub.execute_input":"2025-10-27T21:36:53.711052Z","iopub.status.idle":"2025-10-27T21:36:54.711986Z","shell.execute_reply.started":"2025-10-27T21:36:53.711034Z","shell.execute_reply":"2025-10-27T21:36:54.711191Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/zstikg/NewsConcept Data-set.json\n/kaggle/input/zstikg/DVD playerData-set.json\n/kaggle/input/zstikg/MedicalConcept Data-set.json\n/kaggle/input/zstikg/Cellular phone Data-set.json\n/kaggle/input/zstikg/Digital camera2 Data-set.json\n/kaggle/input/zstikg/Mp3 playerData-set.json\n/kaggle/input/zstikg/Digital camera1 Data-set.json\n/kaggle/input/keywords/Keyword_Medical.json\n/kaggle/input/keywords/Keyword_Canon.json\n/kaggle/input/keywords/Keyword_Creative.json\n/kaggle/input/keywords/Keyword_Apex.json\n/kaggle/input/keywords/Keyword_Apex 1.json\n/kaggle/input/keywords/Keyword_Nokia.json\n/kaggle/input/keywords/Keyword_News.json\n/kaggle/input/keywords/Keyword_Nikon.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================================\n# PART 1: INSTALLATION AND SETUP\n# ============================================================================\n\n# Install required packages\n!pip install dspy-ai huggingface_hub networkx sentence-transformers pandas --quiet\n\nprint(\"✓ Packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:36:54.712940Z","iopub.execute_input":"2025-10-27T21:36:54.713277Z","iopub.status.idle":"2025-10-27T21:38:16.957463Z","shell.execute_reply.started":"2025-10-27T21:36:54.713248Z","shell.execute_reply":"2025-10-27T21:38:16.956678Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✓ Packages installed successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# PART 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom collections import defaultdict\nimport pandas as pd\nimport os\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:16.959664Z","iopub.execute_input":"2025-10-27T21:38:16.959910Z","iopub.status.idle":"2025-10-27T21:38:49.312465Z","shell.execute_reply.started":"2025-10-27T21:38:16.959888Z","shell.execute_reply":"2025-10-27T21:38:49.311571Z"}},"outputs":[{"name":"stderr","text":"2025-10-27 21:38:35.476005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761601115.666633      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761601115.723940      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: INSTALL AND IMPORT (if not done already)\n# ============================================================================\n\nimport os\nfrom huggingface_hub import InferenceClient\nimport dspy\nimport json\nfrom typing import List, Tuple, Dict\n\nprint(\"✓ Imports complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:49.313237Z","iopub.execute_input":"2025-10-27T21:38:49.314089Z","iopub.status.idle":"2025-10-27T21:38:49.394209Z","shell.execute_reply.started":"2025-10-27T21:38:49.314066Z","shell.execute_reply":"2025-10-27T21:38:49.393602Z"}},"outputs":[{"name":"stdout","text":"✓ Imports complete\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: CONFIGURE HUGGING FACE WITH DSPY.LM (PROPER WAY)\n# ============================================================================\nimport os\nimport dspy\n\nos.environ['HUGGINGFACE_API_KEY'] = 'KEY'\n\n# Use DSPy's built-in LM class with the correct prefix\nlm = dspy.LM(\n    model='huggingface/meta-llama/Llama-3.1-8B-Instruct',\n    api_key=os.environ['HUGGINGFACE_API_KEY'],\n    max_tokens=12000,\n    temperature=0.3\n)\n\ndspy.settings.configure(lm=lm)\nprint(\"✓ Llama-3.1-8B-Instruct configured successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:49.394918Z","iopub.execute_input":"2025-10-27T21:38:49.395210Z","iopub.status.idle":"2025-10-27T21:38:49.400311Z","shell.execute_reply.started":"2025-10-27T21:38:49.395174Z","shell.execute_reply":"2025-10-27T21:38:49.399729Z"}},"outputs":[{"name":"stdout","text":"✓ Llama-3.1-8B-Instruct configured successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================================\n# PART 1: DATA LOADING FUNCTION\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"\n    Load JSON data from file\n    \n    Expected format:\n    [\n        {\n            \"Article Title\": [],\n            \"Article Text\": \"text here...\",\n            \"Concept\": [\"concept1\", \"concept2\"]\n        },\n        ...\n    ]\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\n# Test with sample data\nsample_data = [\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"excellent phone, excellent service.\",\n        \"Concept\": []\n    },\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"i am a business user who heavily depend on mobile service.\",\n        \"Concept\": [\"service\"]\n    }\n]\n\nprint(\"✓ Sample data ready for testing\")\nprint(f\"  Sample has {len(sample_data)} documents\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:49.401102Z","iopub.execute_input":"2025-10-27T21:38:49.401351Z","iopub.status.idle":"2025-10-27T21:38:49.413526Z","shell.execute_reply.started":"2025-10-27T21:38:49.401325Z","shell.execute_reply":"2025-10-27T21:38:49.412811Z"}},"outputs":[{"name":"stdout","text":"✓ Sample data ready for testing\n  Sample has 2 documents\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# FIND THE DATASET FILES\n# ============================================================================\n\nimport os\n\n# List files in your dataset directory\ndataset_path = '/kaggle/input/zstikg'\n\nprint(\"Files in the dataset:\")\nprint(\"=\" * 60)\n\ntry:\n    files = os.listdir(dataset_path)\n    for i, file in enumerate(files, 1):\n        print(f\"{i}. {file}\")\n        full_path = os.path.join(dataset_path, file)\n        print(f\"   Full path: {full_path}\")\n        print()\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    print(\"\\nTrying to list all datasets...\")\n    for dataset in os.listdir('/kaggle/input/'):\n        print(f\"- {dataset}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:49.414157Z","iopub.execute_input":"2025-10-27T21:38:49.414433Z","iopub.status.idle":"2025-10-27T21:38:49.431895Z","shell.execute_reply.started":"2025-10-27T21:38:49.414393Z","shell.execute_reply":"2025-10-27T21:38:49.431246Z"}},"outputs":[{"name":"stdout","text":"Files in the dataset:\n============================================================\n1. NewsConcept Data-set.json\n   Full path: /kaggle/input/zstikg/NewsConcept Data-set.json\n\n2. DVD playerData-set.json\n   Full path: /kaggle/input/zstikg/DVD playerData-set.json\n\n3. MedicalConcept Data-set.json\n   Full path: /kaggle/input/zstikg/MedicalConcept Data-set.json\n\n4. Cellular phone Data-set.json\n   Full path: /kaggle/input/zstikg/Cellular phone Data-set.json\n\n5. Digital camera2 Data-set.json\n   Full path: /kaggle/input/zstikg/Digital camera2 Data-set.json\n\n6. Mp3 playerData-set.json\n   Full path: /kaggle/input/zstikg/Mp3 playerData-set.json\n\n7. Digital camera1 Data-set.json\n   Full path: /kaggle/input/zstikg/Digital camera1 Data-set.json\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# LOAD ONE SPECIFIC DATASET\n# ============================================================================\n\n# Choose which dataset you want to work with:\n# Option 1: Cellular phone\nfilepath = '/kaggle/input/zstikg/MedicalConcept Data-set.json'\n\n# Option 2: News\n# filepath = '/kaggle/input/zsltikg/NewsConcept Data-set.json'\n\n# Option 3: Medical\n# filepath = '/kaggle/input/zsltikg/MedicalConcept Data-set.json'\n\n# Load the data\ndata = load_json_data(filepath)[:100]\n\nprint(f\"✓ Loaded {len(data)} documents\")\nprint(f\"\\nFirst document preview:\")\nprint(f\"  Keys: {list(data[0].keys())}\")\nprint(f\"  Text: {data[0].get('Article Text', '')[:100]}...\")\nprint(f\"  Concepts: {data[0].get('Concept', [])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:49.432672Z","iopub.execute_input":"2025-10-27T21:38:49.433190Z","iopub.status.idle":"2025-10-27T21:38:49.695579Z","shell.execute_reply.started":"2025-10-27T21:38:49.433166Z","shell.execute_reply":"2025-10-27T21:38:49.694887Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 2066 documents from /kaggle/input/zstikg/MedicalConcept Data-set.json\n✓ Loaded 100 documents\n\nFirst document preview:\n  Keys: ['Article Title', 'Article Text', 'Concept']\n  Text: 1. Health care reform\n\nHow could the health care reform legislation that President Barack Obama sign...\n  Concepts: []\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# PROCESS YOUR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"PROCESSING ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\n# data is now correctly loaded as a list of dicts\nindividual_articles = []\n\nfor idx, item in enumerate(data):\n    article = {\n        'id': idx,\n        'Article Text': item['Article Text'],\n        'Concept': item['Concept'] if item['Concept'] else []\n    }\n    individual_articles.append(article)\n\nprint(f\"\\n✓ Created list of {len(individual_articles)} individual articles\")\n\n# Show first 3\nprint(f\"\\nFirst 3 articles:\")\nprint(\"-\" * 80)\nfor i in range(min(3, len(individual_articles))):\n    article = individual_articles[i]\n    print(f\"\\nArticle {article['id']}:\")\n    print(f\"  Text: {article['Article Text'][:80]}...\")\n    print(f\"  Concepts: {article['Concept']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:49.698153Z","iopub.execute_input":"2025-10-27T21:38:49.698374Z","iopub.status.idle":"2025-10-27T21:38:49.704196Z","shell.execute_reply.started":"2025-10-27T21:38:49.698356Z","shell.execute_reply":"2025-10-27T21:38:49.703539Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPROCESSING ALL THE ARTICLES\n================================================================================\n\n✓ Created list of 100 individual articles\n\nFirst 3 articles:\n--------------------------------------------------------------------------------\n\nArticle 0:\n  Text: 1. Health care reform\n\nHow could the health care reform legislation that Preside...\n  Concepts: []\n\nArticle 1:\n  Text: My colleagues at Harvard Health Publishing and I have a mission: to provide accu...\n  Concepts: ['Heart Health', \"Women's Health\"]\n\nArticle 2:\n  Text: Does echinacea, the popular natural cold remedy, really work?\n\nIt depends on wha...\n  Concepts: []\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: DEFINE SIGNATURES\n# ============================================================================\n\nclass EntityExtractor(dspy.Signature):\n    \"\"\"Extract key entities from the given text. Extracted entities are nouns, \n    verbs, or adjectives, particularly regarding sentiment. This is for an \n    extraction task, please be thorough and accurate to the reference text.\n    \n    Return ONLY a valid JSON list format: [\"entity1\", \"entity2\", \"entity3\"]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract entities from\")\n    entities = dspy.OutputField(desc=\"List of extracted entities in JSON format\")\n\nclass RelationExtractor(dspy.Signature):\n    \"\"\"Extract subject-predicate-object triples from the assistant message. \n    A predicate (1-3 words) defines the relationship between the subject and \n    object. Relationship may be fact or sentiment based on assistant's message. \n    Subject and object are entities. Entities provided are from the assistant \n    message and prior conversation history, though you may not need all of them. \n    This is for an extraction task, please be thorough, accurate, and faithful \n    to the reference text.\n    \n    Return ONLY valid JSON format: [[\"subject1\", \"predicate1\", \"object1\"], [\"subject2\", \"predicate2\", \"object2\"]]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract relations from\")\n    entities = dspy.InputField(desc=\"List of available entities\")\n    triples = dspy.OutputField(desc=\"List of [subject, predicate, object] triples in JSON format\")\n\nprint(\"✓ Signatures defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:49.704891Z","iopub.execute_input":"2025-10-27T21:38:49.705124Z","iopub.status.idle":"2025-10-27T21:38:49.719492Z","shell.execute_reply.started":"2025-10-27T21:38:49.705102Z","shell.execute_reply":"2025-10-27T21:38:49.718695Z"}},"outputs":[{"name":"stdout","text":"✓ Signatures defined\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: CREATE ENTITY EXTRACTOR\n# ============================================================================\n\nclass ExtractEntities(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(EntityExtractor)\n    \n    def forward(self, text: str) -> List[str]:\n        if not text or len(text.strip()) < 3:\n            return []\n            \n        result = self.extract(text=text)\n        \n        try:\n            entities_text = result.entities.strip()\n            \n            if '[' in entities_text and ']' in entities_text:\n                start = entities_text.find('[')\n                end = entities_text.rfind(']') + 1\n                entities_text = entities_text[start:end]\n            \n            entities = json.loads(entities_text)\n            \n            if isinstance(entities, list):\n                return [str(e).lower().strip() for e in entities if e and len(str(e).strip()) > 1]\n            return []\n            \n        except:\n            try:\n                entities_text = result.entities.strip()\n                if entities_text.startswith('['):\n                    entities_text = entities_text[1:]\n                if entities_text.endswith(']'):\n                    entities_text = entities_text[:-1]\n                \n                entities = []\n                for item in entities_text.split(','):\n                    item = item.strip(' \"\\'\\n\\t')\n                    if item and len(item) > 1:\n                        entities.append(item.lower())\n                \n                return entities[:50]\n            except:\n                return []\n\nentity_extractor = ExtractEntities()\nprint(\"✓ Entity Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:49.720317Z","iopub.execute_input":"2025-10-27T21:38:49.720635Z","iopub.status.idle":"2025-10-27T21:38:49.738108Z","shell.execute_reply.started":"2025-10-27T21:38:49.720611Z","shell.execute_reply":"2025-10-27T21:38:49.737451Z"}},"outputs":[{"name":"stdout","text":"✓ Entity Extractor created\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: CREATE RELATION EXTRACTOR\n# ============================================================================\n\nclass ExtractRelations(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(RelationExtractor)\n    \n    def forward(self, text: str, entities: List[str]) -> List[Tuple[str, str, str]]:\n        if not entities or not text:\n            return []\n        \n        entities_subset = entities[:30]\n        entities_str = json.dumps(entities_subset)\n        \n        result = self.extract(text=text, entities=entities_str)\n        \n        try:\n            triples_text = result.triples.strip()\n            \n            if '[' in triples_text and ']' in triples_text:\n                start = triples_text.find('[')\n                end = triples_text.rfind(']') + 1\n                triples_text = triples_text[start:end]\n            \n            triples = json.loads(triples_text)\n            \n            normalized_triples = []\n            for triple in triples:\n                if isinstance(triple, (list, tuple)) and len(triple) == 3:\n                    s, p, o = triple\n                    s = str(s).lower().strip()\n                    p = str(p).lower().strip()\n                    o = str(o).lower().strip()\n                    \n                    if s and p and o and s != o:\n                        normalized_triples.append((s, p, o))\n            \n            return normalized_triples[:100]\n            \n        except Exception as e:\n            return []\n\nrelation_extractor = ExtractRelations()\nprint(\"✓ Relation Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:49.738784Z","iopub.execute_input":"2025-10-27T21:38:49.738979Z","iopub.status.idle":"2025-10-27T21:38:49.756171Z","shell.execute_reply.started":"2025-10-27T21:38:49.738964Z","shell.execute_reply":"2025-10-27T21:38:49.755472Z"}},"outputs":[{"name":"stdout","text":"✓ Relation Extractor created\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: TEST EXTRACTION\n# ============================================================================\n\nprint(\"\\nTesting Entity & Relation Extraction...\")\nprint(\"=\" * 80)\n\n# Test on first 2 documents\nfor i in range(min(2, len(data))):\n    text = data[i].get('Article Text', '')\n    \n    print(f\"\\nDocument {i+1}:\")\n    print(f\"  Text: {text[:80]}...\")\n    \n    # Extract entities\n    print(f\"  Extracting entities...\")\n    entities = entity_extractor(text)\n    print(f\"  ✓ Found {len(entities)} entities: {entities[:5]}...\")\n    \n    # Extract relations\n    if entities:\n        print(f\"  Extracting relations...\")\n        relations = relation_extractor(text, entities)\n        print(f\"  ✓ Found {len(relations)} relations\")\n        \n        if relations:\n            for j, (s, p, o) in enumerate(relations[:3], 1):\n                print(f\"    {j}. ({s}) --[{p}]--> ({o})\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ Test complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:38:49.756923Z","iopub.execute_input":"2025-10-27T21:38:49.757160Z","iopub.status.idle":"2025-10-27T21:39:20.932136Z","shell.execute_reply.started":"2025-10-27T21:38:49.757145Z","shell.execute_reply":"2025-10-27T21:39:20.931476Z"}},"outputs":[{"name":"stdout","text":"\nTesting Entity & Relation Extraction...\n================================================================================\n\nDocument 1:\n  Text: 1. Health care reform\n\nHow could the health care reform legislation that Preside...\n  Extracting entities...\n  ✓ Found 39 entities: ['health care reform', 'president barack obama', 'patient protection and affordable care act', 'american heart association', 'cardiopulmonary resuscitation']...\n  Extracting relations...\n  ✓ Found 16 relations\n    1. (president barack obama) --[signed into law]--> (patient protection and affordable care act)\n    2. (patient protection and affordable care act) --[extends coverage to]--> (32 million americans)\n    3. (patient protection and affordable care act) --[reins in health care spending]--> (by 2019)\n\nDocument 2:\n  Text: My colleagues at Harvard Health Publishing and I have a mission: to provide accu...\n  Extracting entities...\n  ✓ Found 60 entities: ['harvard health publishing', 'health', 'lives', 'mission', 'feedback']...\n  Extracting relations...\n  ✓ Found 15 relations\n    1. (harvard health publishing) --[provides]--> (accurate and reliable information)\n    2. (harvard health publishing) --[helps readers]--> (live healthier lives)\n    3. (reader) --[was aware of]--> (heart attack symptoms)\n\n================================================================================\n✓ Test complete!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================================\n# CLUSTERING SIGNATURES (DEFINE FIRST!)\n# ============================================================================\n\nclass ClusterValidator(dspy.Signature):\n    \"\"\"Verify if these entities belong in the same cluster.\n    A cluster should contain entities that are the same in meaning, with different:\n    - tenses, plural forms, stem forms, upper/lower cases\n    Or entities with close semantic meanings.\n    \n    Return ONLY valid JSON format: [\"entity1\", \"entity2\", \"entity3\"]\n    Return only entities you are confident belong together.\n    If not confident, return empty list [].\n    \"\"\"\n    \n    entities = dspy.InputField(desc=\"Entities to validate\")\n    valid_cluster = dspy.OutputField(desc=\"Validated cluster in JSON format\")\n\nprint(\"✓ ClusterValidator Signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:39:20.932898Z","iopub.execute_input":"2025-10-27T21:39:20.933173Z","iopub.status.idle":"2025-10-27T21:39:20.939386Z","shell.execute_reply.started":"2025-10-27T21:39:20.933147Z","shell.execute_reply":"2025-10-27T21:39:20.938615Z"}},"outputs":[{"name":"stdout","text":"✓ ClusterValidator Signature defined\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================================================\n# SEMANTIC SIMILARITY CLUSTERING (FROM PAPER)\n# ============================================================================\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nclass SemanticEntityClustering(dspy.Module):\n    def __init__(self, similarity_threshold=0.75):\n        super().__init__()\n        self.validator = dspy.ChainOfThought(ClusterValidator)\n        \n        # Load embedding model (same as paper)\n        print(\"Loading sentence transformer model...\")\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        print(\"✓ Model loaded\")\n        \n        self.similarity_threshold = similarity_threshold\n    \n    def _parse_cluster(self, text: str) -> List[str]:\n        \"\"\"Parse cluster from LLM response\"\"\"\n        try:\n            text = text.strip()\n            if '[' in text and ']' in text:\n                start = text.find('[')\n                end = text.rfind(']') + 1\n                text = text[start:end]\n            \n            cluster = json.loads(text)\n            if isinstance(cluster, list):\n                return [str(e).lower().strip() for e in cluster if e]\n            return []\n        except:\n            return []\n    \n    def _get_semantic_clusters(self, entities: List[str]) -> List[List[str]]:\n        \"\"\"Group entities by semantic similarity using embeddings\"\"\"\n        \n        if len(entities) == 0:\n            return []\n        \n        # Get embeddings for all entities\n        embeddings = self.model.encode(entities)\n        \n        # Compute pairwise cosine similarity\n        similarity_matrix = cosine_similarity(embeddings)\n        \n        # Find clusters using similarity threshold\n        clusters = []\n        remaining = set(range(len(entities)))\n        \n        for i in range(len(entities)):\n            if i not in remaining:\n                continue\n            \n            # Find all entities similar to this one\n            cluster_indices = [i]\n            remaining.discard(i)\n            \n            for j in range(i + 1, len(entities)):\n                if j not in remaining:\n                    continue\n                \n                # Check if similar enough\n                if similarity_matrix[i][j] >= self.similarity_threshold:\n                    cluster_indices.append(j)\n                    remaining.discard(j)\n            \n            # Convert indices to entity names\n            cluster = [entities[idx] for idx in cluster_indices]\n            \n            # Only keep clusters with 2-4 entities\n            if 2 <= len(cluster) <= 4:\n                clusters.append(cluster)\n            elif len(cluster) == 1:\n                # Keep singletons for later\n                pass\n        \n        return clusters\n    \n    def forward(self, entities: List[str]) -> Dict[str, List[str]]:\n        \"\"\"Semantic clustering with LLM validation\"\"\"\n        \n        print(f\"Starting semantic clustering with {len(entities)} entities...\")\n        print(f\"  Similarity threshold: {self.similarity_threshold}\")\n        \n        # Remove duplicates\n        unique_entities = list(set(entities))\n        \n        # Step 1: Find semantic clusters using embeddings\n        print(\"  Computing semantic similarities...\")\n        potential_clusters = self._get_semantic_clusters(unique_entities)\n        \n        print(f\"  Found {len(potential_clusters)} potential clusters\")\n        \n        # Step 2: Validate with LLM\n        validated_clusters = {}\n        cluster_id = 0\n        clustered_entities = set()\n        \n        for cluster in potential_clusters:\n            try:\n                # Ask LLM to validate\n                validation = self.validator(entities=json.dumps(cluster))\n                validated = self._parse_cluster(validation.valid_cluster)\n                \n                if validated and len(validated) >= 2:\n                    cluster_label = validated[0]\n                    validated_clusters[cluster_label] = validated\n                    \n                    for entity in validated:\n                        clustered_entities.add(entity)\n                    \n                    print(f\"  ✓ Cluster {cluster_id}: {validated}\")\n                    cluster_id += 1\n                else:\n                    # LLM rejected - add as singletons\n                    for entity in cluster:\n                        if entity not in clustered_entities:\n                            validated_clusters[entity] = [entity]\n                            clustered_entities.add(entity)\n            except:\n                # Error - add as singletons\n                for entity in cluster:\n                    if entity not in clustered_entities:\n                        validated_clusters[entity] = [entity]\n                        clustered_entities.add(entity)\n        \n        # Step 3: Add all remaining entities as singletons\n        for entity in unique_entities:\n            if entity not in clustered_entities:\n                validated_clusters[entity] = [entity]\n        \n        multi = sum(1 for v in validated_clusters.values() if len(v) > 1)\n        print(f\"✓ Semantic clustering complete: {len(validated_clusters)} total clusters\")\n        print(f\"  Multi-entity clusters: {multi}\")\n        print(f\"  Singleton entities: {len(validated_clusters) - multi}\")\n        \n        return validated_clusters\n\n# Create semantic clusterer with different thresholds\nentity_clusterer_semantic = SemanticEntityClustering(similarity_threshold=0.75)\nprint(\"\\n✓ Semantic Entity Clustering Module created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:39:20.940257Z","iopub.execute_input":"2025-10-27T21:39:20.940585Z","iopub.status.idle":"2025-10-27T21:39:27.715817Z","shell.execute_reply.started":"2025-10-27T21:39:20.940563Z","shell.execute_reply":"2025-10-27T21:39:27.714960Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec6a587110d3458785deec7ff8a85ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c43764ca8a441ada10a211158f5b11c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99abd5ec092b4e0cb7b7d1542db6351a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f331b5a681c644a7af386497b5d58b31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd4a890f2c7c449a963ae01d5cd1a5ca"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3909eef3c3c248b1944558fb91691e1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b2334988eb04b63aa49846fdae7c7d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0c2bb590bd649faadbe58e7a825b18b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"579a136b247f41bba6078ef99b856b9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3c9ada8a90a4d9f8f1a5f3a4f9277e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"791733d8b892422eaa2abc34e278498a"}},"metadata":{}},{"name":"stdout","text":"✓ Model loaded\n\n✓ Semantic Entity Clustering Module created\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ============================================================================\n# TEST SEMANTIC CLUSTERING\n# ============================================================================\n\nprint(\"\\nTesting SEMANTIC Clustering (Embeddings + LLM)...\")\nprint(\"=\" * 80)\n\ntest_entities = [\n    # Should cluster (same concept)\n    'phone', 'mobile phone', 'cellphone', 'cell phone',\n    \n    # Should cluster (same concept)\n    'usa', 'united states', 'america',\n    \n    # Should cluster (similar meaning)\n    'helping hand', 'assistant', 'helper', 'support',\n    \n    # Should cluster (same concept)\n    'service', 'services',\n    \n    # Should cluster (same concept)\n    'call', 'calling', 'calls',\n    \n    # Should NOT cluster (different brands)\n    'nokia', 'motorola', 'samsung',\n    \n    # Should NOT cluster (different concepts)\n    'signal', 'battery', 'screen',\n]\n\nprint(f\"Testing with {len(test_entities)} entities:\")\nfor e in test_entities:\n    print(f\"  - {e}\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Cluster with semantic method\nsemantic_clusters = entity_clusterer_semantic(test_entities)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SEMANTIC CLUSTERING RESULTS:\")\nprint(\"=\" * 80)\n\n# Show multi-entity clusters\nmulti_clusters = {k: v for k, v in semantic_clusters.items() if len(v) > 1}\n\nif multi_clusters:\n    print(f\"\\n✓ Found {len(multi_clusters)} semantic clusters:\")\n    for i, (label, members) in enumerate(multi_clusters.items(), 1):\n        print(f\"  {i}. {members}\")\n        \n        # Show similarity scores\n        if len(members) > 1:\n            embeddings = entity_clusterer_semantic.model.encode(members)\n            similarities = cosine_similarity(embeddings)\n            avg_sim = (similarities.sum() - len(members)) / (len(members) * (len(members) - 1))\n            print(f\"     Average similarity: {avg_sim:.3f}\")\nelse:\n    print(\"\\n  No clusters found\")\n\nprint(f\"\\nSingleton entities: {len(semantic_clusters) - len(multi_clusters)}\")\n\nprint(\"\\n✓ Test complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:39:27.716679Z","iopub.execute_input":"2025-10-27T21:39:27.716885Z","iopub.status.idle":"2025-10-27T21:39:34.585256Z","shell.execute_reply.started":"2025-10-27T21:39:27.716869Z","shell.execute_reply":"2025-10-27T21:39:34.584330Z"}},"outputs":[{"name":"stdout","text":"\nTesting SEMANTIC Clustering (Embeddings + LLM)...\n================================================================================\nTesting with 22 entities:\n  - phone\n  - mobile phone\n  - cellphone\n  - cell phone\n  - usa\n  - united states\n  - america\n  - helping hand\n  - assistant\n  - helper\n  - support\n  - service\n  - services\n  - call\n  - calling\n  - calls\n  - nokia\n  - motorola\n  - samsung\n  - signal\n  - battery\n  - screen\n\n================================================================================\nStarting semantic clustering with 22 entities...\n  Similarity threshold: 0.75\n  Computing semantic similarities...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f1684bac30440769bd252b4ed36c9be"}},"metadata":{}},{"name":"stdout","text":"  Found 3 potential clusters\n  ✓ Cluster 0: ['services', 'service']\n  ✓ Cluster 1: ['call', 'calls', 'calling']\n  ✓ Cluster 2: ['united states', 'america', 'usa']\n✓ Semantic clustering complete: 17 total clusters\n  Multi-entity clusters: 3\n  Singleton entities: 14\n\n================================================================================\nSEMANTIC CLUSTERING RESULTS:\n================================================================================\n\n✓ Found 3 semantic clusters:\n  1. ['services', 'service']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f833c9004c449f8124d2fd00125efe"}},"metadata":{}},{"name":"stdout","text":"     Average similarity: 0.921\n  2. ['call', 'calls', 'calling']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f23766501a4c63bdb32858293ef0e8"}},"metadata":{}},{"name":"stdout","text":"     Average similarity: 0.878\n  3. ['united states', 'america', 'usa']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4da81c3942e041d7994979586a6b506c"}},"metadata":{}},{"name":"stdout","text":"     Average similarity: 0.844\n\nSingleton entities: 14\n\n✓ Test complete!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ============================================================================\n# VISUALIZE HOW CLUSTERING WORKS\n# ============================================================================\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Test entities\ntest_entities = ['phone', 'mobile phone', 'cellphone', 'nokia', 'motorola']\n\n# Get embeddings\nembeddings = entity_clusterer_semantic.model.encode(test_entities)\n\n# Calculate similarities\nprint(\"Similarity Matrix:\")\nprint(\"=\" * 80)\nprint(f\"{'':15s}\", end='')\nfor e in test_entities:\n    print(f\"{e:15s}\", end='')\nprint()\n\nfor i, entity1 in enumerate(test_entities):\n    print(f\"{entity1:15s}\", end='')\n    for j, entity2 in enumerate(test_entities):\n        similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n        print(f\"{similarity:15.3f}\", end='')\n    print()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"Threshold: 0.75\")\nprint(\"=\" * 80)\n\n# Show which pairs would cluster\nprint(\"\\nPairs above threshold (would cluster together):\")\nfor i, entity1 in enumerate(test_entities):\n    for j, entity2 in enumerate(test_entities):\n        if i < j:  # Avoid duplicates\n            similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n            if similarity >= 0.75:\n                print(f\"  ✓ {entity1:15s} ↔ {entity2:15s} = {similarity:.3f}\")\n\nprint(\"\\nPairs below threshold (stay separate):\")\nfor i, entity1 in enumerate(test_entities):\n    for j, entity2 in enumerate(test_entities):\n        if i < j:\n            similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n            if similarity < 0.75:\n                print(f\"  ✗ {entity1:15s} ↔ {entity2:15s} = {similarity:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:39:34.585978Z","iopub.execute_input":"2025-10-27T21:39:34.586215Z","iopub.status.idle":"2025-10-27T21:39:34.637695Z","shell.execute_reply.started":"2025-10-27T21:39:34.586177Z","shell.execute_reply":"2025-10-27T21:39:34.637121Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87609e97a060433abe0fc4890c2ff84e"}},"metadata":{}},{"name":"stdout","text":"Similarity Matrix:\n================================================================================\n               phone          mobile phone   cellphone      nokia          motorola       \nphone                    1.000          0.814          0.845          0.681          0.644\nmobile phone             0.814          1.000          0.843          0.751          0.688\ncellphone                0.845          0.843          1.000          0.674          0.617\nnokia                    0.681          0.751          0.674          1.000          0.640\nmotorola                 0.644          0.688          0.617          0.640          1.000\n\n================================================================================\nThreshold: 0.75\n================================================================================\n\nPairs above threshold (would cluster together):\n  ✓ phone           ↔ mobile phone    = 0.814\n  ✓ phone           ↔ cellphone       = 0.845\n  ✓ mobile phone    ↔ cellphone       = 0.843\n  ✓ mobile phone    ↔ nokia           = 0.751\n\nPairs below threshold (stay separate):\n  ✗ phone           ↔ nokia           = 0.681\n  ✗ phone           ↔ motorola        = 0.644\n  ✗ mobile phone    ↔ motorola        = 0.688\n  ✗ cellphone       ↔ nokia           = 0.674\n  ✗ cellphone       ↔ motorola        = 0.617\n  ✗ nokia           ↔ motorola        = 0.640\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ============================================================================\n# FINAL KGGEN WITH SEMANTIC CLUSTERING\n# ============================================================================\n\nclass KGGenSemantic:\n    def __init__(self, similarity_threshold=0.75):\n        self.entity_extractor = entity_extractor\n        self.relation_extractor = relation_extractor\n        self.entity_clusterer = SemanticEntityClustering(similarity_threshold=similarity_threshold)\n        self.graph = nx.DiGraph()\n        self.entity_clusters = {}\n        \n    def generate_from_json(self, json_data: List[Dict], max_docs: int = None) -> nx.DiGraph:\n        \"\"\"Generate KG from JSON dataset\"\"\"\n        all_entities = set()\n        all_relations = []\n        \n        if max_docs:\n            json_data = json_data[:max_docs]\n        \n        print(f\"Processing {len(json_data)} documents...\")\n        print(\"=\" * 80)\n        \n        for idx, item in enumerate(json_data):\n            text = item.get('Article Text', '')\n            concepts = item.get('Concept', [])\n            \n            if not text or len(text.strip()) < 5:\n                continue\n            \n            try:\n                # Extract entities\n                entities = self.entity_extractor(text)\n                all_entities.update(entities)\n                \n                # Add concepts\n                for concept in concepts:\n                    if concept and isinstance(concept, str):\n                        all_entities.add(concept.lower().strip())\n                \n                # Extract relations\n                relations = self.relation_extractor(text, list(all_entities))\n                all_relations.extend(relations)\n                \n                if (idx + 1) % 20 == 0:\n                    print(f\"  {idx + 1}/{len(json_data)} docs | {len(all_entities)} entities | {len(all_relations)} relations\")\n                    \n            except Exception as e:\n                continue\n        \n        print(f\"\\n✓ Extraction complete!\")\n        print(f\"  Total entities: {len(all_entities)}\")\n        print(f\"  Total relations: {len(all_relations)}\")\n        \n        # Build graph\n        for subj, pred, obj in all_relations:\n            self.graph.add_edge(subj, obj, relation=pred)\n        \n        print(f\"  Graph nodes: {len(self.graph.nodes())}\")\n        print(f\"  Graph edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def cluster_entities(self):\n        \"\"\"Semantic clustering with embeddings\"\"\"\n        nodes = list(self.graph.nodes())\n        \n        if len(nodes) == 0:\n            print(\"No nodes to cluster!\")\n            return self.graph\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"SEMANTIC CLUSTERING: {len(nodes)} ENTITIES\")\n        print(f\"{'='*80}\")\n        \n        self.entity_clusters = self.entity_clusterer(nodes)\n        \n        # Map entities\n        entity_mapping = {}\n        for cluster_label, cluster_entities in self.entity_clusters.items():\n            for entity in cluster_entities:\n                entity_mapping[entity] = cluster_label\n        \n        # Rebuild graph\n        new_graph = nx.DiGraph()\n        for u, v, data in self.graph.edges(data=True):\n            new_u = entity_mapping.get(u, u)\n            new_v = entity_mapping.get(v, v)\n            relation = data.get('relation', 'related_to')\n            \n            if new_u == new_v:\n                continue\n            \n            if not new_graph.has_edge(new_u, new_v):\n                new_graph.add_edge(new_u, new_v, relation=relation)\n        \n        self.graph = new_graph\n        \n        print(f\"\\n✓ Clustering complete!\")\n        print(f\"  Final nodes: {len(self.graph.nodes())}\")\n        print(f\"  Final edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def save_graph(self, filepath: str):\n        data = nx.node_link_data(self.graph)\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n        print(f\"✓ Saved to {filepath}\")\n    \n    def export_triples(self, filepath: str):\n        triples = []\n        for u, v, data in self.graph.edges(data=True):\n            triples.append({\n                'subject': u,\n                'predicate': data.get('relation', 'related_to'),\n                'object': v\n            })\n        import pandas as pd\n        df = pd.DataFrame(triples)\n        df.to_csv(filepath, index=False)\n        print(f\"✓ Exported to {filepath}\")\n\n# Initialize semantic KGGen (threshold 0.75 = balanced)\nkg_gen_semantic = KGGenSemantic(similarity_threshold=0.75)\nprint(\"\\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:39:34.638506Z","iopub.execute_input":"2025-10-27T21:39:34.638778Z","iopub.status.idle":"2025-10-27T21:39:35.443879Z","shell.execute_reply.started":"2025-10-27T21:39:34.638756Z","shell.execute_reply":"2025-10-27T21:39:35.443275Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n✓ Model loaded\n\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# GENERATE KG FOR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\nall_article_kgs = []\n\ntotal = len(individual_articles)\nprint(f\"\\nProcessing {total} articles...\\n\")\n\nfor idx, article in enumerate(individual_articles[:100]):\n    article_id = article['id']\n    text = article['Article Text']\n    concepts = article['Concept']\n    \n    # Skip empty articles\n    if not text or len(text.strip()) < 5:\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n        continue\n    \n    try:\n        # Extract entities for THIS article\n        entities = entity_extractor(text)\n        \n        # Add original concepts as entities\n        for concept in concepts:\n            if concept and isinstance(concept, str):\n                entities.append(concept.lower().strip())\n        \n        entities = list(set(entities))  # Remove duplicates\n        \n        # Extract relations for THIS article\n        if entities:\n            relations = relation_extractor(text, entities)\n        else:\n            relations = []\n        \n        # Build graph for THIS article\n        graph = nx.DiGraph()\n        for subj, pred, obj in relations:\n            graph.add_edge(subj, obj, relation=pred)\n        \n        # Store everything for this article\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            'concepts': concepts,\n            'graph': graph,\n            'entities': entities,\n            'relations': relations,\n            'num_nodes': len(graph.nodes()),\n            'num_edges': len(graph.edges())\n        })\n        \n        # Progress update\n        if (idx + 1) % 50 == 0:\n            print(f\"✓ Processed {idx + 1}/{total} articles...\")\n        \n    except Exception as e:\n        print(f\"✗ Article {article_id}: Error - {str(e)[:100]}\")\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n\nprint(f\"\\n\" + \"=\" * 80)\nprint(\"✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\")\nprint(\"=\" * 80)\nprint(f\"Total articles processed: {len(all_article_kgs)}\")\nprint(f\"Articles with graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] > 0)}\")\nprint(f\"Articles without graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] == 0)}\")\n\n# Statistics\ntotal_entities = sum(len(kg['entities']) for kg in all_article_kgs)\ntotal_relations = sum(len(kg['relations']) for kg in all_article_kgs)\n\nprint(f\"\\nTotal entities extracted: {total_entities}\")\nprint(f\"Total relations extracted: {total_relations}\")\n\nwith_graphs = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\nif with_graphs:\n    avg_nodes = sum(kg['num_nodes'] for kg in with_graphs) / len(with_graphs)\n    avg_edges = sum(kg['num_edges'] for kg in with_graphs) / len(with_graphs)\n    print(f\"\\nAverage nodes per KG: {avg_nodes:.2f}\")\n    print(f\"Average edges per KG: {avg_edges:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T21:39:35.444556Z","iopub.execute_input":"2025-10-27T21:39:35.444735Z","iopub.status.idle":"2025-10-27T22:01:13.530701Z","shell.execute_reply.started":"2025-10-27T21:39:35.444721Z","shell.execute_reply":"2025-10-27T22:01:13.529895Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nGENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\n================================================================================\n\nProcessing 100 articles...\n\n✓ Processed 50/100 articles...\n✓ Processed 100/100 articles...\n\n================================================================================\n✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\n================================================================================\nTotal articles processed: 100\nArticles with graphs: 100\nArticles without graphs: 0\n\nTotal entities extracted: 2701\nTotal relations extracted: 1425\n\nAverage nodes per KG: 19.47\nAverage edges per KG: 13.52\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ============================================================================\n# CHECK WHAT'S BEEN SUCCESSFULLY PROCESSED\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"CHECKING SUCCESSFUL PROCESSING\")\nprint(\"=\" * 80)\n\n# Count successes\nsuccessful = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\nfailed = [kg for kg in all_article_kgs if kg['num_edges'] == 0]\n\nprint(f\"\\nTotal articles: {len(all_article_kgs)}\")\nprint(f\"✓ Successfully processed: {len(successful)}\")\nprint(f\"✗ Failed/Empty: {len(failed)}\")\n\nif successful:\n    print(f\"\\nSuccessful articles with KGs:\")\n    for kg in successful[:10]:\n        print(f\"  Article {kg['id']}: {kg['num_nodes']} nodes, {kg['num_edges']} edges\")\n    \n    # Export the successful ones\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXPORTING SUCCESSFUL RESULTS\")\n    print(\"=\" * 80)\n    \n    import json\n    import os\n    \n    # Create output directory\n    os.makedirs('/kaggle/working/individual_kgs', exist_ok=True)\n    \n    # Export successful KGs\n    for kg in successful:\n        article_id = kg['id']\n        \n        graph_data = {\n            'article_id': article_id,\n            'text': kg['text'],\n            'concepts': kg['concepts'],\n            'entities': kg['entities'],\n            'relations': [\n                {'subject': s, 'predicate': p, 'object': o}\n                for s, p, o in kg['relations']\n            ],\n            'graph': nx.node_link_data(kg['graph'])\n        }\n        \n        filepath = f'/kaggle/working/individual_kgs/article_{article_id}.json'\n        with open(filepath, 'w') as f:\n            json.dump(graph_data, f, indent=2)\n    \n    print(f\"✓ Saved {len(successful)} JSON files\")\n    \n    # Export triples\n    all_triples = []\n    for kg in successful:\n        for subj, pred, obj in kg['relations']:\n            all_triples.append({\n                'article_id': kg['id'],\n                'subject': subj,\n                'predicate': pred,\n                'object': obj\n            })\n    \n    triples_df = pd.DataFrame(all_triples)\n    triples_df.to_csv('/kaggle/working/successful_triples.csv', index=False)\n    print(f\"✓ Saved {len(all_triples)} triples to successful_triples.csv\")\n    \n    # Summary\n    summary = []\n    for kg in all_article_kgs:\n        summary.append({\n            'article_id': kg['id'],\n            'text_preview': kg['text'][:100],\n            'num_entities': len(kg['entities']),\n            'num_relations': len(kg['relations']),\n            'num_nodes': kg['num_nodes'],\n            'num_edges': kg['num_edges'],\n            'status': 'success' if kg['num_edges'] > 0 else 'failed'\n        })\n    \n    summary_df = pd.DataFrame(summary)\n    summary_df.to_csv('/kaggle/working/processing_summary.csv', index=False)\n    print(f\"✓ Saved summary\")\n    \n    print(\"\\n✓ DONE! Downloaded what we could process.\")\nelse:\n    print(\"\\n✗ No successful KGs to export\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:13.531605Z","iopub.execute_input":"2025-10-27T22:01:13.531884Z","iopub.status.idle":"2025-10-27T22:01:13.600075Z","shell.execute_reply.started":"2025-10-27T22:01:13.531860Z","shell.execute_reply":"2025-10-27T22:01:13.599320Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCHECKING SUCCESSFUL PROCESSING\n================================================================================\n\nTotal articles: 100\n✓ Successfully processed: 100\n✗ Failed/Empty: 0\n\nSuccessful articles with KGs:\n  Article 0: 46 nodes, 31 edges\n  Article 1: 20 nodes, 16 edges\n  Article 2: 12 nodes, 7 edges\n  Article 3: 27 nodes, 18 edges\n  Article 4: 13 nodes, 10 edges\n  Article 5: 17 nodes, 9 edges\n  Article 6: 46 nodes, 24 edges\n  Article 7: 18 nodes, 16 edges\n  Article 8: 15 nodes, 13 edges\n  Article 9: 36 nodes, 27 edges\n\n================================================================================\nEXPORTING SUCCESSFUL RESULTS\n================================================================================\n✓ Saved 100 JSON files\n✓ Saved 1425 triples to successful_triples.csv\n✓ Saved summary\n\n✓ DONE! Downloaded what we could process.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/networkx/readwrite/json_graph/node_link.py:145: FutureWarning: \nThe default value will be `edges=\"edges\" in NetworkX 3.6.\n\nTo make this warning go away, explicitly set the edges kwarg, e.g.:\n\n  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ============================================================================\n# VIEW SUCCESSFUL RESULTS\n# ============================================================================\n\nimport pandas as pd\n\nprint(\"=\" * 80)\nprint(\"YOUR 46 SUCCESSFUL KNOWLEDGE GRAPHS\")\nprint(\"=\" * 80)\n\n# Load summary\nsummary_df = pd.read_csv('/kaggle/working/processing_summary.csv')\nsuccessful_df = summary_df[summary_df['status'] == 'success'].copy()\n\nprint(f\"\\nTop 10 Largest KGs:\")\nprint(\"-\" * 80)\ntop_kgs = successful_df.nlargest(10, 'num_edges')\nprint(top_kgs[['article_id', 'num_nodes', 'num_edges', 'text_preview']].to_string(index=False))\n\n# Load triples\ntriples_df = pd.read_csv('/kaggle/working/successful_triples.csv')\n\nprint(f\"\\n\\nSample Triples:\")\nprint(\"-\" * 80)\nprint(triples_df.head(20).to_string(index=False))\n\nprint(f\"\\n\\nStatistics:\")\nprint(\"-\" * 80)\nprint(f\"Total successful articles: {len(successful_df)}\")\nprint(f\"Total triples extracted: {len(triples_df)}\")\nprint(f\"Avg edges per KG: {successful_df['num_edges'].mean():.2f}\")\nprint(f\"Max edges in a KG: {successful_df['num_edges'].max()}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FILES READY TO DOWNLOAD:\")\nprint(\"=\" * 80)\nprint(\"  📁 /kaggle/working/individual_kgs/ (46 JSON files)\")\nprint(\"  📄 /kaggle/working/successful_triples.csv\")\nprint(\"  📄 /kaggle/working/processing_summary.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:13.600811Z","iopub.execute_input":"2025-10-27T22:01:13.601046Z","iopub.status.idle":"2025-10-27T22:01:13.637025Z","shell.execute_reply.started":"2025-10-27T22:01:13.601017Z","shell.execute_reply":"2025-10-27T22:01:13.636466Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nYOUR 46 SUCCESSFUL KNOWLEDGE GRAPHS\n================================================================================\n\nTop 10 Largest KGs:\n--------------------------------------------------------------------------------\n article_id  num_nodes  num_edges                                                                                           text_preview\n         28         44         32   Reading medical journals is the main occupational hazard I face as editor of the Harvard Heart Lette\n          0         46         31 1. Health care reform\\n\\nHow could the health care reform legislation that President Barack Obama sign\n         10         46         31 1. Health care reform\\n\\nHow could the health care reform legislation that President Barack Obama sign\n          9         36         27   The contents displayed within this public group(s), such as text, graphics, and other material (\"Con\n         19         36         27   The contents displayed within this public group(s), such as text, graphics, and other material (\"Con\n         26         38         27   On October 16, 1846, Dr. John Collins Warren, a renowned surgeon at Massachusetts General Hospital, \n          6         46         24   Only one-third of people with major depression achieve remission after trying one antidepressant. Wh\n         16         46         24   Only one-third of people with major depression achieve remission after trying one antidepressant. Wh\n         74         29         23   Limit salt. That piece of dietary advice has been around for decades. Once aimed at individuals, it \n         84         29         23   Limit salt. That piece of dietary advice has been around for decades. Once aimed at individuals, it \n\n\nSample Triples:\n--------------------------------------------------------------------------------\n article_id                                    subject                                                     predicate                                      object\n          0                     president barack obama                                               signed into law  patient protection and affordable care act\n          0 patient protection and affordable care act                                       designed to patch holes                     health insurance system\n          0 patient protection and affordable care act                                            extend coverage to                        32 million americans\n          0                       health care spending                                        accounts for more than 17% of the country's gross domestic product\n          0                 health insurance exchanges                                            should be in place                                        2014\n          0                           medicaid program                                      scheduled to be expanded                                        2014\n          0                            high-risk pools               created for people with pre-existing conditions                                        2010\n          0                               health plans                 required to extend coverage to adult children                                up to age 26\n          0                      indoor tanning salons                                          imposed a 10% tax on                                        2010\n          0              cardiopulmonary resuscitation                                     issued new guidelines for                  american heart association\n          0              cardiopulmonary resuscitation                         continue the trend toward simplifying                  american heart association\n          0              cardiopulmonary resuscitation                                emphasizing chest compressions                  american heart association\n          0                            shinya yamanaka                discovered a technique for reprogramming cells              induced pluripotent stem cells\n          0                            shinya yamanaka                        created induced pluripotent stem cells                 harvard stem cell institute\n          0                              derrick rossi                    reported results demonstrating a technique              induced pluripotent stem cells\n          0                                 telomerase                                             turned on and off                                        mice\n          0                                 telomerase promptly, the brain and other shrunken organs started to grow                                        mice\n          0                                 telomerase                                recovered their sense of smell                                        mice\n          0  concerns about sports-related concussions                             growing as evidence has increased                      permanent brain damage\n          0                   national football league                      started to fine players for illegal hits                                        2010\n\n\nStatistics:\n--------------------------------------------------------------------------------\nTotal successful articles: 100\nTotal triples extracted: 1425\nAvg edges per KG: 13.52\nMax edges in a KG: 32\n\n================================================================================\nFILES READY TO DOWNLOAD:\n================================================================================\n  📁 /kaggle/working/individual_kgs/ (46 JSON files)\n  📄 /kaggle/working/successful_triples.csv\n  📄 /kaggle/working/processing_summary.csv\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ============================================================================\n# VISUALIZE INDIVIDUAL ARTICLE KNOWLEDGE GRAPHS\n# ============================================================================\n\n!pip install pyvis --quiet\n\nfrom pyvis.network import Network\nimport networkx as nx\nimport os\n\ndef visualize_article_kg(kg_data, output_file):\n    \"\"\"\n    Create interactive HTML visualization for one article's KG\n    \"\"\"\n    graph = kg_data['graph']\n    article_id = kg_data['id']\n    \n    if len(graph.nodes()) == 0:\n        print(f\"  Article {article_id}: No graph to visualize\")\n        return None\n    \n    print(f\"  Article {article_id}: {len(graph.nodes())} nodes, {len(graph.edges())} edges\")\n    \n    # Create pyvis network\n    net = Network(\n        height='800px',\n        width='100%',\n        bgcolor='#ffffff',\n        font_color='black',\n        directed=True,\n        notebook=False\n    )\n    \n    # Set physics options\n    net.set_options(\"\"\"\n    {\n      \"physics\": {\n        \"enabled\": true,\n        \"barnesHut\": {\n          \"gravitationalConstant\": -8000,\n          \"centralGravity\": 0.3,\n          \"springLength\": 95,\n          \"springConstant\": 0.04\n        },\n        \"minVelocity\": 0.75\n      },\n      \"nodes\": {\n        \"font\": {\"size\": 16}\n      },\n      \"edges\": {\n        \"font\": {\"size\": 12},\n        \"arrows\": {\"to\": {\"enabled\": true, \"scaleFactor\": 0.5}}\n      }\n    }\n    \"\"\")\n    \n    # Add nodes with colors based on degree\n    for node in graph.nodes():\n        degree = graph.degree(node)\n        size = 20 + degree * 5\n        \n        # Color based on degree\n        if degree > 5:\n            color = '#e74c3c'  # Red for highly connected\n        elif degree > 2:\n            color = '#3498db'  # Blue for medium\n        else:\n            color = '#95a5a6'  # Gray for low\n        \n        net.add_node(\n            node, \n            label=node, \n            size=size, \n            title=f\"{node}\\nConnections: {degree}\",\n            color=color\n        )\n    \n    # Add edges with labels\n    for source, target, data in graph.edges(data=True):\n        relation = data.get('relation', 'related_to')\n        net.add_edge(source, target, label=relation, title=relation)\n    \n    # Save\n    net.save_graph(output_file)\n    \n    return net\n\n# Create visualizations directory\nos.makedirs('/kaggle/working/kg_visualizations', exist_ok=True)\n\nprint(\"=\" * 80)\nprint(\"CREATING INTERACTIVE VISUALIZATIONS\")\nprint(\"=\" * 80)\n\n# Visualize all successful KGs\nvisualized = 0\nfor kg in all_article_kgs:\n    if kg['num_edges'] > 0:  # Only visualize articles with graphs\n        article_id = kg['id']\n        output_file = f'/kaggle/working/kg_visualizations/article_{article_id}_graph.html'\n        visualize_article_kg(kg, output_file)\n        visualized += 1\n\nprint(f\"\\n✓ Created {visualized} interactive visualizations!\")\nprint(f\"  Location: /kaggle/working/kg_visualizations/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:13.637677Z","iopub.execute_input":"2025-10-27T22:01:13.637909Z","iopub.status.idle":"2025-10-27T22:01:19.036948Z","shell.execute_reply.started":"2025-10-27T22:01:13.637887Z","shell.execute_reply":"2025-10-27T22:01:19.036243Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h================================================================================\nCREATING INTERACTIVE VISUALIZATIONS\n================================================================================\n  Article 0: 46 nodes, 31 edges\n  Article 1: 20 nodes, 16 edges\n  Article 2: 12 nodes, 7 edges\n  Article 3: 27 nodes, 18 edges\n  Article 4: 13 nodes, 10 edges\n  Article 5: 17 nodes, 9 edges\n  Article 6: 46 nodes, 24 edges\n  Article 7: 18 nodes, 16 edges\n  Article 8: 15 nodes, 13 edges\n  Article 9: 36 nodes, 27 edges\n  Article 10: 46 nodes, 31 edges\n  Article 11: 20 nodes, 16 edges\n  Article 12: 12 nodes, 7 edges\n  Article 13: 27 nodes, 18 edges\n  Article 14: 13 nodes, 10 edges\n  Article 15: 17 nodes, 9 edges\n  Article 16: 46 nodes, 24 edges\n  Article 17: 18 nodes, 16 edges\n  Article 18: 15 nodes, 13 edges\n  Article 19: 36 nodes, 27 edges\n  Article 20: 27 nodes, 19 edges\n  Article 21: 22 nodes, 18 edges\n  Article 22: 27 nodes, 16 edges\n  Article 23: 3 nodes, 2 edges\n  Article 24: 35 nodes, 19 edges\n  Article 25: 18 nodes, 11 edges\n  Article 26: 38 nodes, 27 edges\n  Article 27: 27 nodes, 17 edges\n  Article 28: 44 nodes, 32 edges\n  Article 29: 22 nodes, 17 edges\n  Article 30: 20 nodes, 11 edges\n  Article 31: 2 nodes, 1 edges\n  Article 32: 7 nodes, 8 edges\n  Article 33: 29 nodes, 15 edges\n  Article 34: 15 nodes, 8 edges\n  Article 35: 7 nodes, 4 edges\n  Article 36: 14 nodes, 9 edges\n  Article 37: 6 nodes, 3 edges\n  Article 38: 19 nodes, 14 edges\n  Article 39: 10 nodes, 6 edges\n  Article 40: 11 nodes, 9 edges\n  Article 41: 22 nodes, 18 edges\n  Article 42: 20 nodes, 13 edges\n  Article 43: 16 nodes, 12 edges\n  Article 44: 12 nodes, 9 edges\n  Article 45: 21 nodes, 11 edges\n  Article 46: 15 nodes, 14 edges\n  Article 47: 13 nodes, 9 edges\n  Article 48: 13 nodes, 13 edges\n  Article 49: 18 nodes, 13 edges\n  Article 50: 19 nodes, 17 edges\n  Article 51: 15 nodes, 13 edges\n  Article 52: 12 nodes, 10 edges\n  Article 53: 13 nodes, 9 edges\n  Article 54: 18 nodes, 13 edges\n  Article 55: 15 nodes, 9 edges\n  Article 56: 11 nodes, 10 edges\n  Article 57: 18 nodes, 13 edges\n  Article 58: 12 nodes, 13 edges\n  Article 59: 22 nodes, 13 edges\n  Article 60: 14 nodes, 9 edges\n  Article 61: 12 nodes, 8 edges\n  Article 62: 14 nodes, 9 edges\n  Article 63: 10 nodes, 6 edges\n  Article 64: 10 nodes, 9 edges\n  Article 65: 9 nodes, 6 edges\n  Article 66: 17 nodes, 13 edges\n  Article 67: 20 nodes, 14 edges\n  Article 68: 25 nodes, 16 edges\n  Article 69: 27 nodes, 14 edges\n  Article 70: 27 nodes, 19 edges\n  Article 71: 21 nodes, 12 edges\n  Article 72: 13 nodes, 12 edges\n  Article 73: 21 nodes, 14 edges\n  Article 74: 29 nodes, 23 edges\n  Article 75: 13 nodes, 13 edges\n  Article 76: 17 nodes, 17 edges\n  Article 77: 23 nodes, 12 edges\n  Article 78: 25 nodes, 16 edges\n  Article 79: 27 nodes, 14 edges\n  Article 80: 27 nodes, 19 edges\n  Article 81: 21 nodes, 12 edges\n  Article 82: 13 nodes, 12 edges\n  Article 83: 21 nodes, 14 edges\n  Article 84: 29 nodes, 23 edges\n  Article 85: 13 nodes, 13 edges\n  Article 86: 17 nodes, 17 edges\n  Article 87: 23 nodes, 12 edges\n  Article 88: 24 nodes, 12 edges\n  Article 89: 13 nodes, 10 edges\n  Article 90: 3 nodes, 2 edges\n  Article 91: 10 nodes, 6 edges\n  Article 92: 26 nodes, 15 edges\n  Article 93: 20 nodes, 16 edges\n  Article 94: 17 nodes, 10 edges\n  Article 95: 14 nodes, 10 edges\n  Article 96: 9 nodes, 8 edges\n  Article 97: 18 nodes, 10 edges\n  Article 98: 29 nodes, 22 edges\n  Article 99: 18 nodes, 13 edges\n\n✓ Created 100 interactive visualizations!\n  Location: /kaggle/working/kg_visualizations/\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# DIAGNOSTIC: Check KG Structure\n# ============================================================================\n\n# Check what's actually in the first KG\nprint(\"Checking structure of all_article_kgs...\")\nprint(f\"Total KGs: {len(all_article_kgs)}\\n\")\n\n# Look at first KG\nfirst_kg = all_article_kgs[0]\nprint(\"First KG keys:\")\nprint(first_kg.keys())\nprint(\"\\nFirst KG structure:\")\nfor key, value in first_kg.items():\n    print(f\"  {key}: {type(value)}\")\n    if key not in ['graph']:  # Don't print the whole graph\n        print(f\"    Value: {value}\")\n\nprint(\"\\n\" + \"=\"*80)\n\n# If there's a 'graph' key with NetworkX object, extract its data\nif 'graph' in first_kg:\n    graph = first_kg['graph']\n    print(\"\\nNetworkX Graph found!\")\n    print(f\"  Nodes: {list(graph.nodes())[:10]}\")  # First 10 nodes\n    print(f\"  Number of nodes: {graph.number_of_nodes()}\")\n    print(f\"  Number of edges: {graph.number_of_edges()}\")\n    \n    # Show a few edges\n    edges_sample = list(graph.edges(data=True))[:5]\n    print(f\"\\n  Sample edges:\")\n    for source, target, data in edges_sample:\n        print(f\"    {source} -> {target}, data: {data}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.037739Z","iopub.execute_input":"2025-10-27T22:01:19.037991Z","iopub.status.idle":"2025-10-27T22:01:19.045627Z","shell.execute_reply.started":"2025-10-27T22:01:19.037965Z","shell.execute_reply":"2025-10-27T22:01:19.044769Z"}},"outputs":[{"name":"stdout","text":"Checking structure of all_article_kgs...\nTotal KGs: 100\n\nFirst KG keys:\ndict_keys(['id', 'text', 'concepts', 'graph', 'entities', 'relations', 'num_nodes', 'num_edges'])\n\nFirst KG structure:\n  id: <class 'int'>\n    Value: 0\n  text: <class 'str'>\n    Value: 1. Health care reform\n\nHow could the health care reform legislation that President Barack Obama signed into law on March 23, 2010, not be the #1 story of the year? Whether you are for or against it, the Patient Protection and Affordable Care Act is nothing if not ambitious, and if implemented, it will fundamentally alter how American health care is financed and perhaps delivered. The law is designed to patch holes in the health insurance system and extend coverage to 32 million Americans by 2019 while also reining in health care spending, which now accounts for more than 17% of the country’s gross domestic product. The biggest changes aren’t scheduled to occur until 2014, when most people will be required to have health insurance or pay a penalty (the so-called individual mandate) and when state-level health insurance exchanges should be in place. The Medicaid program is also scheduled to be expanded that year so that it covers more people, and subsidized insurance will be available through the exchanges for people in lower- and middle-income brackets. But plenty is happening before 2014. The 1,000-page law contains hundreds of provisions, and they’re being rolled out in phases. This year, for example, the law created high-risk pools for people with pre-existing conditions, required health plans to extend coverage to adult children up to age 26, and imposed a 10% tax on indoor tanning salons. Next year, about 20 different provisions are scheduled to take effect, including the elimination of copayments for many preventive services for Medicare enrollees, the imposition of limits on non-medical spending by health plans, and the creation of a voluntary insurance that will help pay for home health care and other long-term care services received outside a nursing home. Getting a handle on the complicated law is difficult. If you’re looking for a short course, the Kaiser Family Foundation has created an excellent timeline of the law’s implementation (we depended on it for this post) and a short (nine minutes) animated video that’s one of the best (and most amusing) overviews available. The big question now is whether the sweeping health care law can survive various legal and political challenges. In December, a federal judge in Virginia ruled that the individual mandate was unconstitutional. Meanwhile, congressional Republicans have vowed to thwart the legislation, and if the party were to win the White House and control of the Senate in the 2012 election, Republicans would be in a position to follow through on their threats to repeal it.\n\n2. Smartphones, medical apps, and remote monitoring\n\nSmartphones and tablet computers are making it easier to get health care information, advice, and reminders on an anywhere-and-anytime basis. Hundreds of health and medical apps for smartphones like the iPhone became available this year. Some are just for fun. Others provide useful information (calorie counters, first aid and CPR instructions) or perform calculations. Even the federal government is getting into the act: the app store it opened this summer has several free health-related apps, including one called My Dietary Supplements for keeping track of vitamins and supplements and another one from the Environmental Protection Agency that allows you to check the UV index and air quality wherever you are. Smartphones are also being used with at-home monitoring devices; for example, glucose meters have been developed that send blood sugar readings wirelessly to an app on a smartphone. The number of doctors using apps and mobile devices is increasing, a trend that is likely to accelerate as electronic health records become more common. Check out iMedicalapps if you want to see the apps your doctor might be using or talking about. It has become a popular Web site for commentary and critiques of medical apps for doctors and medical students. Meanwhile, the FDA is wrestling with the issue of how tightly it should regulate medical apps. Some adverse events resulting from programming errors have been reported to the agency. Medical apps are part of a larger “e-health” trend toward delivering health care reminders and advice remotely with the help of computers and phones of all types. These phone services are being used in combination with increasingly sophisticated at-home monitoring devices. Research results have been mixed. Simple, low-cost text messages have been shown to be effective in getting people wear sunscreen. But one study published this year found that regular telephone contact and at-home monitoring of heart failure patients had no effect on hospitalizations of death from any cause over a six-month period. Another study found that remote monitoring did lower hospital readmission rates among heart failure patients, although the difference between remote monitoring and regular care didn’t reach statistical significance.\n\n3. New CPR guidelines\n\nThe American Heart Association issued new guidelines for cardiopulmonary resuscitation (CPR) this year that continue the trend toward simplifying CPR and emphasizing chest compressions. For trained rescuers, the guidelines change the CPR sequence from airway, breathing, and chest compressions and the A-B-C mnemonic to putting chest compressions first, followed by checks of the airway and breathing, or C-A-B. People who haven’t had CPR training are encouraged to do only chest compressions because they are easier and “more readily guided by dispatchers over the telephone.” The compressions should be fast (about 100 times a minute) and hard (so that the chest goes down by two inches or more). The American Heart Association produced a very good video about the guidelines that’s well worth watching. Fewer than half of those who suffer cardiac arrest receive CPR, so the hope is that more people will give CPR—and do so quickly— if it’s simpler and doesn’t involve mouth-to-mouth breathing. The guidelines note that the results for chest compression–only CPR are similar to those for traditional CPR for adults in cardiac arrest outside of the hospital. But conventional CPR is still better for children because cardiac arrest in children is usually preceded by a lack of breathing, so the mouth-to-mouth breaths are needed to restore oxygen levels in the blood. Research results reported this year in The Lancet, Journal of the American Medical Association, and The New England Journal of Medicine all suggested that in most cardiac arrest cases, chest compression–only CPR is as effective, if not more so, than conventional CPR.\n\n4. Making stem cells\n\nHeart attacks, strokes, and many other conditions destroy cells, and for years, scientists believed that it was impossible to make replacements. Then, four years ago, a Japanese researcher, Shinya Yamanaka, discovered a technique for reprogramming cells back into stem cells, so that they would function like a biological blank slate and be capable of turning into any other type of cell. Dr. Yamanka called his creations induced pluripotent stem cells, and a whole new frontier of stem cell research opened up. Scientists have since figured out ways to turn one cell type directly into another type: blood vessel cells have been turned into bone and fat cells, and skin cells have been turned into blood cells. And this year, stem cell research took another leap forward when a Harvard researcher, Derrick Rossi, reported results demonstrating a technique that may make the creation of induced pluripotent stem cells a lot easier and safer. Rossi and his colleagues at the Harvard Stem Cell Institute reprogrammed adult skin cells with synthetic messenger RNA that leaves DNA intact, instead of inserting genes into DNA. Research with embryonic stem cells remains important, and in October 2010, Geron, a California biotech company, began enrolling people in a trial to test the safety of using cells derived from embryonic stem cells to treat spinal cord injuries. But researchers are also making some remarkable progress toward turning readily available cells, such as skin or blood cells, into other types of cells. These new cells would be genetically identical to other cells in the body and therefore shouldn’t be rejected by the immune system when they’re transplanted to replace cells lost to disease.\n\n5. Heightened awareness of concussions\n\nConcern about sports-related concussions, especially in football, has been growing as evidence has increased that repeated concussions can cause permanent brain damage over the long term, even if the short-term effects are fairly mild (most concussions do not result in a loss of consciousness, for example) and CT and MRI scans are normal. Some researchers are calling concussion-related brain damage chronic traumatic encephalopathy (encephalopathy is a catchall term for any degenerative disease of the brain). There were several efforts in 2010 to reduce the number and severity of concussions. The National Football League started to fine players for illegal hits this season. The American Academy of Neurology came out with a position paper that says any athlete who might have suffered a concussion shouldn’t be allowed to partcipate again until he or she has been evaluated by a doctor with training in the evalulation and management of sports concussions. The American Academy of Pediatrics released a report about sports-related concussions in children and adolescents that says younger people often need more time (7 to 10 days or even longer) to recover from a concussion than college or professional athletes. Several states have passed laws requiring high schools to have concussion management programs. The concussion risk is greatest for football players, but girl basketball and soccer players also have relatively high rates. Meanwhile, research into concussions continues. Boston University researchers who have been prominent in the field caused a stir with a finding that linked concussions and chronic traumatic encephalopathy to amyotrophic lateral sclerosis (Lou Gherig’s disease).\n\n6. An anti-aging possibility\n\nResearchers at the Harvard-affliliated Dana-Farber Cancer Institute reported results this year that kindled hopes for altering the fundamental biology of aging. Their experiment involved mice that had been genetically engineered so that an enzyme called telomerase that is known to be important in the aging of cells could be turned on and off. When the enzyme was turned off, the mice aged prematurely. When they reached the chronological equivalent of adolescence, they appeared to be biologically very old: their brains and other organs had shrunk and were starting to fail. Then the scientists turned on the enzyme. Promptly, the brain and other shrunken organs started to grow with new cells, and organ failure ceased. The animals recovered their sense of smell. You might say the mice became adolescents again. Of course, what works in mice doesn’t always work in humans. There are concerns that the activation of telomerase could cause cancer, although that didn’t occur in this particular experiment. And this is very much an experimental finding; at this point, all those products making anti-aging claims are way ahead of the game and not to be trusted. Still, along with other research, this study hints at a future when it might be possible to slow down biological aging and possibly prevent some of the diseases associated with it.\n\n7. CT scans for lung cancer screening\n\nMore Americans die from lung cancer than from any other type of cancer, yet there’s no accepted screening test for the disease. Study results reported this year may change that situation. The National Cancer Institute (NCI) stopped the National Lung Screening Trial comparing CT scans to chest x-rays earlier than expected because the CT scans appeared to be so effective at reducing lung cancer deaths. The trial included 53,000 current and former heavy (30 pack years or more) smokers. Results released in October showed that over a five-year period, 354 of those screened with CT scans died from lung cancer (or about 1.4%) compared with 442 of those screened with chest x-rays (about 1.7%). Catching any cancer at an earlier, more treatable stage is an appealing idea, and especially lung cancer, because of its high mortality rate. But screening tests have become more controversial lately because of concerns that they lead to overdiagnosis and overtreatment. Almost one in every four people who were screened with CT scans in the National Lung Cancer Screening Trial had a false positive, the finding of an abnormality that turns out not to be cancer. There are also concerns about radiation exposure from CT scans and whether scans for lung cancer will add to that problem.\n\n8. New vitamin D guidelines\n\nAfter years of debate, discussion, and research, the Institute of Medicine (IOM) issued new vitamin D guidelines this year. The Recommended Dietary Allowance (RDA) is now 600 international units (IU) a day for people ages 1 to 70 and 800 IU a day for those 71 and older. The previous guidelines, set in 1997, recommended a daily intake of 200 IU through age 50, 400 IU between the ages of 51 and 70, and 600 IU for those 71 and older. The IOM panel also established a new safe upper limit of 4,000 IU a day, double the old limit of 2,000 IU. The new guidelines were criticized as being too conservative by many experts, who would have preferred an RDA closer to 1,000 IU a day and a blood level target of 30 ng/ml (75 nmol/l) for the vitamin, not the 20 ng/ml (50 nmol/l) set by the IOM panel. The difference of opinion stems, in part, from the fact that the IOM panel gave results from randomized clincial trials (RCTs) far more weight than results from other types of studies. As a result, the panel found evidence that vitamin D benefits bone and little else. If other kinds of studies are taken into account, a case can be made that blood levels of 30 ng/ml or even higher would result in optimal bone health and that the vitamin has a wide range of health benefits beyond bone, including protective effects against some cancers (especially colon cancer) and some autoimmune disorders. The debate about vitamin D is bound to continue. Soon after the IOM panel released its report, a different set of experts, the U.S. Preventive Services Task Force, came out with fall-prevention recommendations that include an endorsement of vitamin D.\n\n9. Alternatives to warfarin\n\nThe FDA approved one alternative to warfarin this year, a drug called dabigatran (sold as Pradaxa). Another alternative, rivaroxaban, seems to be waiting in the wings after largely favorable results were reported this year (here and here) from trials testing the drug in patients with deep-vein thrombosis and atrial fibrillation. A third drug, apixaban, which is related to rivaroxaban, is also looking promising. Warfarin (the brand-name version is called Coumadin) has been the mainstay for preventing blood clots for decades, but it’s a tricky, high-maintenance drug that requires frequent blood tests to make sure the dose is producing the desired results: enough anti-coagulation to prevent blood clots but not so much as to cause bleeding . Warfarin also interacts with many foods and drugs. In contrast, these warfarin alternatives seem simple as pie: they can be given in fixed doses, don’t require blood monitoring, and don’t seem to pose interteaction problems. Cost, however, will be a barrier. Drug companies set high prices for new brand-name drugs. Warfarin, widely available as a generic, is relatively cheap. And there’s always the possibility of unforeseen side effects once the new drugs are more widely used. Still, millions of people stand to benefit if good alternatives to difficult-to-use warfarin pan out.\n\n10. Concerns about bisphosphonates\n\nBisphosphonates are prescribed to prevent and treat osteoporosis, a decrease in bone density that makes fractures more likely. Well-known brands include Fosamax (alendronate) and Actonel (risedronate). Millions of people, most of them postmenopausal women, take bisphosphonates; for the most part, they are safe and effective medications that have been shown to cut the risk of fractures by 50%. But concerns about ill effects from long-term use have been growing. In October, the FDA issued a new warning about bisphosphonates increasing the risk of a rare kind of thighbone (femur) fracture. Two years ago, the agency issued a different warning about the bone drugs causing bone, joint, and muscle pain. There have also been reports about a small percentage of bisphosphonate users developing osteonecrosis in their jawbones, although most of those cases have occurred in cancer patients who have received high intravenous doses (bisphosphonates can relieve pain and strengthen bone if cancer has spread to the bone). Some doctors are now recommending “drug holidays” for people who take bisphosphontes for osteoporosis for extended periods. Other bone-building drugs, such as denosumab (sold as Prolia), which was approved by the FDA this year, may get a closer look because of concerns about the side effects of bisphosphonates. And perhaps the nonpharmacological ways to strengthen bones will gain some adherents. Pill-free bone builders include a regimen of regular weight-bearing exercise and adequte intake of calicum and vitamin D (see item #8).\n  concepts: <class 'list'>\n    Value: []\n  graph: <class 'networkx.classes.digraph.DiGraph'>\n  entities: <class 'list'>\n    Value: ['cpr', 'rivaroxaban', 'osteoporosis', 'cardiopulmonary resuscitation', 'national cancer institute', 'chronic traumatic encephalopathy', 'apixaban', 'warfarin', 'boston university', 'national football league', 'telomerase', 'induced pluripotent stem cells', 'chest x-rays', 'institute of medicine', 'amyotrophic lateral sclerosis', 'president barack obama', 'denosumab', 'harvard-affiliated dana-farber cancer institute', 'fosamax', 'american heart association', 'american academy of neurology', 'lung cancer', 'harvard stem cell institute', 'patient protection and affordable care act', \"lou gherig's disease\", 'actonel', 'shinya yamanaka', 'derrick rossi', 'coumadin', 'vitamin d', 'dabigatran', 'pradaxa', 'prolia', 'health care reform', 'concussions', 'bisphosphonates', 'calcium', 'ct scans']\n  relations: <class 'list'>\n    Value: [('president barack obama', 'signed into law', 'patient protection and affordable care act'), ('patient protection and affordable care act', 'designed to patch holes', 'health insurance system'), ('patient protection and affordable care act', 'extend coverage to', '32 million americans'), ('health care spending', 'accounts for more than', \"17% of the country's gross domestic product\"), ('health insurance exchanges', 'should be in place', '2014'), ('medicaid program', 'scheduled to be expanded', '2014'), ('high-risk pools', 'created for people with pre-existing conditions', '2010'), ('health plans', 'required to extend coverage to adult children', 'up to age 26'), ('indoor tanning salons', 'imposed a 10% tax on', '2010'), ('cardiopulmonary resuscitation', 'issued new guidelines for', 'american heart association'), ('cardiopulmonary resuscitation', 'continue the trend toward simplifying', 'american heart association'), ('cardiopulmonary resuscitation', 'emphasizing chest compressions', 'american heart association'), ('shinya yamanaka', 'discovered a technique for reprogramming cells', 'induced pluripotent stem cells'), ('shinya yamanaka', 'created induced pluripotent stem cells', 'harvard stem cell institute'), ('derrick rossi', 'reported results demonstrating a technique', 'induced pluripotent stem cells'), ('telomerase', 'turned on and off', 'mice'), ('telomerase', 'promptly, the brain and other shrunken organs started to grow', 'mice'), ('telomerase', 'recovered their sense of smell', 'mice'), ('concerns about sports-related concussions', 'growing as evidence has increased', 'permanent brain damage'), ('national football league', 'started to fine players for illegal hits', '2010'), ('american academy of neurology', 'came out with a position paper', 'sports concussions'), ('american academy of pediatrics', 'released a report about sports-related concussions', 'children and adolescents'), ('boston university researchers', 'linked concussions and chronic traumatic encephalopathy', 'amyotrophic lateral sclerosis'), ('harvard-affiliated dana-farber cancer institute', 'reported results kindling hopes for altering the fundamental biology of aging', 'telomerase'), ('harvard-affiliated dana-farber cancer institute', 'experiment involved mice that had been genetically engineered', 'telomerase'), ('national cancer institute', 'stopped the national lung screening trial', 'ct scans'), ('national cancer institute', 'results released in october showed', 'ct scans'), ('institute of medicine', 'issued new vitamin d guidelines', 'recommended dietary allowance'), ('institute of medicine', 'established a new safe upper limit of', '4,000 iu a day'), ('fda', 'approved one alternative to warfarin', 'dabigatran'), ('fda', 'approved another alternative to warfarin', 'rivaroxaban'), ('fda', 'approved another alternative to warfarin', 'apixaban'), ('bisphosphonates', 'prescribed to prevent and treat osteoporosis', 'millions of people'), ('bisphosphonates', 'increasing the risk of a rare kind of thighbone fracture', 'fda'), ('bisphosphonates', 'causing bone, joint, and muscle pain', 'fda'), ('bisphosphonates', 'developing osteonecrosis in their jawbones', 'fda'), ('denosumab', 'approved by the fda', 'prolia'), ('denosumab', 'may get a closer look because of concerns about the side effects of bisphosphonates', 'fda'), ('pill-free bone builders', 'include a regimen of regular weight-bearing exercise', 'fda'), ('pill-free bone builders', 'include adequate intake of calcium and vitamin d', 'fda')]\n  num_nodes: <class 'int'>\n    Value: 46\n  num_edges: <class 'int'>\n    Value: 31\n\n================================================================================\n\nNetworkX Graph found!\n  Nodes: ['president barack obama', 'patient protection and affordable care act', 'health insurance system', '32 million americans', 'health care spending', \"17% of the country's gross domestic product\", 'health insurance exchanges', '2014', 'medicaid program', 'high-risk pools']\n  Number of nodes: 46\n  Number of edges: 31\n\n  Sample edges:\n    president barack obama -> patient protection and affordable care act, data: {'relation': 'signed into law'}\n    patient protection and affordable care act -> health insurance system, data: {'relation': 'designed to patch holes'}\n    patient protection and affordable care act -> 32 million americans, data: {'relation': 'extend coverage to'}\n    health care spending -> 17% of the country's gross domestic product, data: {'relation': 'accounts for more than'}\n    health insurance exchanges -> 2014, data: {'relation': 'should be in place'}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ============================================================================\n# CONVERT AND SAVE KNOWLEDGE GRAPHS TO JSON (CORRECTED)\n# ============================================================================\n\nimport json\n\ndef convert_kg_to_json_format(kg_dict):\n    \"\"\"Convert KG dict to JSON-serializable format\"\"\"\n    json_kg = {\n        'id': kg_dict['id'],\n        'text': kg_dict['text'],\n        'concepts': kg_dict['concepts'],\n        'entities': kg_dict['entities'],  # This is the nodes list\n        'relations': kg_dict['relations'],  # This is the edges list\n        'num_nodes': kg_dict['num_nodes'],\n        'num_edges': kg_dict['num_edges']\n    }\n    return json_kg\n\n# Convert all KGs\nprint(\"Converting knowledge graphs to JSON format...\")\njson_serializable_kgs = [convert_kg_to_json_format(kg) for kg in all_article_kgs]\n\n# Save to JSON\noutput_path = '/kaggle/working/article_knowledge_graphs.json'\nwith open(output_path, 'w', encoding='utf-8') as f:\n    json.dump(json_serializable_kgs, f, indent=2, ensure_ascii=False)\n\nprint(f\"✓ Saved {len(json_serializable_kgs)} KGs to {output_path}\")\nprint(f\"  File size: {os.path.getsize(output_path) / (1024 * 1024):.2f} MB\")\n\n# Verify\nwith open(output_path, 'r') as f:\n    loaded = json.load(f)\nprint(f\"✓ Verified! Loaded {len(loaded)} KGs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.046322Z","iopub.execute_input":"2025-10-27T22:01:19.046941Z","iopub.status.idle":"2025-10-27T22:01:19.080053Z","shell.execute_reply.started":"2025-10-27T22:01:19.046916Z","shell.execute_reply":"2025-10-27T22:01:19.079532Z"}},"outputs":[{"name":"stdout","text":"Converting knowledge graphs to JSON format...\n✓ Saved 100 KGs to /kaggle/working/article_knowledge_graphs.json\n  File size: 0.55 MB\n✓ Verified! Loaded 100 KGs\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: LOAD DATA AND EXISTING KNOWLEDGE GRAPHS\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"Load JSON data from file\"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\ndef load_knowledge_graphs(filepath: str) -> List[Dict]:\n    \"\"\"Load previously created knowledge graphs\"\"\"\n    if not os.path.exists(filepath):\n        print(f\"\\n⚠️  WARNING: Knowledge graph file not found at {filepath}\")\n        return None\n    \n    with open(filepath, 'r', encoding='utf-8') as f:\n        kgs = json.load(f)\n    print(f\"✓ Loaded {len(kgs)} knowledge graphs from {filepath}\")\n    return kgs\n\n# Load your data\narticles = load_json_data('/kaggle/input/zstikg/MedicalConcept Data-set.json')\nknowledge_graphs = load_knowledge_graphs('/kaggle/working/article_knowledge_graphs.json')\n\n# Only classify articles that have KGs\narticles_with_kgs = articles[:len(knowledge_graphs)]  # First 100 articles\n\nif knowledge_graphs is None:\n    print(\"\\n❌ Cannot proceed without knowledge graphs. Please save them first.\")\nelse:\n    print(f\"\\n✓ Successfully loaded {len(articles)} articles and {len(knowledge_graphs)} KGs\")\n    print(\"✓ Ready to proceed with topic classification!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.080813Z","iopub.execute_input":"2025-10-27T22:01:19.081251Z","iopub.status.idle":"2025-10-27T22:01:19.160466Z","shell.execute_reply.started":"2025-10-27T22:01:19.081233Z","shell.execute_reply":"2025-10-27T22:01:19.159770Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 2066 documents from /kaggle/input/zstikg/MedicalConcept Data-set.json\n✓ Loaded 100 knowledge graphs from /kaggle/working/article_knowledge_graphs.json\n\n✓ Successfully loaded 2066 articles and 100 KGs\n✓ Ready to proceed with topic classification!\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: EXTRACT ALL UNIQUE CONCEPTS FROM DATASET\n# ============================================================================\n\ndef extract_unique_concepts(articles: List[Dict]) -> List[str]:\n    \"\"\"\n    Extract all unique concepts from the dataset's 'Concept' field.\n    \n    Args:\n        articles: List of article dictionaries\n    \n    Returns:\n        Sorted list of unique concepts\n    \"\"\"\n    all_concepts = set()\n    \n    for article in articles:\n        # Get concepts from the Concept field\n        if 'Concept' in article and article['Concept']:\n            if isinstance(article['Concept'], list):\n                all_concepts.update([c.lower().strip() for c in article['Concept'] if c])\n            elif isinstance(article['Concept'], str):\n                all_concepts.add(article['Concept'].lower().strip())\n    \n    # Convert to sorted list\n    unique_concepts = sorted(list(all_concepts))\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"UNIQUE CONCEPTS EXTRACTED FROM DATASET\")\n    print(f\"{'='*80}\")\n    print(f\"Total unique concepts: {len(unique_concepts)}\")\n    print(f\"\\nFirst 20 concepts: {unique_concepts[:20]}\")\n    print(f\"\\nLast 20 concepts: {unique_concepts[-20:]}\")\n    \n    return unique_concepts\n\n\n# Extract all unique concepts from the dataset\nall_concepts = extract_unique_concepts(articles)\n\n# Save concept list for reference\nwith open('/kaggle/working/all_concepts.json', 'w') as f:\n    json.dump(all_concepts, f, indent=2)\n\nprint(f\"\\n✓ Saved concept list to /kaggle/working/all_concepts.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.161038Z","iopub.execute_input":"2025-10-27T22:01:19.161229Z","iopub.status.idle":"2025-10-27T22:01:19.169960Z","shell.execute_reply.started":"2025-10-27T22:01:19.161214Z","shell.execute_reply":"2025-10-27T22:01:19.169225Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nUNIQUE CONCEPTS EXTRACTED FROM DATASET\n================================================================================\nTotal unique concepts: 18\n\nFirst 20 concepts: ['addiction', 'alcohol', 'arthritis', 'brain and cognitive health', 'breast cancer', 'cancer', \"children's health\", 'exercise and fitness', 'headache', 'healthy eating', 'heart health', 'mental health', 'osteoporosis', 'pain management', 'prostate knowledge', 'sleep', 'smoking cessation', \"women's health\"]\n\nLast 20 concepts: ['addiction', 'alcohol', 'arthritis', 'brain and cognitive health', 'breast cancer', 'cancer', \"children's health\", 'exercise and fitness', 'headache', 'healthy eating', 'heart health', 'mental health', 'osteoporosis', 'pain management', 'prostate knowledge', 'sleep', 'smoking cessation', \"women's health\"]\n\n✓ Saved concept list to /kaggle/working/all_concepts.json\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom dspy.signatures import Signature\nfrom dspy import OutputField, InputField\nimport pandas as pd\nimport os\nfrom collections import Counter\nimport numpy as np\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.173910Z","iopub.execute_input":"2025-10-27T22:01:19.174116Z","iopub.status.idle":"2025-10-27T22:01:19.182928Z","shell.execute_reply.started":"2025-10-27T22:01:19.174095Z","shell.execute_reply":"2025-10-27T22:01:19.182130Z"}},"outputs":[{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: CONFIGURE LLM\n# ============================================================================\n\nos.environ['HUGGINGFACE_API_KEY'] = 'KEY' \nlm = dspy.LM(\n    model='huggingface/meta-llama/Llama-3.1-8B-Instruct',\n    api_key=os.environ['HUGGINGFACE_API_KEY'],\n    max_tokens=12000,\n    temperature=0.3\n)\n\ndspy.settings.configure(lm=lm)\nprint(\"✓ Llama-3.1-8B-Instruct configured successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.183558Z","iopub.execute_input":"2025-10-27T22:01:19.183734Z","iopub.status.idle":"2025-10-27T22:01:19.193394Z","shell.execute_reply.started":"2025-10-27T22:01:19.183719Z","shell.execute_reply":"2025-10-27T22:01:19.192724Z"}},"outputs":[{"name":"stdout","text":"✓ Llama-3.1-8B-Instruct configured successfully!\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: KG FORMATTING FUNCTION\n# ============================================================================\n\ndef format_kg_for_llm(kg_data: Dict) -> str:\n    \"\"\"Format knowledge graph into readable text for the LLM.\"\"\"\n    if not kg_data or kg_data.get('num_edges', 0) == 0:\n        return \"No knowledge graph available.\"\n    \n    # Format entities\n    entities = kg_data.get('entities', [])\n    entities_str = \", \".join(entities[:30]) if entities else \"None\"\n    if len(entities) > 30:\n        entities_str += f\"... ({len(entities) - 30} more)\"\n    \n    # Format relationships\n    relations = kg_data.get('relations', [])\n    if relations:\n        relationships = []\n        for relation in relations[:20]:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                relationships.append(f\"{source} --[{rel_type}]--> {target}\")\n        relationships_str = \"\\n\".join(relationships)\n        if len(relations) > 20:\n            relationships_str += f\"\\n... ({len(relations) - 20} more relationships)\"\n    else:\n        relationships_str = \"None\"\n    \n    kg_summary = f\"\"\"Knowledge Graph Summary:\nEntities: {entities_str}\n\nKey Relationships:\n{relationships_str}\"\"\"\n    \n    return kg_summary\n\nprint(\"✓ KG formatting function ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.194099Z","iopub.execute_input":"2025-10-27T22:01:19.194332Z","iopub.status.idle":"2025-10-27T22:01:19.206117Z","shell.execute_reply.started":"2025-10-27T22:01:19.194309Z","shell.execute_reply":"2025-10-27T22:01:19.205451Z"}},"outputs":[{"name":"stdout","text":"✓ KG formatting function ready\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: DEFINE DSPY SIGNATURE FOR TOPIC CLASSIFICATION\n# ============================================================================\n\nclass TopicClassification(Signature):\n    \"\"\"Given an article text, its knowledge graph, and a list of possible topics,\n    determine which topics (can be multiple) are most relevant to this article.\n    Return only topic names that are actually present in the available_topics list.\"\"\"\n    \n    article_text: str = InputField(\n        desc=\"The text content of the article\"\n    )\n    knowledge_graph: str = InputField(\n        desc=\"Knowledge graph extracted from the article showing entities and relationships\"\n    )\n    available_topics: str = InputField(\n        desc=\"Comma-separated list of all possible topic/concept names\"\n    )\n    \n    predicted_topics: str = OutputField(\n        desc=\"Comma-separated list of relevant topics from the available_topics list. \"\n             \"Only return topics that actually appear in the available_topics list. \"\n             \"If multiple topics apply, list them all. If no topics match well, return 'none'.\"\n    )\n    confidence: str = OutputField(\n        desc=\"Confidence level: high, medium, or low\"\n    )\n    reasoning: str = OutputField(\n        desc=\"Brief explanation of why these topics were chosen based on the article and knowledge graph\"\n    )\n\nprint(\"✓ Topic classification signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.206874Z","iopub.execute_input":"2025-10-27T22:01:19.207167Z","iopub.status.idle":"2025-10-27T22:01:19.222359Z","shell.execute_reply.started":"2025-10-27T22:01:19.207151Z","shell.execute_reply":"2025-10-27T22:01:19.221729Z"}},"outputs":[{"name":"stdout","text":"✓ Topic classification signature defined\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# ============================================================================\n# STEP 7: CREATE TOPIC CLASSIFIER CLASS\n# ============================================================================\n\nclass ArticleTopicClassifier:\n    \"\"\"Multi-label topic classifier using article text, KG, and available topics.\"\"\"\n    \n    def __init__(self, available_topics: List[str]):\n        self.available_topics = available_topics\n        self.available_topics_str = \", \".join(available_topics)\n        self.classifier = dspy.ChainOfThought(TopicClassification)\n        print(f\"✓ Classifier initialized with {len(available_topics)} possible topics\")\n    \n    def classify_article(self, article_text: str, kg_data: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        kg_summary = format_kg_for_llm(kg_data)\n        \n        # Truncate article if too long\n        max_text_length = 500\n        if len(article_text) > max_text_length:\n            article_text = article_text[:max_text_length] + \"...\"\n        \n        try:\n            result = self.classifier(\n                article_text=article_text,\n                knowledge_graph=kg_summary,\n                available_topics=self.available_topics_str\n            )\n            \n            predicted_topics_raw = result.predicted_topics\n            \n            if predicted_topics_raw.lower() == 'none':\n                predicted_topics = []\n            else:\n                predicted_topics = [\n                    t.strip().lower() \n                    for t in predicted_topics_raw.split(',')\n                    if t.strip()\n                ]\n                predicted_topics = [\n                    t for t in predicted_topics \n                    if t in [at.lower() for at in self.available_topics]\n                ]\n            \n            return {\n                'predicted_topics': predicted_topics,\n                'num_topics': len(predicted_topics),\n                'confidence': result.confidence,\n                'reasoning': result.reasoning\n            }\n            \n        except Exception as e:\n            print(f\"  Error: {str(e)}\")\n            return {\n                'predicted_topics': [],\n                'num_topics': 0,\n                'confidence': 'error',\n                'reasoning': f'Classification failed: {str(e)}'\n            }\n    \n    def classify_dataset(self, articles: List[Dict], knowledge_graphs: List[Dict],\n                        max_articles: int = None) -> List[Dict]:\n        \"\"\"Classify multiple articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        if max_articles:\n            articles = articles[:max_articles]\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(articles)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx, article in enumerate(articles):\n            print(f\"Processing article {idx}...\", end=\" \")\n            \n            article_text = article.get('Article Text', '')\n            kg_data = kg_dict.get(idx, {'num_edges': 0})\n            classification = self.classify_article(article_text, kg_data)\n            \n            result = {\n                'article_id': idx,\n                'article_text': article_text[:100] + \"...\",\n                'true_concepts': article.get('Concept', []),\n                'predicted_topics': classification['predicted_topics'],\n                'num_predicted': classification['num_topics'],\n                'confidence': classification['confidence'],\n                'reasoning': classification['reasoning']\n            }\n            \n            results.append(result)\n            print(f\"✓ Found {len(classification['predicted_topics'])} topics\")\n        \n        return results\n\nprint(\"✓ Classifier class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.223219Z","iopub.execute_input":"2025-10-27T22:01:19.223888Z","iopub.status.idle":"2025-10-27T22:01:19.237069Z","shell.execute_reply.started":"2025-10-27T22:01:19.223869Z","shell.execute_reply":"2025-10-27T22:01:19.236460Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier class defined\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# ============================================================================\n# STEP 8: ANALYSIS FUNCTIONS\n# ============================================================================\n\ndef analyze_classification_results(results: List[Dict], available_topics: List[str]) -> Dict:\n    \"\"\"Analyze classification results and generate statistics.\"\"\"\n    from collections import Counter\n    \n    topic_counts = Counter()\n    confidence_dist = Counter()\n    \n    multi_label_count = 0\n    no_label_count = 0\n    \n    for result in results:\n        predicted = result['predicted_topics']\n        \n        if len(predicted) == 0:\n            no_label_count += 1\n        elif len(predicted) > 1:\n            multi_label_count += 1\n        \n        for topic in predicted:\n            topic_counts[topic] += 1\n        \n        confidence_dist[result['confidence']] += 1\n    \n    stats = {\n        'total_articles': len(results),\n        'articles_with_topics': len(results) - no_label_count,\n        'articles_without_topics': no_label_count,\n        'multi_label_articles': multi_label_count,\n        'avg_topics_per_article': sum(len(r['predicted_topics']) for r in results) / len(results),\n        'most_common_topics': topic_counts.most_common(10)\n    }\n    \n    print(f\"\\n{'='*80}\")\n    print(\"CLASSIFICATION RESULTS SUMMARY\")\n    print(f\"{'='*80}\")\n    print(f\"\\nTotal articles: {stats['total_articles']}\")\n    print(f\"Articles with topics: {stats['articles_with_topics']} ({stats['articles_with_topics']/stats['total_articles']*100:.1f}%)\")\n    print(f\"Multi-label articles: {stats['multi_label_articles']} ({stats['multi_label_articles']/stats['total_articles']*100:.1f}%)\")\n    print(f\"Average topics per article: {stats['avg_topics_per_article']:.2f}\")\n    \n    print(f\"\\n{'='*80}\")\n    print(\"TOP 10 MOST FREQUENT TOPICS\")\n    print(f\"{'='*80}\")\n    for topic, count in stats['most_common_topics']:\n        print(f\"  {topic}: {count} articles ({count/stats['total_articles']*100:.1f}%)\")\n    \n    return stats\n\ndef save_results(results: List[Dict], output_path: str):\n    \"\"\"Save classification results to JSON file.\"\"\"\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(results, f, indent=2, ensure_ascii=False)\n    print(f\"\\n✓ Results saved to {output_path}\")\n\ndef export_to_csv(results: List[Dict], output_path: str):\n    \"\"\"Export results to CSV for easy analysis.\"\"\"\n    rows = []\n    for r in results:\n        rows.append({\n            'article_id': r['article_id'],\n            'article_preview': r['article_text'],\n            'true_concepts': '|'.join(r['true_concepts']) if r['true_concepts'] else '',\n            'predicted_topics': '|'.join(r['predicted_topics']),\n            'num_predicted': r['num_predicted'],\n            'confidence': r['confidence'],\n            'reasoning': r['reasoning']\n        })\n    \n    df = pd.DataFrame(rows)\n    df.to_csv(output_path, index=False, encoding='utf-8')\n    print(f\"✓ CSV exported to {output_path}\")\n\nprint(\"✓ Analysis functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.237757Z","iopub.execute_input":"2025-10-27T22:01:19.238008Z","iopub.status.idle":"2025-10-27T22:01:19.253164Z","shell.execute_reply.started":"2025-10-27T22:01:19.237986Z","shell.execute_reply":"2025-10-27T22:01:19.252460Z"}},"outputs":[{"name":"stdout","text":"✓ Analysis functions defined\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# ============================================================================\n# RUN CLASSIFICATION ON 100 ARTICLES\n# ============================================================================\n\n# Initialize classifier\nclassifier = ArticleTopicClassifier(all_concepts)\n\n# Classify all 100 articles with KGs\nresults = classifier.classify_dataset(\n    articles=articles,\n    knowledge_graphs=knowledge_graphs,\n    max_articles=100\n)\n\n# Analyze results\nstats = analyze_classification_results(results, all_concepts)\n\n# Save results\nsave_results(results, '/kaggle/working/topic_classification_results.json')\nexport_to_csv(results, '/kaggle/working/topic_classification_results.csv')\n\n# Show sample results\nprint(f\"\\n{'='*80}\")\nprint(\"SAMPLE RESULTS (First 3)\")\nprint(f\"{'='*80}\\n\")\n\nfor result in results[:3]:\n    print(f\"Article {result['article_id']}:\")\n    print(f\"  Preview: {result['article_text']}\")\n    print(f\"  True: {result['true_concepts']}\")\n    print(f\"  Predicted: {result['predicted_topics']}\")\n    print(f\"  Confidence: {result['confidence']}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:01:19.253880Z","iopub.execute_input":"2025-10-27T22:01:19.254126Z","iopub.status.idle":"2025-10-27T22:07:37.904710Z","shell.execute_reply.started":"2025-10-27T22:01:19.254105Z","shell.execute_reply":"2025-10-27T22:07:37.903933Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier initialized with 18 possible topics\n\n================================================================================\nCLASSIFYING 100 ARTICLES\n================================================================================\n\nProcessing article 0... ✓ Found 5 topics\nProcessing article 1... ✓ Found 2 topics\nProcessing article 2... ✓ Found 5 topics\nProcessing article 3... ✓ Found 3 topics\nProcessing article 4... ✓ Found 2 topics\nProcessing article 5... ✓ Found 2 topics\nProcessing article 6... ✓ Found 2 topics\nProcessing article 7... ✓ Found 3 topics\nProcessing article 8... ✓ Found 2 topics\nProcessing article 9... ✓ Found 2 topics\nProcessing article 10... ✓ Found 5 topics\nProcessing article 11... ✓ Found 2 topics\nProcessing article 12... ✓ Found 5 topics\nProcessing article 13... ✓ Found 3 topics\nProcessing article 14... ✓ Found 2 topics\nProcessing article 15... ✓ Found 2 topics\nProcessing article 16... ✓ Found 2 topics\nProcessing article 17... ✓ Found 3 topics\nProcessing article 18... ✓ Found 2 topics\nProcessing article 19... ✓ Found 2 topics\nProcessing article 20... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:01:58 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n✓ Found 0 topics\nProcessing article 21... ✓ Found 1 topics\nProcessing article 22... ✓ Found 2 topics\nProcessing article 23... ✓ Found 3 topics\nProcessing article 24... ✓ Found 2 topics\nProcessing article 25... ✓ Found 1 topics\nProcessing article 26... ✓ Found 2 topics\nProcessing article 27... ✓ Found 2 topics\nProcessing article 28... ✓ Found 5 topics\nProcessing article 29... ✓ Found 2 topics\nProcessing article 30... ✓ Found 1 topics\nProcessing article 31... ✓ Found 3 topics\nProcessing article 32... ✓ Found 1 topics\nProcessing article 33... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:02:49 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n✓ Found 0 topics\nProcessing article 34... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:03:00 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"✓ Found 1 topics\nProcessing article 35... ✓ Found 2 topics\nProcessing article 36... ✓ Found 2 topics\nProcessing article 37... ✓ Found 2 topics\nProcessing article 38... ✓ Found 2 topics\nProcessing article 39... ✓ Found 1 topics\nProcessing article 40... ✓ Found 0 topics\nProcessing article 41... ✓ Found 2 topics\nProcessing article 42... ✓ Found 4 topics\nProcessing article 43... ✓ Found 1 topics\nProcessing article 44... ✓ Found 2 topics\nProcessing article 45... ✓ Found 1 topics\nProcessing article 46... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:03:45 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n✓ Found 0 topics\nProcessing article 47... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:03:55 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n✓ Found 0 topics\nProcessing article 48... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:04:07 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"✓ Found 2 topics\nProcessing article 49... ✓ Found 12 topics\nProcessing article 50... ✓ Found 2 topics\nProcessing article 51... ✓ Found 3 topics\nProcessing article 52... ✓ Found 3 topics\nProcessing article 53... ✓ Found 3 topics\nProcessing article 54... ✓ Found 1 topics\nProcessing article 55... ✓ Found 2 topics\nProcessing article 56... ✓ Found 2 topics\nProcessing article 57... ✓ Found 1 topics\nProcessing article 58... ✓ Found 1 topics\nProcessing article 59... ✓ Found 2 topics\nProcessing article 60... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:04:53 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n✓ Found 0 topics\nProcessing article 61... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:05:04 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"✓ Found 2 topics\nProcessing article 62... ✓ Found 1 topics\nProcessing article 63... ✓ Found 1 topics\nProcessing article 64... ✓ Found 3 topics\nProcessing article 65... ✓ Found 2 topics\nProcessing article 66... ✓ Found 1 topics\nProcessing article 67... ✓ Found 2 topics\nProcessing article 68... ✓ Found 2 topics\nProcessing article 69... ✓ Found 1 topics\nProcessing article 70... ✓ Found 1 topics\nProcessing article 71... ✓ Found 3 topics\nProcessing article 72... ✓ Found 2 topics\nProcessing article 73... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:05:49 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n✓ Found 0 topics\nProcessing article 74... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:06:00 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"✓ Found 2 topics\nProcessing article 75... ✓ Found 2 topics\nProcessing article 76... ✓ Found 2 topics\nProcessing article 77... ✓ Found 1 topics\nProcessing article 78... ✓ Found 2 topics\nProcessing article 79... ✓ Found 1 topics\nProcessing article 80... ✓ Found 1 topics\nProcessing article 81... ✓ Found 3 topics\nProcessing article 82... ✓ Found 2 topics\nProcessing article 83... ✓ Found 0 topics\nProcessing article 84... ✓ Found 2 topics\nProcessing article 85... ✓ Found 2 topics\nProcessing article 86... ✓ Found 2 topics\nProcessing article 87... ✓ Found 1 topics\nProcessing article 88... ✓ Found 2 topics\nProcessing article 89... ✓ Found 2 topics\nProcessing article 90... ✓ Found 3 topics\nProcessing article 91... ✓ Found 3 topics\nProcessing article 92... ✓ Found 2 topics\nProcessing article 93... ✓ Found 4 topics\nProcessing article 94... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:06:48 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n✓ Found 0 topics\nProcessing article 95... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:06:59 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n✓ Found 0 topics\nProcessing article 96... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:07:10 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n✓ Found 0 topics\nProcessing article 97... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:07:21 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n✓ Found 0 topics\nProcessing article 98... ","output_type":"stream"},{"name":"stderr","text":"2025/10/27 22:07:32 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"✓ Found 0 topics\nProcessing article 99... ✓ Found 2 topics\n\n================================================================================\nCLASSIFICATION RESULTS SUMMARY\n================================================================================\n\nTotal articles: 100\nArticles with topics: 87 (87.0%)\nMulti-label articles: 67 (67.0%)\nAverage topics per article: 1.97\n\n================================================================================\nTOP 10 MOST FREQUENT TOPICS\n================================================================================\n  heart health: 41 articles (41.0%)\n  mental health: 31 articles (31.0%)\n  healthy eating: 23 articles (23.0%)\n  cancer: 19 articles (19.0%)\n  exercise and fitness: 14 articles (14.0%)\n  prostate knowledge: 14 articles (14.0%)\n  women's health: 12 articles (12.0%)\n  children's health: 11 articles (11.0%)\n  brain and cognitive health: 10 articles (10.0%)\n  pain management: 7 articles (7.0%)\n\n✓ Results saved to /kaggle/working/topic_classification_results.json\n✓ CSV exported to /kaggle/working/topic_classification_results.csv\n\n================================================================================\nSAMPLE RESULTS (First 3)\n================================================================================\n\nArticle 0:\n  Preview: 1. Health care reform\n\nHow could the health care reform legislation that President Barack Obama sign...\n  True: []\n  Predicted: ['heart health', 'healthy eating', 'mental health', \"women's health\", \"children's health\"]\n  Confidence: high\n\nArticle 1:\n  Preview: My colleagues at Harvard Health Publishing and I have a mission: to provide accurate, reliable infor...\n  True: ['Heart Health', \"Women's Health\"]\n  Predicted: ['heart health', \"women's health\"]\n  Confidence: high\n\nArticle 2:\n  Preview: Does echinacea, the popular natural cold remedy, really work?\n\nIt depends on what you mean by “work....\n  True: []\n  Predicted: [\"women's health\", \"children's health\", 'heart health', 'mental health', 'healthy eating']\n  Confidence: medium\n\nThe confidence level is medium because the article provides some evidence for the effectiveness of echinacea, but also mentions limitations and potential drawbacks.\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# ============================================================================\n# RUN CLASSIFICATION ON 100 ARTICLES\n# ============================================================================\n\n# Initialize classifier\nclassifier = ArticleTopicClassifier(all_concepts)\n\n# Classify all 100 articles with KGs\nresults = classifier.classify_dataset(\n    articles=articles,\n    knowledge_graphs=knowledge_graphs,\n    max_articles=100\n)\n\n# Analyze results\nstats = analyze_classification_results(results, all_concepts)\n\n# Save results\nsave_results(results, '/kaggle/working/topic_classification_results.json')\nexport_to_csv(results, '/kaggle/working/topic_classification_results.csv')\n\n# Show sample results\nprint(f\"\\n{'='*80}\")\nprint(\"SAMPLE RESULTS (First 3)\")\nprint(f\"{'='*80}\\n\")\n\nfor result in results[:3]:\n    print(f\"Article {result['article_id']}:\")\n    print(f\"  Preview: {result['article_text']}\")\n    print(f\"  True: {result['true_concepts']}\")\n    print(f\"  Predicted: {result['predicted_topics']}\")\n    print(f\"  Confidence: {result['confidence']}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:07:37.905401Z","iopub.execute_input":"2025-10-27T22:07:37.905632Z","iopub.status.idle":"2025-10-27T22:08:34.340855Z","shell.execute_reply.started":"2025-10-27T22:07:37.905616Z","shell.execute_reply":"2025-10-27T22:08:34.340111Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier initialized with 18 possible topics\n\n================================================================================\nCLASSIFYING 100 ARTICLES\n================================================================================\n\nProcessing article 0... ✓ Found 5 topics\nProcessing article 1... ✓ Found 2 topics\nProcessing article 2... ✓ Found 5 topics\nProcessing article 3... ✓ Found 3 topics\nProcessing article 4... ✓ Found 2 topics\nProcessing article 5... ✓ Found 2 topics\nProcessing article 6... ✓ Found 2 topics\nProcessing article 7... ✓ Found 3 topics\nProcessing article 8... ✓ Found 2 topics\nProcessing article 9... ✓ Found 2 topics\nProcessing article 10... ✓ Found 5 topics\nProcessing article 11... ✓ Found 2 topics\nProcessing article 12... ✓ Found 5 topics\nProcessing article 13... ✓ Found 3 topics\nProcessing article 14... ✓ Found 2 topics\nProcessing article 15... ✓ Found 2 topics\nProcessing article 16... ✓ Found 2 topics\nProcessing article 17... ✓ Found 3 topics\nProcessing article 18... ✓ Found 2 topics\nProcessing article 19... ✓ Found 2 topics\nProcessing article 20... ✓ Found 3 topics\nProcessing article 21... ✓ Found 1 topics\nProcessing article 22... ✓ Found 2 topics\nProcessing article 23... ✓ Found 3 topics\nProcessing article 24... ✓ Found 2 topics\nProcessing article 25... ✓ Found 1 topics\nProcessing article 26... ✓ Found 2 topics\nProcessing article 27... ✓ Found 2 topics\nProcessing article 28... ✓ Found 5 topics\nProcessing article 29... ✓ Found 2 topics\nProcessing article 30... ✓ Found 1 topics\nProcessing article 31... ✓ Found 3 topics\nProcessing article 32... ✓ Found 1 topics\nProcessing article 33... ✓ Found 3 topics\nProcessing article 34... ✓ Found 1 topics\nProcessing article 35... ✓ Found 2 topics\nProcessing article 36... ✓ Found 2 topics\nProcessing article 37... ✓ Found 2 topics\nProcessing article 38... ✓ Found 2 topics\nProcessing article 39... ✓ Found 1 topics\nProcessing article 40... ✓ Found 0 topics\nProcessing article 41... ✓ Found 2 topics\nProcessing article 42... ✓ Found 4 topics\nProcessing article 43... ✓ Found 1 topics\nProcessing article 44... ✓ Found 2 topics\nProcessing article 45... ✓ Found 1 topics\nProcessing article 46... ✓ Found 3 topics\nProcessing article 47... ✓ Found 2 topics\nProcessing article 48... ✓ Found 2 topics\nProcessing article 49... ✓ Found 12 topics\nProcessing article 50... ✓ Found 2 topics\nProcessing article 51... ✓ Found 3 topics\nProcessing article 52... ✓ Found 3 topics\nProcessing article 53... ✓ Found 3 topics\nProcessing article 54... ✓ Found 1 topics\nProcessing article 55... ✓ Found 2 topics\nProcessing article 56... ✓ Found 2 topics\nProcessing article 57... ✓ Found 1 topics\nProcessing article 58... ✓ Found 1 topics\nProcessing article 59... ✓ Found 2 topics\nProcessing article 60... ✓ Found 2 topics\nProcessing article 61... ✓ Found 6 topics\nProcessing article 62... ✓ Found 1 topics\nProcessing article 63... ✓ Found 1 topics\nProcessing article 64... ✓ Found 3 topics\nProcessing article 65... ✓ Found 2 topics\nProcessing article 66... ✓ Found 1 topics\nProcessing article 67... ✓ Found 2 topics\nProcessing article 68... ✓ Found 2 topics\nProcessing article 69... ✓ Found 1 topics\nProcessing article 70... ✓ Found 1 topics\nProcessing article 71... ✓ Found 3 topics\nProcessing article 72... ✓ Found 2 topics\nProcessing article 73... ✓ Found 0 topics\nProcessing article 74... ✓ Found 2 topics\nProcessing article 75... ✓ Found 2 topics\nProcessing article 76... ✓ Found 2 topics\nProcessing article 77... ✓ Found 1 topics\nProcessing article 78... ✓ Found 2 topics\nProcessing article 79... ✓ Found 1 topics\nProcessing article 80... ✓ Found 1 topics\nProcessing article 81... ✓ Found 3 topics\nProcessing article 82... ✓ Found 2 topics\nProcessing article 83... ✓ Found 0 topics\nProcessing article 84... ✓ Found 2 topics\nProcessing article 85... ✓ Found 2 topics\nProcessing article 86... ✓ Found 2 topics\nProcessing article 87... ✓ Found 1 topics\nProcessing article 88... ✓ Found 2 topics\nProcessing article 89... ✓ Found 2 topics\nProcessing article 90... ✓ Found 3 topics\nProcessing article 91... ✓ Found 3 topics\nProcessing article 92... ✓ Found 2 topics\nProcessing article 93... ✓ Found 4 topics\nProcessing article 94... ✓ Found 0 topics\nProcessing article 95... ✓ Found 0 topics\nProcessing article 96... ✓ Found 1 topics\nProcessing article 97... ✓ Found 1 topics\nProcessing article 98... ✓ Found 0 topics\nProcessing article 99... ✓ Found 2 topics\n\n================================================================================\nCLASSIFICATION RESULTS SUMMARY\n================================================================================\n\nTotal articles: 100\nArticles with topics: 94 (94.0%)\nMulti-label articles: 72 (72.0%)\nAverage topics per article: 2.16\n\n================================================================================\nTOP 10 MOST FREQUENT TOPICS\n================================================================================\n  heart health: 45 articles (45.0%)\n  mental health: 32 articles (32.0%)\n  healthy eating: 24 articles (24.0%)\n  cancer: 22 articles (22.0%)\n  prostate knowledge: 16 articles (16.0%)\n  exercise and fitness: 15 articles (15.0%)\n  women's health: 13 articles (13.0%)\n  children's health: 12 articles (12.0%)\n  brain and cognitive health: 11 articles (11.0%)\n  pain management: 9 articles (9.0%)\n\n✓ Results saved to /kaggle/working/topic_classification_results.json\n✓ CSV exported to /kaggle/working/topic_classification_results.csv\n\n================================================================================\nSAMPLE RESULTS (First 3)\n================================================================================\n\nArticle 0:\n  Preview: 1. Health care reform\n\nHow could the health care reform legislation that President Barack Obama sign...\n  True: []\n  Predicted: ['heart health', 'healthy eating', 'mental health', \"women's health\", \"children's health\"]\n  Confidence: high\n\nArticle 1:\n  Preview: My colleagues at Harvard Health Publishing and I have a mission: to provide accurate, reliable infor...\n  True: ['Heart Health', \"Women's Health\"]\n  Predicted: ['heart health', \"women's health\"]\n  Confidence: high\n\nArticle 2:\n  Preview: Does echinacea, the popular natural cold remedy, really work?\n\nIt depends on what you mean by “work....\n  True: []\n  Predicted: [\"women's health\", \"children's health\", 'heart health', 'mental health', 'healthy eating']\n  Confidence: medium\n\nThe confidence level is medium because the article provides some evidence for the effectiveness of echinacea, but also mentions limitations and potential drawbacks.\n\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION: Compare Predicted vs True Concepts\n# ============================================================================\n\ndef evaluate_classification(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate accuracy metrics by comparing predicted topics with true concepts.\n    \"\"\"\n    total = len(results)\n    exact_match = 0  # All predicted topics match exactly with true concepts\n    partial_match = 0  # At least one topic matches\n    no_match = 0  # No topics match\n    no_true_labels = 0  # Articles with no true concepts\n    \n    all_precisions = []\n    all_recalls = []\n    all_f1s = []\n    \n    correct_predictions = []\n    incorrect_predictions = []\n    \n    for result in results:\n        true_set = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_set = set(result['predicted_topics'])\n        \n        # Skip articles with no true labels\n        if len(true_set) == 0:\n            no_true_labels += 1\n            continue\n        \n        # Find intersection\n        intersection = true_set & pred_set\n        \n        # Exact match\n        if true_set == pred_set and len(true_set) > 0:\n            exact_match += 1\n            correct_predictions.append(result)\n        elif len(intersection) > 0:\n            partial_match += 1\n        else:\n            no_match += 1\n            incorrect_predictions.append(result)\n        \n        # Calculate precision, recall, F1\n        if len(pred_set) > 0:\n            precision = len(intersection) / len(pred_set)\n            all_precisions.append(precision)\n        else:\n            all_precisions.append(0)\n        \n        if len(true_set) > 0:\n            recall = len(intersection) / len(true_set)\n            all_recalls.append(recall)\n        else:\n            all_recalls.append(0)\n        \n        if all_precisions[-1] + all_recalls[-1] > 0:\n            f1 = 2 * (all_precisions[-1] * all_recalls[-1]) / (all_precisions[-1] + all_recalls[-1])\n            all_f1s.append(f1)\n        else:\n            all_f1s.append(0)\n    \n    articles_with_true_labels = total - no_true_labels\n    \n    metrics = {\n        'total_articles': total,\n        'articles_with_true_labels': articles_with_true_labels,\n        'articles_without_true_labels': no_true_labels,\n        'exact_matches': exact_match,\n        'partial_matches': partial_match,\n        'no_matches': no_match,\n        'exact_match_rate': exact_match / articles_with_true_labels if articles_with_true_labels > 0 else 0,\n        'partial_match_rate': partial_match / articles_with_true_labels if articles_with_true_labels > 0 else 0,\n        'avg_precision': np.mean(all_precisions) if all_precisions else 0,\n        'avg_recall': np.mean(all_recalls) if all_recalls else 0,\n        'avg_f1': np.mean(all_f1s) if all_f1s else 0,\n        'correct_examples': correct_predictions[:3],\n        'incorrect_examples': incorrect_predictions[:3]\n    }\n    \n    # Print results\n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS\")\n    print(f\"{'='*80}\")\n    print(f\"\\nTotal articles evaluated: {articles_with_true_labels}\")\n    print(f\"Articles without labels (skipped): {no_true_labels}\")\n    print(f\"\\n📊 ACCURACY:\")\n    print(f\"  ✓ Exact matches: {exact_match} ({metrics['exact_match_rate']*100:.1f}%)\")\n    print(f\"  ~ Partial matches: {partial_match} ({metrics['partial_match_rate']*100:.1f}%)\")\n    print(f\"  ✗ No matches: {no_match} ({no_match/articles_with_true_labels*100:.1f}%)\")\n    print(f\"\\n📈 PERFORMANCE METRICS:\")\n    print(f\"  Precision: {metrics['avg_precision']:.3f} (how many predicted topics were correct)\")\n    print(f\"  Recall: {metrics['avg_recall']:.3f} (how many true topics were found)\")\n    print(f\"  F1 Score: {metrics['avg_f1']:.3f} (overall accuracy)\")\n    \n    return metrics\n\n\n# Run evaluation\nprint(\"\\nEvaluating classification accuracy...\")\neval_metrics = evaluate_classification(results)\n\n# Show correct examples\nprint(f\"\\n{'='*80}\")\nprint(\"✓ CORRECTLY CLASSIFIED EXAMPLES\")\nprint(f\"{'='*80}\")\nfor ex in eval_metrics['correct_examples']:\n    print(f\"\\nArticle {ex['article_id']}:\")\n    print(f\"  True: {ex['true_concepts']}\")\n    print(f\"  Predicted: {ex['predicted_topics']}\")\n    print(f\"  ✓ EXACT MATCH!\")\n\n# Show incorrect examples\nprint(f\"\\n{'='*80}\")\nprint(\"✗ INCORRECTLY CLASSIFIED EXAMPLES\")\nprint(f\"{'='*80}\")\nfor ex in eval_metrics['incorrect_examples']:\n    print(f\"\\nArticle {ex['article_id']}:\")\n    print(f\"  True: {ex['true_concepts']}\")\n    print(f\"  Predicted: {ex['predicted_topics']}\")\n    print(f\"  Reasoning: {ex['reasoning'][:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:08:34.341798Z","iopub.execute_input":"2025-10-27T22:08:34.342061Z","iopub.status.idle":"2025-10-27T22:08:34.357225Z","shell.execute_reply.started":"2025-10-27T22:08:34.342043Z","shell.execute_reply":"2025-10-27T22:08:34.356463Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating classification accuracy...\n\n================================================================================\nEVALUATION METRICS\n================================================================================\n\nTotal articles evaluated: 76\nArticles without labels (skipped): 24\n\n📊 ACCURACY:\n  ✓ Exact matches: 38 (50.0%)\n  ~ Partial matches: 36 (47.4%)\n  ✗ No matches: 2 (2.6%)\n\n📈 PERFORMANCE METRICS:\n  Precision: 0.739 (how many predicted topics were correct)\n  Recall: 0.932 (how many true topics were found)\n  F1 Score: 0.798 (overall accuracy)\n\n================================================================================\n✓ CORRECTLY CLASSIFIED EXAMPLES\n================================================================================\n\nArticle 1:\n  True: ['Heart Health', \"Women's Health\"]\n  Predicted: ['heart health', \"women's health\"]\n  ✓ EXACT MATCH!\n\nArticle 4:\n  True: ['Healthy Eating', 'Heart Health']\n  Predicted: ['healthy eating', 'heart health']\n  ✓ EXACT MATCH!\n\nArticle 5:\n  True: ['Exercise and Fitness', 'Heart Health']\n  Predicted: ['heart health', 'exercise and fitness']\n  ✓ EXACT MATCH!\n\n================================================================================\n✗ INCORRECTLY CLASSIFIED EXAMPLES\n================================================================================\n\nArticle 57:\n  True: ['Mental Health']\n  Predicted: ['brain and cognitive health']\n  Reasoning: The article discusses a study on a three-protein signature that may help identify people with Alzheimer's disease. The knowledge graph highlights the ...\n\nArticle 59:\n  True: ['Exercise and Fitness', 'Mental Health']\n  Predicted: ['brain and cognitive health', \"children's health\"]\n  Reasoning: The article discusses the risks of head injuries in football, specifically the potential for brain damage and chronic traumatic encephalopathy (CTE). ...\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION METRICS (Following the Paper's Methodology)\n# ============================================================================\n\ndef calculate_metrics_paper_style(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate evaluation metrics following the paper's approach:\n    \"For each article, the model inferred topic(s) were compared against \n    the list of 'gold' topic(s) to compute the true positive, false positive, \n    and false negative statistics for that article. Then, all such statistics \n    for all the articles in a dataset were aggregated and used to compute \n    the final Precision, Recall, and micro-averaged F1 score.\"\n    \n    Reference: Section 5.4 of the paper\n    \"\"\"\n    # Aggregate statistics across all articles\n    total_tp = 0  # True Positives\n    total_fp = 0  # False Positives\n    total_fn = 0  # False Negatives\n    \n    articles_evaluated = 0\n    articles_skipped = 0\n    \n    for result in results:\n        # Get true and predicted topics (lowercased and stripped)\n        true_topics = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_topics = set([t.lower().strip() for t in result['predicted_topics']])\n        \n        # Skip articles with no ground truth labels\n        if len(true_topics) == 0:\n            articles_skipped += 1\n            continue\n        \n        articles_evaluated += 1\n        \n        # Calculate TP, FP, FN for this article\n        tp = len(true_topics & pred_topics)  # Intersection\n        fp = len(pred_topics - true_topics)  # Predicted but not true\n        fn = len(true_topics - pred_topics)  # True but not predicted\n        \n        # Aggregate\n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n    \n    # Calculate micro-averaged metrics\n    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    metrics = {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'total_tp': total_tp,\n        'total_fp': total_fp,\n        'total_fn': total_fn,\n        'articles_evaluated': articles_evaluated,\n        'articles_skipped': articles_skipped\n    }\n    \n    # Print results\n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS (Paper's Methodology)\")\n    print(f\"{'='*80}\")\n    print(f\"\\nArticles evaluated: {articles_evaluated}\")\n    print(f\"Articles skipped (no ground truth): {articles_skipped}\")\n    print(f\"\\nAggregated Statistics:\")\n    print(f\"  True Positives (TP): {total_tp}\")\n    print(f\"  False Positives (FP): {total_fp}\")\n    print(f\"  False Negatives (FN): {total_fn}\")\n    print(f\"\\n📊 MICRO-AVERAGED METRICS:\")\n    print(f\"  Precision: {precision:.3f}\")\n    print(f\"  Recall: {recall:.3f}\")\n    print(f\"  F1 Score: {f1_score:.3f}\")\n    \n    return metrics\n\n\n# Run evaluation using the paper's methodology\nprint(\"\\nEvaluating using paper's methodology...\")\npaper_metrics = calculate_metrics_paper_style(results)\n\n# Compare with their baselines (from Table 6 in the paper)\nprint(f\"\\n{'='*80}\")\nprint(\"COMPARISON WITH PAPER'S BASELINES (Medical Dataset)\")\nprint(f\"{'='*80}\")\nprint(f\"Your F1 Score: {paper_metrics['f1_score']:.3f}\")\nprint(f\"\\nPaper's Results for Medical Dataset:\")\nprint(f\"  GFLM-S baseline: 0.532\")\nprint(f\"  GFLM-W baseline: 0.530\")\nprint(f\"  SBERT (best mid encoder): 0.594\")\nprint(f\"  ChatGPT-3.5 (best overall): 0.606\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:08:34.358064Z","iopub.execute_input":"2025-10-27T22:08:34.358297Z","iopub.status.idle":"2025-10-27T22:08:34.375343Z","shell.execute_reply.started":"2025-10-27T22:08:34.358272Z","shell.execute_reply":"2025-10-27T22:08:34.374541Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating using paper's methodology...\n\n================================================================================\nEVALUATION METRICS (Paper's Methodology)\n================================================================================\n\nArticles evaluated: 76\nArticles skipped (no ground truth): 24\n\nAggregated Statistics:\n  True Positives (TP): 106\n  False Positives (FP): 58\n  False Negatives (FN): 10\n\n📊 MICRO-AVERAGED METRICS:\n  Precision: 0.646\n  Recall: 0.914\n  F1 Score: 0.757\n\n================================================================================\nCOMPARISON WITH PAPER'S BASELINES (Medical Dataset)\n================================================================================\nYour F1 Score: 0.757\n\nPaper's Results for Medical Dataset:\n  GFLM-S baseline: 0.532\n  GFLM-W baseline: 0.530\n  SBERT (best mid encoder): 0.594\n  ChatGPT-3.5 (best overall): 0.606\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# ============================================================================\n# DIAGNOSTIC: Check for potential issues\n# ============================================================================\n\ndef diagnose_results(results: List[Dict], all_concepts: List[str]):\n    \"\"\"Check what might be causing inflated metrics.\"\"\"\n    \n    print(f\"\\n{'='*80}\")\n    print(\"DIAGNOSTIC ANALYSIS\")\n    print(f\"{'='*80}\")\n    \n    # Check for direct entity-topic matches\n    print(\"\\n1. Checking for data leakage (entity names matching topic names)...\")\n    \n    # Sample a few KGs to see if entities match topics\n    if len(knowledge_graphs) > 0:\n        sample_kg = knowledge_graphs[0]\n        entities = set([e.lower() for e in sample_kg.get('entities', [])])\n        topics = set([t.lower() for t in all_concepts])\n        \n        direct_matches = entities & topics\n        if direct_matches:\n            print(f\"   ⚠️  WARNING: Found {len(direct_matches)} direct entity-topic matches:\")\n            print(f\"   {list(direct_matches)[:10]}\")\n            print(\"   This could cause data leakage!\")\n        else:\n            print(\"   ✓ No direct entity-topic name matches found\")\n    \n    # Check prediction statistics\n    print(\"\\n2. Prediction statistics:\")\n    topics_per_article = [len(r['predicted_topics']) for r in results]\n    print(f\"   Average predicted topics per article: {np.mean(topics_per_article):.2f}\")\n    print(f\"   Max predicted topics: {max(topics_per_article)}\")\n    print(f\"   Min predicted topics: {min(topics_per_article)}\")\n    \n    # Check how many articles have predictions\n    articles_with_predictions = sum(1 for r in results if len(r['predicted_topics']) > 0)\n    print(f\"   Articles with predictions: {articles_with_predictions}/{len(results)}\")\n    \n    # Check confidence distribution\n    print(\"\\n3. Confidence distribution:\")\n    confidences = [r['confidence'] for r in results]\n    from collections import Counter\n    conf_dist = Counter(confidences)\n    for conf, count in conf_dist.most_common():\n        print(f\"   {conf}: {count} articles ({count/len(results)*100:.1f}%)\")\n    \n    # Show some examples\n    print(f\"\\n4. Sample predictions (first 5 articles):\")\n    for i, result in enumerate(results[:5]):\n        print(f\"\\n   Article {i}:\")\n        print(f\"   True: {result['true_concepts']}\")\n        print(f\"   Predicted: {result['predicted_topics']}\")\n        match = set(result['true_concepts']) & set(result['predicted_topics'])\n        print(f\"   Matches: {match if match else 'None'}\")\n\n# Run diagnostics\ndiagnose_results(results, all_concepts)\n\n# Then run evaluation\npaper_metrics = calculate_metrics_paper_style(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T22:08:34.376190Z","iopub.execute_input":"2025-10-27T22:08:34.376528Z","iopub.status.idle":"2025-10-27T22:08:34.394700Z","shell.execute_reply.started":"2025-10-27T22:08:34.376511Z","shell.execute_reply":"2025-10-27T22:08:34.394027Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nDIAGNOSTIC ANALYSIS\n================================================================================\n\n1. Checking for data leakage (entity names matching topic names)...\n   ⚠️  WARNING: Found 1 direct entity-topic matches:\n   ['osteoporosis']\n   This could cause data leakage!\n\n2. Prediction statistics:\n   Average predicted topics per article: 2.16\n   Max predicted topics: 12\n   Min predicted topics: 0\n   Articles with predictions: 94/100\n\n3. Confidence distribution:\n   high: 93 articles (93.0%)\n   medium: 4 articles (4.0%)\n   medium\n\nThe confidence level is medium because the article provides some evidence for the effectiveness of echinacea, but also mentions limitations and potential drawbacks.: 2 articles (2.0%)\n   low: 1 articles (1.0%)\n\n4. Sample predictions (first 5 articles):\n\n   Article 0:\n   True: []\n   Predicted: ['heart health', 'healthy eating', 'mental health', \"women's health\", \"children's health\"]\n   Matches: None\n\n   Article 1:\n   True: ['Heart Health', \"Women's Health\"]\n   Predicted: ['heart health', \"women's health\"]\n   Matches: None\n\n   Article 2:\n   True: []\n   Predicted: [\"women's health\", \"children's health\", 'heart health', 'mental health', 'healthy eating']\n   Matches: None\n\n   Article 3:\n   True: ['Arthritis', 'Pain Management']\n   Predicted: ['pain management', 'addiction', 'heart health']\n   Matches: None\n\n   Article 4:\n   True: ['Healthy Eating', 'Heart Health']\n   Predicted: ['healthy eating', 'heart health']\n   Matches: None\n\n================================================================================\nEVALUATION METRICS (Paper's Methodology)\n================================================================================\n\nArticles evaluated: 76\nArticles skipped (no ground truth): 24\n\nAggregated Statistics:\n  True Positives (TP): 106\n  False Positives (FP): 58\n  False Negatives (FN): 10\n\n📊 MICRO-AVERAGED METRICS:\n  Precision: 0.646\n  Recall: 0.914\n  F1 Score: 0.757\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}