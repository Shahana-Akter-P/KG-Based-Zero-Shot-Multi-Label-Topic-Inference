{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13468504,"sourceType":"datasetVersion","datasetId":8549797},{"sourceId":13524188,"sourceType":"datasetVersion","datasetId":8587369}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:24:41.189854Z","iopub.execute_input":"2025-10-28T14:24:41.190127Z","iopub.status.idle":"2025-10-28T14:24:41.200533Z","shell.execute_reply.started":"2025-10-28T14:24:41.190109Z","shell.execute_reply":"2025-10-28T14:24:41.199969Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/zstikg/NewsConcept Data-set.json\n/kaggle/input/zstikg/DVD playerData-set.json\n/kaggle/input/zstikg/MedicalConcept Data-set.json\n/kaggle/input/zstikg/Cellular phone Data-set.json\n/kaggle/input/zstikg/Digital camera2 Data-set.json\n/kaggle/input/zstikg/Mp3 playerData-set.json\n/kaggle/input/zstikg/Digital camera1 Data-set.json\n/kaggle/input/keywords/Keyword_Medical.json\n/kaggle/input/keywords/Keyword_Canon.json\n/kaggle/input/keywords/Keyword_Creative.json\n/kaggle/input/keywords/Keyword_Apex.json\n/kaggle/input/keywords/Keyword_Apex 1.json\n/kaggle/input/keywords/Keyword_Nokia.json\n/kaggle/input/keywords/Keyword_News.json\n/kaggle/input/keywords/Keyword_Nikon.json\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# PART 1: INSTALLATION AND SETUP\n# ============================================================================\n\n# Install required packages\n!pip install dspy-ai huggingface_hub networkx sentence-transformers pandas --quiet\n\nprint(\"✓ Packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:24:44.220678Z","iopub.execute_input":"2025-10-28T14:24:44.221362Z","iopub.status.idle":"2025-10-28T14:24:47.918667Z","shell.execute_reply.started":"2025-10-28T14:24:44.221336Z","shell.execute_reply":"2025-10-28T14:24:47.917836Z"}},"outputs":[{"name":"stdout","text":"✓ Packages installed successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================================\n# PART 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom collections import defaultdict\nimport pandas as pd\nimport os\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:24:50.087335Z","iopub.execute_input":"2025-10-28T14:24:50.087964Z","iopub.status.idle":"2025-10-28T14:25:03.639067Z","shell.execute_reply.started":"2025-10-28T14:24:50.087932Z","shell.execute_reply":"2025-10-28T14:25:03.638249Z"}},"outputs":[{"name":"stderr","text":"2025-10-28 14:25:00.411067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761661500.434199     182 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761661500.441299     182 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: INSTALL AND IMPORT (if not done already)\n# ============================================================================\n\nimport os\nfrom huggingface_hub import InferenceClient\nimport dspy\nimport json\nfrom typing import List, Tuple, Dict\n\nprint(\"✓ Imports complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:25:07.663478Z","iopub.execute_input":"2025-10-28T14:25:07.664670Z","iopub.status.idle":"2025-10-28T14:25:07.743073Z","shell.execute_reply.started":"2025-10-28T14:25:07.664619Z","shell.execute_reply":"2025-10-28T14:25:07.742476Z"}},"outputs":[{"name":"stdout","text":"✓ Imports complete\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: CONFIGURE HUGGING FACE WITH DSPY.LM (PROPER WAY)\n# ============================================================================\nimport os\nimport dspy\n\nos.environ['HUGGINGFACE_API_KEY'] = 'KEY'\n\n# Use DSPy's built-in LM class with the correct prefix\nlm = dspy.LM(\n    model='huggingface/meta-llama/Llama-3.1-8B-Instruct',\n    api_key=os.environ['HUGGINGFACE_API_KEY'],\n    max_tokens=12000,\n    temperature=0.3\n)\n\ndspy.settings.configure(lm=lm)\nprint(\"✓ Llama-3.1-8B-Instruct configured successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:25:10.191503Z","iopub.execute_input":"2025-10-28T14:25:10.192138Z","iopub.status.idle":"2025-10-28T14:25:10.197277Z","shell.execute_reply.started":"2025-10-28T14:25:10.192109Z","shell.execute_reply":"2025-10-28T14:25:10.196495Z"}},"outputs":[{"name":"stdout","text":"✓ Llama-3.1-8B-Instruct configured successfully!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# PART 1: DATA LOADING FUNCTION\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"\n    Load JSON data from file\n    \n    Expected format:\n    [\n        {\n            \"Article Title\": [],\n            \"Article Text\": \"text here...\",\n            \"Concept\": [\"concept1\", \"concept2\"]\n        },\n        ...\n    ]\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\n# Test with sample data\nsample_data = [\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"excellent phone, excellent service.\",\n        \"Concept\": []\n    },\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"i am a business user who heavily depend on mobile service.\",\n        \"Concept\": [\"service\"]\n    }\n]\n\nprint(\"✓ Sample data ready for testing\")\nprint(f\"  Sample has {len(sample_data)} documents\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:25:12.984929Z","iopub.execute_input":"2025-10-28T14:25:12.985523Z","iopub.status.idle":"2025-10-28T14:25:12.991150Z","shell.execute_reply.started":"2025-10-28T14:25:12.985500Z","shell.execute_reply":"2025-10-28T14:25:12.990386Z"}},"outputs":[{"name":"stdout","text":"✓ Sample data ready for testing\n  Sample has 2 documents\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================================================\n# LOAD ONE SPECIFIC DATASET\n# ============================================================================\n\n# Choose which dataset you want to work with:\n# Option 1: Cellular phone\nfilepath = '/kaggle/input/zstikg/MedicalConcept Data-set.json'\n\n# Option 2: News\n# filepath = '/kaggle/input/zsltikg/NewsConcept Data-set.json'\n\n# Option 3: Medical\n# filepath = '/kaggle/input/zsltikg/MedicalConcept Data-set.json'\n\n# Load the data\ndata = load_json_data(filepath)[:100]\n\nprint(f\"✓ Loaded {len(data)} documents\")\nprint(f\"\\nFirst document preview:\")\nprint(f\"  Keys: {list(data[0].keys())}\")\nprint(f\"  Text: {data[0].get('Article Text', '')[:100]}...\")\nprint(f\"  Concepts: {data[0].get('Concept', [])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:25:17.224426Z","iopub.execute_input":"2025-10-28T14:25:17.224731Z","iopub.status.idle":"2025-10-28T14:25:17.290346Z","shell.execute_reply.started":"2025-10-28T14:25:17.224710Z","shell.execute_reply":"2025-10-28T14:25:17.289679Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 2066 documents from /kaggle/input/zstikg/MedicalConcept Data-set.json\n✓ Loaded 100 documents\n\nFirst document preview:\n  Keys: ['Article Title', 'Article Text', 'Concept']\n  Text: 1. Health care reform\n\nHow could the health care reform legislation that President Barack Obama sign...\n  Concepts: []\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# PROCESS YOUR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"PROCESSING ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\n# data is now correctly loaded as a list of dicts\nindividual_articles = []\n\nfor idx, item in enumerate(data):\n    article = {\n        'id': idx,\n        'Article Text': item['Article Text'],\n        #'Concept': item['Concept'] if item['Concept'] else []\n    }\n    individual_articles.append(article)\n\nprint(f\"\\n✓ Created list of {len(individual_articles)} individual articles\")\n\n# Show first 3\nprint(f\"\\nFirst 3 articles:\")\nprint(\"-\" * 80)\nfor i in range(min(3, len(individual_articles))):\n    article = individual_articles[i]\n    print(f\"\\nArticle {article['id']}:\")\n    print(f\"  Text: {article['Article Text'][:80]}...\")\n    #print(f\"  Concepts: {article['Concept']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:25:45.525604Z","iopub.execute_input":"2025-10-28T14:25:45.526225Z","iopub.status.idle":"2025-10-28T14:25:45.531918Z","shell.execute_reply.started":"2025-10-28T14:25:45.526203Z","shell.execute_reply":"2025-10-28T14:25:45.531213Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPROCESSING ALL THE ARTICLES\n================================================================================\n\n✓ Created list of 100 individual articles\n\nFirst 3 articles:\n--------------------------------------------------------------------------------\n\nArticle 0:\n  Text: 1. Health care reform\n\nHow could the health care reform legislation that Preside...\n\nArticle 1:\n  Text: My colleagues at Harvard Health Publishing and I have a mission: to provide accu...\n\nArticle 2:\n  Text: Does echinacea, the popular natural cold remedy, really work?\n\nIt depends on wha...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: DEFINE SIGNATURES\n# ============================================================================\n\nclass EntityExtractor(dspy.Signature):\n    \"\"\"Extract key entities from the given text. Extracted entities are nouns, \n    verbs, or adjectives, particularly regarding sentiment. This is for an \n    extraction task, please be thorough and accurate to the reference text.\n    \n    Return ONLY a valid JSON list format: [\"entity1\", \"entity2\", \"entity3\"]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract entities from\")\n    entities = dspy.OutputField(desc=\"List of extracted entities in JSON format\")\n\nclass RelationExtractor(dspy.Signature):\n    \"\"\"Extract subject-predicate-object triples from the assistant message. \n    A predicate (1-3 words) defines the relationship between the subject and \n    object. Relationship may be fact or sentiment based on assistant's message. \n    Subject and object are entities. Entities provided are from the assistant \n    message and prior conversation history, though you may not need all of them. \n    This is for an extraction task, please be thorough, accurate, and faithful \n    to the reference text.\n    \n    Return ONLY valid JSON format: [[\"subject1\", \"predicate1\", \"object1\"], [\"subject2\", \"predicate2\", \"object2\"]]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract relations from\")\n    entities = dspy.InputField(desc=\"List of available entities\")\n    triples = dspy.OutputField(desc=\"List of [subject, predicate, object] triples in JSON format\")\n\nprint(\"✓ Signatures defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:26:03.983758Z","iopub.execute_input":"2025-10-28T14:26:03.984029Z","iopub.status.idle":"2025-10-28T14:26:03.991443Z","shell.execute_reply.started":"2025-10-28T14:26:03.984009Z","shell.execute_reply":"2025-10-28T14:26:03.990831Z"}},"outputs":[{"name":"stdout","text":"✓ Signatures defined\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: CREATE ENTITY EXTRACTOR\n# ============================================================================\n\nclass ExtractEntities(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(EntityExtractor)\n    \n    def forward(self, text: str) -> List[str]:\n        if not text or len(text.strip()) < 3:\n            return []\n            \n        result = self.extract(text=text)\n        \n        try:\n            entities_text = result.entities.strip()\n            \n            if '[' in entities_text and ']' in entities_text:\n                start = entities_text.find('[')\n                end = entities_text.rfind(']') + 1\n                entities_text = entities_text[start:end]\n            \n            entities = json.loads(entities_text)\n            \n            if isinstance(entities, list):\n                return [str(e).lower().strip() for e in entities if e and len(str(e).strip()) > 1]\n            return []\n            \n        except:\n            try:\n                entities_text = result.entities.strip()\n                if entities_text.startswith('['):\n                    entities_text = entities_text[1:]\n                if entities_text.endswith(']'):\n                    entities_text = entities_text[:-1]\n                \n                entities = []\n                for item in entities_text.split(','):\n                    item = item.strip(' \"\\'\\n\\t')\n                    if item and len(item) > 1:\n                        entities.append(item.lower())\n                \n                return entities[:50]\n            except:\n                return []\n\nentity_extractor = ExtractEntities()\nprint(\"✓ Entity Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:26:07.264257Z","iopub.execute_input":"2025-10-28T14:26:07.264539Z","iopub.status.idle":"2025-10-28T14:26:07.274442Z","shell.execute_reply.started":"2025-10-28T14:26:07.264516Z","shell.execute_reply":"2025-10-28T14:26:07.273611Z"}},"outputs":[{"name":"stdout","text":"✓ Entity Extractor created\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: CREATE RELATION EXTRACTOR\n# ============================================================================\n\nclass ExtractRelations(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(RelationExtractor)\n    \n    def forward(self, text: str, entities: List[str]) -> List[Tuple[str, str, str]]:\n        if not entities or not text:\n            return []\n        \n        entities_subset = entities[:30]\n        entities_str = json.dumps(entities_subset)\n        \n        result = self.extract(text=text, entities=entities_str)\n        \n        try:\n            triples_text = result.triples.strip()\n            \n            if '[' in triples_text and ']' in triples_text:\n                start = triples_text.find('[')\n                end = triples_text.rfind(']') + 1\n                triples_text = triples_text[start:end]\n            \n            triples = json.loads(triples_text)\n            \n            normalized_triples = []\n            for triple in triples:\n                if isinstance(triple, (list, tuple)) and len(triple) == 3:\n                    s, p, o = triple\n                    s = str(s).lower().strip()\n                    p = str(p).lower().strip()\n                    o = str(o).lower().strip()\n                    \n                    if s and p and o and s != o:\n                        normalized_triples.append((s, p, o))\n            \n            return normalized_triples[:100]\n            \n        except Exception as e:\n            return []\n\nrelation_extractor = ExtractRelations()\nprint(\"✓ Relation Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:26:19.933583Z","iopub.execute_input":"2025-10-28T14:26:19.934291Z","iopub.status.idle":"2025-10-28T14:26:19.942983Z","shell.execute_reply.started":"2025-10-28T14:26:19.934265Z","shell.execute_reply":"2025-10-28T14:26:19.942357Z"}},"outputs":[{"name":"stdout","text":"✓ Relation Extractor created\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ============================================================================\n# CLUSTERING SIGNATURES (DEFINE FIRST!)\n# ============================================================================\n\nclass ClusterValidator(dspy.Signature):\n    \"\"\"Verify if these entities belong in the same cluster.\n    A cluster should contain entities that are the same in meaning, with different:\n    - tenses, plural forms, stem forms, upper/lower cases\n    Or entities with close semantic meanings.\n    \n    Return ONLY valid JSON format: [\"entity1\", \"entity2\", \"entity3\"]\n    Return only entities you are confident belong together.\n    If not confident, return empty list [].\n    \"\"\"\n    \n    entities = dspy.InputField(desc=\"Entities to validate\")\n    valid_cluster = dspy.OutputField(desc=\"Validated cluster in JSON format\")\n\nprint(\"✓ ClusterValidator Signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:26:23.896507Z","iopub.execute_input":"2025-10-28T14:26:23.897273Z","iopub.status.idle":"2025-10-28T14:26:23.902919Z","shell.execute_reply.started":"2025-10-28T14:26:23.897245Z","shell.execute_reply":"2025-10-28T14:26:23.902243Z"}},"outputs":[{"name":"stdout","text":"✓ ClusterValidator Signature defined\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ============================================================================\n# SEMANTIC SIMILARITY CLUSTERING (FROM PAPER)\n# ============================================================================\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nclass SemanticEntityClustering(dspy.Module):\n    def __init__(self, similarity_threshold=0.75):\n        super().__init__()\n        self.validator = dspy.ChainOfThought(ClusterValidator)\n        \n        # Load embedding model (same as paper)\n        print(\"Loading sentence transformer model...\")\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        print(\"✓ Model loaded\")\n        \n        self.similarity_threshold = similarity_threshold\n    \n    def _parse_cluster(self, text: str) -> List[str]:\n        \"\"\"Parse cluster from LLM response\"\"\"\n        try:\n            text = text.strip()\n            if '[' in text and ']' in text:\n                start = text.find('[')\n                end = text.rfind(']') + 1\n                text = text[start:end]\n            \n            cluster = json.loads(text)\n            if isinstance(cluster, list):\n                return [str(e).lower().strip() for e in cluster if e]\n            return []\n        except:\n            return []\n    \n    def _get_semantic_clusters(self, entities: List[str]) -> List[List[str]]:\n        \"\"\"Group entities by semantic similarity using embeddings\"\"\"\n        \n        if len(entities) == 0:\n            return []\n        \n        # Get embeddings for all entities\n        embeddings = self.model.encode(entities)\n        \n        # Compute pairwise cosine similarity\n        similarity_matrix = cosine_similarity(embeddings)\n        \n        # Find clusters using similarity threshold\n        clusters = []\n        remaining = set(range(len(entities)))\n        \n        for i in range(len(entities)):\n            if i not in remaining:\n                continue\n            \n            # Find all entities similar to this one\n            cluster_indices = [i]\n            remaining.discard(i)\n            \n            for j in range(i + 1, len(entities)):\n                if j not in remaining:\n                    continue\n                \n                # Check if similar enough\n                if similarity_matrix[i][j] >= self.similarity_threshold:\n                    cluster_indices.append(j)\n                    remaining.discard(j)\n            \n            # Convert indices to entity names\n            cluster = [entities[idx] for idx in cluster_indices]\n            \n            # Only keep clusters with 2-4 entities\n            if 2 <= len(cluster) <= 4:\n                clusters.append(cluster)\n            elif len(cluster) == 1:\n                # Keep singletons for later\n                pass\n        \n        return clusters\n    \n    def forward(self, entities: List[str]) -> Dict[str, List[str]]:\n        \"\"\"Semantic clustering with LLM validation\"\"\"\n        \n        print(f\"Starting semantic clustering with {len(entities)} entities...\")\n        print(f\"  Similarity threshold: {self.similarity_threshold}\")\n        \n        # Remove duplicates\n        unique_entities = list(set(entities))\n        \n        # Step 1: Find semantic clusters using embeddings\n        print(\"  Computing semantic similarities...\")\n        potential_clusters = self._get_semantic_clusters(unique_entities)\n        \n        print(f\"  Found {len(potential_clusters)} potential clusters\")\n        \n        # Step 2: Validate with LLM\n        validated_clusters = {}\n        cluster_id = 0\n        clustered_entities = set()\n        \n        for cluster in potential_clusters:\n            try:\n                # Ask LLM to validate\n                validation = self.validator(entities=json.dumps(cluster))\n                validated = self._parse_cluster(validation.valid_cluster)\n                \n                if validated and len(validated) >= 2:\n                    cluster_label = validated[0]\n                    validated_clusters[cluster_label] = validated\n                    \n                    for entity in validated:\n                        clustered_entities.add(entity)\n                    \n                    print(f\"  ✓ Cluster {cluster_id}: {validated}\")\n                    cluster_id += 1\n                else:\n                    # LLM rejected - add as singletons\n                    for entity in cluster:\n                        if entity not in clustered_entities:\n                            validated_clusters[entity] = [entity]\n                            clustered_entities.add(entity)\n            except:\n                # Error - add as singletons\n                for entity in cluster:\n                    if entity not in clustered_entities:\n                        validated_clusters[entity] = [entity]\n                        clustered_entities.add(entity)\n        \n        # Step 3: Add all remaining entities as singletons\n        for entity in unique_entities:\n            if entity not in clustered_entities:\n                validated_clusters[entity] = [entity]\n        \n        multi = sum(1 for v in validated_clusters.values() if len(v) > 1)\n        print(f\"✓ Semantic clustering complete: {len(validated_clusters)} total clusters\")\n        print(f\"  Multi-entity clusters: {multi}\")\n        print(f\"  Singleton entities: {len(validated_clusters) - multi}\")\n        \n        return validated_clusters\n\n# Create semantic clusterer with different thresholds\nentity_clusterer_semantic = SemanticEntityClustering(similarity_threshold=0.75)\nprint(\"\\n✓ Semantic Entity Clustering Module created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:26:26.471604Z","iopub.execute_input":"2025-10-28T14:26:26.471917Z","iopub.status.idle":"2025-10-28T14:26:31.177800Z","shell.execute_reply.started":"2025-10-28T14:26:26.471897Z","shell.execute_reply":"2025-10-28T14:26:31.176969Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✓ Model loaded\n\n✓ Semantic Entity Clustering Module created\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ============================================================================\n# FINAL KGGEN WITH SEMANTIC CLUSTERING\n# ============================================================================\n\nclass KGGenSemantic:\n    def __init__(self, similarity_threshold=0.75):\n        self.entity_extractor = entity_extractor\n        self.relation_extractor = relation_extractor\n        self.entity_clusterer = SemanticEntityClustering(similarity_threshold=similarity_threshold)\n        self.graph = nx.DiGraph()\n        self.entity_clusters = {}\n        \n    def generate_from_json(self, json_data: List[Dict], max_docs: int = None) -> nx.DiGraph:\n        \"\"\"Generate KG from JSON dataset\"\"\"\n        all_entities = set()\n        all_relations = []\n        \n        if max_docs:\n            json_data = json_data[:max_docs]\n        \n        print(f\"Processing {len(json_data)} documents...\")\n        print(\"=\" * 80)\n        \n        for idx, item in enumerate(json_data):\n            text = item.get('Article Text', '')\n            concepts = item.get('Concept', [])\n            \n            if not text or len(text.strip()) < 5:\n                continue\n            \n            try:\n                # Extract entities\n                entities = self.entity_extractor(text)\n                all_entities.update(entities)\n                \n                # Add concepts\n                #for concept in concepts:\n                    #if concept and isinstance(concept, str):\n                        #all_entities.add(concept.lower().strip())\n                \n                # Extract relations\n                relations = self.relation_extractor(text, list(all_entities))\n                all_relations.extend(relations)\n                \n                if (idx + 1) % 20 == 0:\n                    print(f\"  {idx + 1}/{len(json_data)} docs | {len(all_entities)} entities | {len(all_relations)} relations\")\n                    \n            except Exception as e:\n                continue\n        \n        print(f\"\\n✓ Extraction complete!\")\n        print(f\"  Total entities: {len(all_entities)}\")\n        print(f\"  Total relations: {len(all_relations)}\")\n        \n        # Build graph\n        for subj, pred, obj in all_relations:\n            self.graph.add_edge(subj, obj, relation=pred)\n        \n        print(f\"  Graph nodes: {len(self.graph.nodes())}\")\n        print(f\"  Graph edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def cluster_entities(self):\n        \"\"\"Semantic clustering with embeddings\"\"\"\n        nodes = list(self.graph.nodes())\n        \n        if len(nodes) == 0:\n            print(\"No nodes to cluster!\")\n            return self.graph\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"SEMANTIC CLUSTERING: {len(nodes)} ENTITIES\")\n        print(f\"{'='*80}\")\n        \n        self.entity_clusters = self.entity_clusterer(nodes)\n        \n        # Map entities\n        entity_mapping = {}\n        for cluster_label, cluster_entities in self.entity_clusters.items():\n            for entity in cluster_entities:\n                entity_mapping[entity] = cluster_label\n        \n        # Rebuild graph\n        new_graph = nx.DiGraph()\n        for u, v, data in self.graph.edges(data=True):\n            new_u = entity_mapping.get(u, u)\n            new_v = entity_mapping.get(v, v)\n            relation = data.get('relation', 'related_to')\n            \n            if new_u == new_v:\n                continue\n            \n            if not new_graph.has_edge(new_u, new_v):\n                new_graph.add_edge(new_u, new_v, relation=relation)\n        \n        self.graph = new_graph\n        \n        print(f\"\\n✓ Clustering complete!\")\n        print(f\"  Final nodes: {len(self.graph.nodes())}\")\n        print(f\"  Final edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def save_graph(self, filepath: str):\n        data = nx.node_link_data(self.graph)\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n        print(f\"✓ Saved to {filepath}\")\n    \n    def export_triples(self, filepath: str):\n        triples = []\n        for u, v, data in self.graph.edges(data=True):\n            triples.append({\n                'subject': u,\n                'predicate': data.get('relation', 'related_to'),\n                'object': v\n            })\n        import pandas as pd\n        df = pd.DataFrame(triples)\n        df.to_csv(filepath, index=False)\n        print(f\"✓ Exported to {filepath}\")\n\n# Initialize semantic KGGen (threshold 0.75 = balanced)\nkg_gen_semantic = KGGenSemantic(similarity_threshold=0.75)\nprint(\"\\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:27:08.541684Z","iopub.execute_input":"2025-10-28T14:27:08.542265Z","iopub.status.idle":"2025-10-28T14:27:11.339590Z","shell.execute_reply.started":"2025-10-28T14:27:08.542242Z","shell.execute_reply":"2025-10-28T14:27:11.338680Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n✓ Model loaded\n\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ============================================================================\n# GENERATE KG FOR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\nall_article_kgs = []\n\ntotal = len(individual_articles)\nprint(f\"\\nProcessing {total} articles...\\n\")\n\nfor idx, article in enumerate(individual_articles[:100]):\n    article_id = article['id']\n    text = article['Article Text']\n    #concepts = article['Concept']\n    \n    # Skip empty articles\n    if not text or len(text.strip()) < 5:\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n        continue\n    \n    try:\n        # Extract entities for THIS article\n        entities = entity_extractor(text)\n        \n        # Add original concepts as entities\n        #for concept in concepts:\n            #if concept and isinstance(concept, str):\n                #entities.append(concept.lower().strip())\n        \n        entities = list(set(entities))  # Remove duplicates\n        \n        # Extract relations for THIS article\n        if entities:\n            relations = relation_extractor(text, entities)\n        else:\n            relations = []\n        \n        # Build graph for THIS article\n        graph = nx.DiGraph()\n        for subj, pred, obj in relations:\n            graph.add_edge(subj, obj, relation=pred)\n        \n        # Store everything for this article\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': graph,\n            'entities': entities,\n            'relations': relations,\n            'num_nodes': len(graph.nodes()),\n            'num_edges': len(graph.edges())\n        })\n        \n        # Progress update\n        if (idx + 1) % 50 == 0:\n            print(f\"✓ Processed {idx + 1}/{total} articles...\")\n        \n    except Exception as e:\n        print(f\"✗ Article {article_id}: Error - {str(e)[:100]}\")\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n\nprint(f\"\\n\" + \"=\" * 80)\nprint(\"✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\")\nprint(\"=\" * 80)\nprint(f\"Total articles processed: {len(all_article_kgs)}\")\nprint(f\"Articles with graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] > 0)}\")\nprint(f\"Articles without graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] == 0)}\")\n\n# Statistics\ntotal_entities = sum(len(kg['entities']) for kg in all_article_kgs)\ntotal_relations = sum(len(kg['relations']) for kg in all_article_kgs)\n\nprint(f\"\\nTotal entities extracted: {total_entities}\")\nprint(f\"Total relations extracted: {total_relations}\")\n\nwith_graphs = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\nif with_graphs:\n    avg_nodes = sum(kg['num_nodes'] for kg in with_graphs) / len(with_graphs)\n    avg_edges = sum(kg['num_edges'] for kg in with_graphs) / len(with_graphs)\n    #print(f\"\\nAverage nodes per KG: {avg_nodes:.2f}\")\n    #print(f\"Average edges per KG: {avg_edges:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:28:19.305191Z","iopub.execute_input":"2025-10-28T14:28:19.305468Z","iopub.status.idle":"2025-10-28T14:38:08.797340Z","shell.execute_reply.started":"2025-10-28T14:28:19.305446Z","shell.execute_reply":"2025-10-28T14:38:08.796535Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nGENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\n================================================================================\n\nProcessing 100 articles...\n\n✓ Processed 50/100 articles...\n✓ Processed 100/100 articles...\n\n================================================================================\n✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\n================================================================================\nTotal articles processed: 100\nArticles with graphs: 98\nArticles without graphs: 2\n\nTotal entities extracted: 2709\nTotal relations extracted: 1488\n\nAverage nodes per KG: 20.35\nAverage edges per KG: 14.36\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom dspy.signatures import Signature\nfrom dspy import OutputField, InputField\nimport pandas as pd\nimport os\nfrom collections import Counter\nimport numpy as np\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:47:23.356041Z","iopub.execute_input":"2025-10-28T14:47:23.356507Z","iopub.status.idle":"2025-10-28T14:47:23.361311Z","shell.execute_reply.started":"2025-10-28T14:47:23.356481Z","shell.execute_reply":"2025-10-28T14:47:23.360665Z"}},"outputs":[{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: LOAD DATA AND EXISTING KNOWLEDGE GRAPHS\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"Load JSON data from file\"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\ndef load_knowledge_graphs(filepath: str) -> List[Dict]:\n    \"\"\"Load previously created knowledge graphs\"\"\"\n    if not os.path.exists(filepath):\n        print(f\"\\n⚠️  WARNING: Knowledge graph file not found at {filepath}\")\n        return None\n    \n    with open(filepath, 'r', encoding='utf-8') as f:\n        kgs = json.load(f)\n    print(f\"✓ Loaded {len(kgs)} knowledge graphs from {filepath}\")\n    return kgs\n\n# Load your data\narticles = load_json_data('/kaggle/input/zstikg/MedicalConcept Data-set.json')\nknowledge_graphs = load_knowledge_graphs('/kaggle/working/article_knowledge_graphs.json')\n\n# Only classify articles that have KGs\narticles_with_kgs = articles[:len(knowledge_graphs)]  # First 100 articles\n\nif knowledge_graphs is None:\n    print(\"\\n❌ Cannot proceed without knowledge graphs. Please save them first.\")\nelse:\n    print(f\"\\n✓ Successfully loaded {len(articles)} articles and {len(knowledge_graphs)} KGs\")\n    print(\"✓ Ready to proceed with topic classification!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:47:26.822688Z","iopub.execute_input":"2025-10-28T14:47:26.823232Z","iopub.status.idle":"2025-10-28T14:47:26.884140Z","shell.execute_reply.started":"2025-10-28T14:47:26.823209Z","shell.execute_reply":"2025-10-28T14:47:26.883376Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 2066 documents from /kaggle/input/zstikg/MedicalConcept Data-set.json\n✓ Loaded 100 knowledge graphs from /kaggle/working/article_knowledge_graphs.json\n\n✓ Successfully loaded 2066 articles and 100 KGs\n✓ Ready to proceed with topic classification!\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: EXTRACT ALL UNIQUE CONCEPTS FROM DATASET\n# ============================================================================\n\ndef extract_unique_concepts(articles: List[Dict]) -> List[str]:\n    \"\"\"\n    Extract all unique concepts from the dataset's 'Concept' field.\n    \n    Args:\n        articles: List of article dictionaries\n    \n    Returns:\n        Sorted list of unique concepts\n    \"\"\"\n    all_concepts = set()\n    \n    for article in articles:\n        # Get concepts from the Concept field\n        if 'Concept' in article and article['Concept']:\n            if isinstance(article['Concept'], list):\n                all_concepts.update([c.lower().strip() for c in article['Concept'] if c])\n            elif isinstance(article['Concept'], str):\n                all_concepts.add(article['Concept'].lower().strip())\n    \n    # Convert to sorted list\n    unique_concepts = sorted(list(all_concepts))\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"UNIQUE CONCEPTS EXTRACTED FROM DATASET\")\n    print(f\"{'='*80}\")\n    print(f\"Total unique concepts: {len(unique_concepts)}\")\n    print(f\"\\nFirst 20 concepts: {unique_concepts[:20]}\")\n    print(f\"\\nLast 20 concepts: {unique_concepts[-20:]}\")\n    \n    return unique_concepts\n\n\n# Extract all unique concepts from the dataset\nall_concepts = extract_unique_concepts(articles)\n\n# Save concept list for reference\nwith open('/kaggle/working/all_concepts.json', 'w') as f:\n    json.dump(all_concepts, f, indent=2)\n\nprint(f\"\\n✓ Saved concept list to /kaggle/working/all_concepts.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:47:30.537958Z","iopub.execute_input":"2025-10-28T14:47:30.538818Z","iopub.status.idle":"2025-10-28T14:47:30.547782Z","shell.execute_reply.started":"2025-10-28T14:47:30.538784Z","shell.execute_reply":"2025-10-28T14:47:30.546987Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nUNIQUE CONCEPTS EXTRACTED FROM DATASET\n================================================================================\nTotal unique concepts: 18\n\nFirst 20 concepts: ['addiction', 'alcohol', 'arthritis', 'brain and cognitive health', 'breast cancer', 'cancer', \"children's health\", 'exercise and fitness', 'headache', 'healthy eating', 'heart health', 'mental health', 'osteoporosis', 'pain management', 'prostate knowledge', 'sleep', 'smoking cessation', \"women's health\"]\n\nLast 20 concepts: ['addiction', 'alcohol', 'arthritis', 'brain and cognitive health', 'breast cancer', 'cancer', \"children's health\", 'exercise and fitness', 'headache', 'healthy eating', 'heart health', 'mental health', 'osteoporosis', 'pain management', 'prostate knowledge', 'sleep', 'smoking cessation', \"women's health\"]\n\n✓ Saved concept list to /kaggle/working/all_concepts.json\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: KG FORMATTING FUNCTION\n# ============================================================================\n\ndef format_kg_for_llm(kg_data: Dict) -> str:\n    \"\"\"Format knowledge graph into readable text for the LLM.\"\"\"\n    if not kg_data or kg_data.get('num_edges', 0) == 0:\n        return \"No knowledge graph available.\"\n    \n    # Format entities\n    entities = kg_data.get('entities', [])\n    entities_str = \", \".join(entities[:30]) if entities else \"None\"\n    if len(entities) > 30:\n        entities_str += f\"... ({len(entities) - 30} more)\"\n    \n    # Format relationships\n    relations = kg_data.get('relations', [])\n    if relations:\n        relationships = []\n        for relation in relations[:20]:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                relationships.append(f\"{source} --[{rel_type}]--> {target}\")\n        relationships_str = \"\\n\".join(relationships)\n        if len(relations) > 20:\n            relationships_str += f\"\\n... ({len(relations) - 20} more relationships)\"\n    else:\n        relationships_str = \"None\"\n    \n    kg_summary = f\"\"\"Knowledge Graph Summary:\nEntities: {entities_str}\n\nKey Relationships:\n{relationships_str}\"\"\"\n    \n    return kg_summary\n\nprint(\"✓ KG formatting function ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T14:47:39.626875Z","iopub.execute_input":"2025-10-28T14:47:39.627435Z","iopub.status.idle":"2025-10-28T14:47:39.634356Z","shell.execute_reply.started":"2025-10-28T14:47:39.627413Z","shell.execute_reply":"2025-10-28T14:47:39.633497Z"}},"outputs":[{"name":"stdout","text":"✓ KG formatting function ready\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: DEFINE DSPY SIGNATURE FOR TOPIC CLASSIFICATION\n# ============================================================================\n\nclass TopicClassification(Signature):\n    \"\"\"Given an article text, its knowledge graph, and a list of possible topics,\n    determine which topics (can be multiple) are most relevant to this article.\n    Return only topic names that are actually present in the available_topics list.\"\"\"\n    \n    article_text: str = InputField(\n        desc=\"The text content of the article\"\n    )\n    knowledge_graph: str = InputField(\n        desc=\"Knowledge graph extracted from the article showing entities and relationships\"\n    )\n    available_topics: str = InputField(\n        desc=\"Comma-separated list of all possible topic/concept names\"\n    )\n    \n    predicted_topics: str = OutputField(\n        desc=\"Comma-separated list of relevant topics from the available_topics list. \"\n             \"Only return topics that actually appear in the available_topics list. \"\n             \"If multiple topics apply, list them all. If no topics match well, return 'none'.\"\n    )\n    #confidence: str = OutputField(\n        #desc=\"Confidence level: high, medium, or low\"\n    #)\n    #reasoning: str = OutputField(\n        #desc=\"Brief explanation of why these topics were chosen based on the article and knowledge graph\"\n    #)\n\nprint(\"✓ Topic classification signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:00:21.295463Z","iopub.execute_input":"2025-10-28T15:00:21.296238Z","iopub.status.idle":"2025-10-28T15:00:21.302746Z","shell.execute_reply.started":"2025-10-28T15:00:21.296209Z","shell.execute_reply":"2025-10-28T15:00:21.301993Z"}},"outputs":[{"name":"stdout","text":"✓ Topic classification signature defined\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# ============================================================================\n# STEP 7: CREATE TOPIC CLASSIFIER CLASS\n# ============================================================================\n\nclass ArticleTopicClassifier:\n    \"\"\"Multi-label topic classifier using article text, KG, and available topics.\"\"\"\n    \n    def __init__(self, available_topics: List[str]):\n        self.available_topics = available_topics\n        self.available_topics_str = \", \".join(available_topics)\n        self.classifier = dspy.ChainOfThought(TopicClassification)\n        print(f\"✓ Classifier initialized with {len(available_topics)} possible topics\")\n    \n    def classify_article(self, article_text: str, kg_data: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        kg_summary = format_kg_for_llm(kg_data)\n        \n        # Truncate article if too long\n        #max_text_length = 500\n        #if len(article_text) > max_text_length:\n            #article_text = article_text[:max_text_length] + \"...\"\n        \n        try:\n            result = self.classifier(\n                article_text=article_text,\n                knowledge_graph=kg_summary,\n                available_topics=self.available_topics_str\n            )\n            \n            predicted_topics_raw = result.predicted_topics\n            \n            if predicted_topics_raw.lower() == 'none':\n                predicted_topics = []\n            else:\n                predicted_topics = [\n                    t.strip().lower() \n                    for t in predicted_topics_raw.split(',')\n                    if t.strip()\n                ]\n                predicted_topics = [\n                    t for t in predicted_topics \n                    if t in [at.lower() for at in self.available_topics]\n                ]\n            \n            return {\n                'predicted_topics': predicted_topics,\n                'num_topics': len(predicted_topics),\n                #'confidence': result.confidence,\n                #'reasoning': result.reasoning\n            }\n            \n        except Exception as e:\n            print(f\"  Error: {str(e)}\")\n            return {\n                'predicted_topics': [],\n                'num_topics': 0,\n                #'confidence': 'error',\n                #'reasoning': f'Classification failed: {str(e)}'\n            }\n    \n    def classify_dataset(self, articles: List[Dict], knowledge_graphs: List[Dict],\n                        max_articles: int = None) -> List[Dict]:\n        \"\"\"Classify multiple articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        if max_articles:\n            articles = articles[:max_articles]\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(articles)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx, article in enumerate(articles):\n            print(f\"Processing article {idx}...\", end=\" \")\n            \n            article_text = article.get('Article Text', '')\n            kg_data = kg_dict.get(idx, {'num_edges': 0})\n            classification = self.classify_article(article_text, kg_data)\n            \n            result = {\n                'article_id': idx,\n                'article_text': article_text[:100] + \"...\",\n                'true_concepts': article.get('Concept', []),\n                'predicted_topics': classification['predicted_topics'],\n                'num_predicted': classification['num_topics'],\n                #'confidence': classification['confidence'],\n                #'reasoning': classification['reasoning']\n            }\n            \n            results.append(result)\n            print(f\"✓ Found {len(classification['predicted_topics'])} topics\")\n        \n        return results\n\nprint(\"✓ Classifier class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:12:41.176077Z","iopub.execute_input":"2025-10-28T15:12:41.176793Z","iopub.status.idle":"2025-10-28T15:12:41.187576Z","shell.execute_reply.started":"2025-10-28T15:12:41.176766Z","shell.execute_reply":"2025-10-28T15:12:41.186923Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier class defined\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# ============================================================================\n# STEP 8: ANALYSIS FUNCTIONS\n# ============================================================================\n\ndef analyze_classification_results(results: List[Dict], available_topics: List[str]) -> Dict:\n    \"\"\"Analyze classification results and generate statistics.\"\"\"\n    from collections import Counter\n    \n    topic_counts = Counter()\n    #confidence_dist = Counter()\n    \n    multi_label_count = 0\n    no_label_count = 0\n    \n    for result in results:\n        predicted = result['predicted_topics']\n        \n        if len(predicted) == 0:\n            no_label_count += 1\n        elif len(predicted) > 1:\n            multi_label_count += 1\n        \n        for topic in predicted:\n            topic_counts[topic] += 1\n        \n        #confidence_dist[result['confidence']] += 1\n    \n    stats = {\n        'total_articles': len(results),\n        'articles_with_topics': len(results) - no_label_count,\n        'articles_without_topics': no_label_count,\n        'multi_label_articles': multi_label_count,\n        #'avg_topics_per_article': sum(len(r['predicted_topics']) for r in results) / len(results),\n        #'most_common_topics': topic_counts.most_common(10)\n    }\n    \n    print(f\"\\n{'='*80}\")\n    print(\"CLASSIFICATION RESULTS SUMMARY\")\n    print(f\"{'='*80}\")\n    print(f\"\\nTotal articles: {stats['total_articles']}\")\n    print(f\"Articles with topics: {stats['articles_with_topics']} ({stats['articles_with_topics']/stats['total_articles']*100:.1f}%)\")\n    print(f\"Multi-label articles: {stats['multi_label_articles']} ({stats['multi_label_articles']/stats['total_articles']*100:.1f}%)\")\n    #print(f\"Average topics per article: {stats['avg_topics_per_article']:.2f}\")\n    \n    print(f\"\\n{'='*80}\")\n    print(\"TOP 10 MOST FREQUENT TOPICS\")\n    print(f\"{'='*80}\")\n    #for topic, count in stats['most_common_topics']:\n        #print(f\"  {topic}: {count} articles ({count/stats['total_articles']*100:.1f}%)\")\n    \n    return stats\n\ndef save_results(results: List[Dict], output_path: str):\n    \"\"\"Save classification results to JSON file.\"\"\"\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(results, f, indent=2, ensure_ascii=False)\n    print(f\"\\n✓ Results saved to {output_path}\")\n\ndef export_to_csv(results: List[Dict], output_path: str):\n    \"\"\"Export results to CSV for easy analysis.\"\"\"\n    rows = []\n    for r in results:\n        rows.append({\n            'article_id': r['article_id'],\n            'article_preview': r['article_text'],\n            'true_concepts': '|'.join(r['true_concepts']) if r['true_concepts'] else '',\n            'predicted_topics': '|'.join(r['predicted_topics']),\n            'num_predicted': r['num_predicted'],\n            #'confidence': r['confidence'],\n            #'reasoning': r['reasoning']\n        })\n    \n    df = pd.DataFrame(rows)\n    df.to_csv(output_path, index=False, encoding='utf-8')\n    print(f\"✓ CSV exported to {output_path}\")\n\nprint(\"✓ Analysis functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:12:47.959070Z","iopub.execute_input":"2025-10-28T15:12:47.959344Z","iopub.status.idle":"2025-10-28T15:12:47.968721Z","shell.execute_reply.started":"2025-10-28T15:12:47.959324Z","shell.execute_reply":"2025-10-28T15:12:47.968016Z"}},"outputs":[{"name":"stdout","text":"✓ Analysis functions defined\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# ============================================================================\n# RUN CLASSIFICATION ON 100 ARTICLES\n# ============================================================================\n\n# Initialize classifier\nclassifier = ArticleTopicClassifier(all_concepts)\n\n# Classify all 100 articles with KGs\nresults = classifier.classify_dataset(\n    articles=articles,\n    knowledge_graphs=knowledge_graphs,\n    max_articles=100\n)\n\n# Analyze results\nstats = analyze_classification_results(results, all_concepts)\n\n# Save results\nsave_results(results, '/kaggle/working/topic_classification_results.json')\nexport_to_csv(results, '/kaggle/working/topic_classification_results.csv')\n\n# Show sample results\nprint(f\"\\n{'='*80}\")\nprint(\"SAMPLE RESULTS (First 3)\")\nprint(f\"{'='*80}\\n\")\n\nfor result in results[:3]:\n    print(f\"Article {result['article_id']}:\")\n    print(f\"  Preview: {result['article_text']}\")\n    print(f\"  True: {result['true_concepts']}\")\n    print(f\"  Predicted: {result['predicted_topics']}\")\n    #print(f\"  Confidence: {result['confidence']}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:12:51.990081Z","iopub.execute_input":"2025-10-28T15:12:51.990315Z","iopub.status.idle":"2025-10-28T15:14:11.741453Z","shell.execute_reply.started":"2025-10-28T15:12:51.990298Z","shell.execute_reply":"2025-10-28T15:14:11.740621Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier initialized with 18 possible topics\n\n================================================================================\nCLASSIFYING 100 ARTICLES\n================================================================================\n\nProcessing article 0... ","output_type":"stream"},{"name":"stderr","text":"2025/10/28 15:13:12 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"This model's maximum context length is 16384 tokens. However, you requested 16417 tokens (4417 in the messages, 12000 in the completion). Please reduce the length of the messages or completion.\",\"type\":\"BadRequestError\"}\n\n✓ Found 0 topics\nProcessing article 1... ✓ Found 1 topics\nProcessing article 2... ✓ Found 0 topics\nProcessing article 3... ✓ Found 5 topics\nProcessing article 4... ✓ Found 2 topics\nProcessing article 5... ✓ Found 7 topics\nProcessing article 6... ✓ Found 1 topics\nProcessing article 7... ✓ Found 3 topics\nProcessing article 8... ✓ Found 2 topics\nProcessing article 9... ✓ Found 0 topics\nProcessing article 10... ","output_type":"stream"},{"name":"stderr","text":"2025/10/28 15:13:44 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n","output_type":"stream"},{"name":"stdout","text":"  Error: litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"This model's maximum context length is 16384 tokens. However, you requested 16417 tokens (4417 in the messages, 12000 in the completion). Please reduce the length of the messages or completion.\",\"type\":\"BadRequestError\"}\n\n✓ Found 0 topics\nProcessing article 11... ✓ Found 1 topics\nProcessing article 12... ✓ Found 0 topics\nProcessing article 13... ✓ Found 5 topics\nProcessing article 14... ✓ Found 2 topics\nProcessing article 15... ✓ Found 7 topics\nProcessing article 16... ✓ Found 1 topics\nProcessing article 17... ✓ Found 3 topics\nProcessing article 18... ✓ Found 2 topics\nProcessing article 19... ✓ Found 0 topics\nProcessing article 20... ✓ Found 3 topics\nProcessing article 21... ✓ Found 1 topics\nProcessing article 22... ✓ Found 4 topics\nProcessing article 23... ✓ Found 4 topics\nProcessing article 24... ✓ Found 4 topics\nProcessing article 25... ✓ Found 3 topics\nProcessing article 26... ✓ Found 3 topics\nProcessing article 27... ✓ Found 1 topics\nProcessing article 28... ✓ Found 4 topics\nProcessing article 29... ✓ Found 3 topics\nProcessing article 30... ✓ Found 5 topics\nProcessing article 31... ✓ Found 5 topics\nProcessing article 32... ✓ Found 1 topics\nProcessing article 33... ✓ Found 7 topics\nProcessing article 34... ✓ Found 1 topics\nProcessing article 35... ✓ Found 1 topics\nProcessing article 36... ✓ Found 3 topics\nProcessing article 37... ✓ Found 1 topics\nProcessing article 38... ✓ Found 2 topics\nProcessing article 39... ✓ Found 4 topics\nProcessing article 40... ✓ Found 3 topics\nProcessing article 41... ✓ Found 1 topics\nProcessing article 42... ✓ Found 1 topics\nProcessing article 43... ✓ Found 1 topics\nProcessing article 44... ✓ Found 4 topics\nProcessing article 45... ✓ Found 1 topics\nProcessing article 46... ✓ Found 2 topics\nProcessing article 47... ✓ Found 2 topics\nProcessing article 48... ✓ Found 1 topics\nProcessing article 49... ✓ Found 1 topics\nProcessing article 50... ✓ Found 7 topics\nProcessing article 51... ✓ Found 6 topics\nProcessing article 52... ✓ Found 3 topics\nProcessing article 53... ✓ Found 3 topics\nProcessing article 54... ✓ Found 1 topics\nProcessing article 55... ✓ Found 0 topics\nProcessing article 56... ✓ Found 2 topics\nProcessing article 57... ✓ Found 2 topics\nProcessing article 58... ✓ Found 2 topics\nProcessing article 59... ✓ Found 2 topics\nProcessing article 60... ✓ Found 2 topics\nProcessing article 61... ✓ Found 0 topics\nProcessing article 62... ✓ Found 5 topics\nProcessing article 63... ✓ Found 2 topics\nProcessing article 64... ✓ Found 2 topics\nProcessing article 65... ✓ Found 5 topics\nProcessing article 66... ✓ Found 1 topics\nProcessing article 67... ✓ Found 2 topics\nProcessing article 68... ✓ Found 2 topics\nProcessing article 69... ✓ Found 2 topics\nProcessing article 70... ✓ Found 1 topics\nProcessing article 71... ✓ Found 5 topics\nProcessing article 72... ✓ Found 1 topics\nProcessing article 73... ✓ Found 3 topics\nProcessing article 74... ✓ Found 5 topics\nProcessing article 75... ✓ Found 4 topics\nProcessing article 76... ✓ Found 3 topics\nProcessing article 77... ✓ Found 12 topics\nProcessing article 78... ✓ Found 2 topics\nProcessing article 79... ✓ Found 2 topics\nProcessing article 80... ✓ Found 1 topics\nProcessing article 81... ✓ Found 5 topics\nProcessing article 82... ✓ Found 1 topics\nProcessing article 83... ✓ Found 3 topics\nProcessing article 84... ✓ Found 5 topics\nProcessing article 85... ✓ Found 4 topics\nProcessing article 86... ✓ Found 3 topics\nProcessing article 87... ✓ Found 12 topics\nProcessing article 88... ✓ Found 2 topics\nProcessing article 89... ✓ Found 4 topics\nProcessing article 90... ✓ Found 3 topics\nProcessing article 91... ✓ Found 2 topics\nProcessing article 92... ✓ Found 3 topics\nProcessing article 93... ✓ Found 4 topics\nProcessing article 94... ✓ Found 3 topics\nProcessing article 95... ✓ Found 0 topics\nProcessing article 96... ✓ Found 2 topics\nProcessing article 97... ✓ Found 1 topics\nProcessing article 98... ✓ Found 4 topics\nProcessing article 99... ✓ Found 4 topics\n\n================================================================================\nCLASSIFICATION RESULTS SUMMARY\n================================================================================\n\nTotal articles: 100\nArticles with topics: 91 (91.0%)\nMulti-label articles: 68 (68.0%)\n\n================================================================================\nTOP 10 MOST FREQUENT TOPICS\n================================================================================\n\n✓ Results saved to /kaggle/working/topic_classification_results.json\n✓ CSV exported to /kaggle/working/topic_classification_results.csv\n\n================================================================================\nSAMPLE RESULTS (First 3)\n================================================================================\n\nArticle 0:\n  Preview: 1. Health care reform\n\nHow could the health care reform legislation that President Barack Obama sign...\n  True: []\n  Predicted: []\n\nArticle 1:\n  Preview: My colleagues at Harvard Health Publishing and I have a mission: to provide accurate, reliable infor...\n  True: ['Heart Health', \"Women's Health\"]\n  Predicted: ['heart health']\n\nArticle 2:\n  Preview: Does echinacea, the popular natural cold remedy, really work?\n\nIt depends on what you mean by “work....\n  True: []\n  Predicted: []\n\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION: Compare Predicted vs True Concepts\n# ============================================================================\n\ndef evaluate_classification(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate accuracy metrics by comparing predicted topics with true concepts.\n    \"\"\"\n    total = len(results)\n    exact_match = 0  # All predicted topics match exactly with true concepts\n    partial_match = 0  # At least one topic matches\n    no_match = 0  # No topics match\n    no_true_labels = 0  # Articles with no true concepts\n    \n    all_precisions = []\n    all_recalls = []\n    all_f1s = []\n    \n    correct_predictions = []\n    incorrect_predictions = []\n    \n    for result in results:\n        true_set = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_set = set(result['predicted_topics'])\n        \n        # Skip articles with no true labels\n        if len(true_set) == 0:\n            no_true_labels += 1\n            continue\n        \n        # Find intersection\n        intersection = true_set & pred_set\n        \n        # Exact match\n        if true_set == pred_set and len(true_set) > 0:\n            exact_match += 1\n            correct_predictions.append(result)\n        elif len(intersection) > 0:\n            partial_match += 1\n        else:\n            no_match += 1\n            incorrect_predictions.append(result)\n        \n        # Calculate precision, recall, F1\n        if len(pred_set) > 0:\n            precision = len(intersection) / len(pred_set)\n            all_precisions.append(precision)\n        else:\n            all_precisions.append(0)\n        \n        if len(true_set) > 0:\n            recall = len(intersection) / len(true_set)\n            all_recalls.append(recall)\n        else:\n            all_recalls.append(0)\n        \n        if all_precisions[-1] + all_recalls[-1] > 0:\n            f1 = 2 * (all_precisions[-1] * all_recalls[-1]) / (all_precisions[-1] + all_recalls[-1])\n            all_f1s.append(f1)\n        else:\n            all_f1s.append(0)\n    \n    articles_with_true_labels = total - no_true_labels\n    \n    metrics = {\n        'total_articles': total,\n        'articles_with_true_labels': articles_with_true_labels,\n        'articles_without_true_labels': no_true_labels,\n        'exact_matches': exact_match,\n        'partial_matches': partial_match,\n        'no_matches': no_match,\n        'exact_match_rate': exact_match / articles_with_true_labels if articles_with_true_labels > 0 else 0,\n        'partial_match_rate': partial_match / articles_with_true_labels if articles_with_true_labels > 0 else 0,\n        'avg_precision': np.mean(all_precisions) if all_precisions else 0,\n        'avg_recall': np.mean(all_recalls) if all_recalls else 0,\n        'avg_f1': np.mean(all_f1s) if all_f1s else 0,\n        'correct_examples': correct_predictions[:3],\n        'incorrect_examples': incorrect_predictions[:3]\n    }\n    \n    # Print results\n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS\")\n    print(f\"{'='*80}\")\n    print(f\"\\nTotal articles evaluated: {articles_with_true_labels}\")\n    print(f\"Articles without labels (skipped): {no_true_labels}\")\n    print(f\"\\n📊 ACCURACY:\")\n    print(f\"  ✓ Exact matches: {exact_match} ({metrics['exact_match_rate']*100:.1f}%)\")\n    print(f\"  ~ Partial matches: {partial_match} ({metrics['partial_match_rate']*100:.1f}%)\")\n    print(f\"  ✗ No matches: {no_match} ({no_match/articles_with_true_labels*100:.1f}%)\")\n    print(f\"\\n📈 PERFORMANCE METRICS:\")\n    print(f\"  Precision: {metrics['avg_precision']:.3f} (how many predicted topics were correct)\")\n    print(f\"  Recall: {metrics['avg_recall']:.3f} (how many true topics were found)\")\n    print(f\"  F1 Score: {metrics['avg_f1']:.3f} (overall accuracy)\")\n    \n    return metrics\n\n\n# Run evaluation\nprint(\"\\nEvaluating classification accuracy...\")\neval_metrics = evaluate_classification(results)\n\n# Show correct examples\nprint(f\"\\n{'='*80}\")\nprint(\"✓ CORRECTLY CLASSIFIED EXAMPLES\")\nprint(f\"{'='*80}\")\nfor ex in eval_metrics['correct_examples']:\n    print(f\"\\nArticle {ex['article_id']}:\")\n    print(f\"  True: {ex['true_concepts']}\")\n    print(f\"  Predicted: {ex['predicted_topics']}\")\n    print(f\"  ✓ EXACT MATCH!\")\n\n# Show incorrect examples\nprint(f\"\\n{'='*80}\")\nprint(\"✗ INCORRECTLY CLASSIFIED EXAMPLES\")\nprint(f\"{'='*80}\")\nfor ex in eval_metrics['incorrect_examples']:\n    print(f\"\\nArticle {ex['article_id']}:\")\n    print(f\"  True: {ex['true_concepts']}\")\n    print(f\"  Predicted: {ex['predicted_topics']}\")\n    #print(f\"  Reasoning: {ex['reasoning'][:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:18:35.160091Z","iopub.execute_input":"2025-10-28T15:18:35.160589Z","iopub.status.idle":"2025-10-28T15:18:35.174289Z","shell.execute_reply.started":"2025-10-28T15:18:35.160567Z","shell.execute_reply":"2025-10-28T15:18:35.173478Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating classification accuracy...\n\n================================================================================\nEVALUATION METRICS\n================================================================================\n\nTotal articles evaluated: 76\nArticles without labels (skipped): 24\n\n📊 ACCURACY:\n  ✓ Exact matches: 24 (31.6%)\n  ~ Partial matches: 46 (60.5%)\n  ✗ No matches: 6 (7.9%)\n\n📈 PERFORMANCE METRICS:\n  Precision: 0.596 (how many predicted topics were correct)\n  Recall: 0.857 (how many true topics were found)\n  F1 Score: 0.656 (overall accuracy)\n\n================================================================================\n✓ CORRECTLY CLASSIFIED EXAMPLES\n================================================================================\n\nArticle 4:\n  True: ['Healthy Eating', 'Heart Health']\n  Predicted: ['healthy eating', 'heart health']\n  ✓ EXACT MATCH!\n\nArticle 6:\n  True: ['Mental Health']\n  Predicted: ['mental health']\n  ✓ EXACT MATCH!\n\nArticle 8:\n  True: ['Heart Health', 'Mental Health']\n  Predicted: ['heart health', 'mental health']\n  ✓ EXACT MATCH!\n\n================================================================================\n✗ INCORRECTLY CLASSIFIED EXAMPLES\n================================================================================\n\nArticle 9:\n  True: ['Exercise and Fitness', 'Mental Health']\n  Predicted: []\n\nArticle 19:\n  True: ['Exercise and Fitness', 'Mental Health']\n  Predicted: []\n\nArticle 49:\n  True: ['Cancer', 'Prostate Knowledge']\n  Predicted: [\"women's health\"]\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION METRICS (Following the Paper's Methodology)\n# ============================================================================\n\ndef calculate_metrics_paper_style(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate evaluation metrics following the paper's approach:\n    \"For each article, the model inferred topic(s) were compared against \n    the list of 'gold' topic(s) to compute the true positive, false positive, \n    and false negative statistics for that article. Then, all such statistics \n    for all the articles in a dataset were aggregated and used to compute \n    the final Precision, Recall, and micro-averaged F1 score.\"\n    \n    Reference: Section 5.4 of the paper\n    \"\"\"\n    # Aggregate statistics across all articles\n    total_tp = 0  # True Positives\n    total_fp = 0  # False Positives\n    total_fn = 0  # False Negatives\n    \n    articles_evaluated = 0\n    articles_skipped = 0\n    \n    for result in results:\n        # Get true and predicted topics (lowercased and stripped)\n        true_topics = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_topics = set([t.lower().strip() for t in result['predicted_topics']])\n        \n        # Skip articles with no ground truth labels\n        if len(true_topics) == 0:\n            articles_skipped += 1\n            continue\n        \n        articles_evaluated += 1\n        \n        # Calculate TP, FP, FN for this article\n        tp = len(true_topics & pred_topics)  # Intersection\n        fp = len(pred_topics - true_topics)  # Predicted but not true\n        fn = len(true_topics - pred_topics)  # True but not predicted\n        \n        # Aggregate\n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n    \n    # Calculate micro-averaged metrics\n    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    metrics = {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'total_tp': total_tp,\n        'total_fp': total_fp,\n        'total_fn': total_fn,\n        'articles_evaluated': articles_evaluated,\n        'articles_skipped': articles_skipped\n    }\n    \n    # Print results\n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS (Paper's Methodology)\")\n    print(f\"{'='*80}\")\n    print(f\"\\nArticles evaluated: {articles_evaluated}\")\n    print(f\"Articles skipped (no ground truth): {articles_skipped}\")\n    print(f\"\\nAggregated Statistics:\")\n    print(f\"  True Positives (TP): {total_tp}\")\n    print(f\"  False Positives (FP): {total_fp}\")\n    print(f\"  False Negatives (FN): {total_fn}\")\n    print(f\"\\n📊 MICRO-AVERAGED METRICS:\")\n    print(f\"  Precision: {precision:.3f}\")\n    print(f\"  Recall: {recall:.3f}\")\n    print(f\"  F1 Score: {f1_score:.3f}\")\n    \n    return metrics\n\n\n# Run evaluation using the paper's methodology\nprint(\"\\nEvaluating using paper's methodology...\")\npaper_metrics = calculate_metrics_paper_style(results)\n\n# Compare with their baselines (from Table 6 in the paper)\nprint(f\"\\n{'='*80}\")\nprint(\"COMPARISON WITH PAPER'S BASELINES (Medical Dataset)\")\nprint(f\"{'='*80}\")\nprint(f\"This model's F1 Score: {paper_metrics['f1_score']:.3f}\")\nprint(f\"\\nPaper's Results for Medical Dataset:\")\nprint(f\"  GFLM-S baseline: 0.532\")\nprint(f\"  GFLM-W baseline: 0.530\")\nprint(f\"  SBERT (best mid encoder): 0.594\")\nprint(f\"  ChatGPT-3.5 (best overall): 0.606\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:19:29.022762Z","iopub.execute_input":"2025-10-28T15:19:29.023458Z","iopub.status.idle":"2025-10-28T15:19:29.033780Z","shell.execute_reply.started":"2025-10-28T15:19:29.023432Z","shell.execute_reply":"2025-10-28T15:19:29.033192Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating using paper's methodology...\n\n================================================================================\nEVALUATION METRICS (Paper's Methodology)\n================================================================================\n\nArticles evaluated: 76\nArticles skipped (no ground truth): 24\n\nAggregated Statistics:\n  True Positives (TP): 95\n  False Positives (FP): 97\n  False Negatives (FN): 21\n\n📊 MICRO-AVERAGED METRICS:\n  Precision: 0.495\n  Recall: 0.819\n  F1 Score: 0.617\n\n================================================================================\nCOMPARISON WITH PAPER'S BASELINES (Medical Dataset)\n================================================================================\nThis model's F1 Score: 0.617\n\nPaper's Results for Medical Dataset:\n  GFLM-S baseline: 0.532\n  GFLM-W baseline: 0.530\n  SBERT (best mid encoder): 0.594\n  ChatGPT-3.5 (best overall): 0.606\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: LOAD KEYWORDS\n# ============================================================================\n\n# Load the keyword file\nwith open('/kaggle/input/zstikg/Keyword_Medical.json', 'r') as f:\n    keyword_data = json.load(f)\n\n# Parse into dictionary: {topic_name: [keywords]}\nconcept_keywords = {}\n\nfor item in keyword_data:\n    keywords_list = item['Keyword']\n    topic_name = keywords_list[0].lower()  # First item is topic name\n    related_keywords = [kw.lower() for kw in keywords_list[1:]]  # Rest are keywords\n    concept_keywords[topic_name] = related_keywords\n\nprint(f\"\\n{'='*80}\")\nprint(\"LOADED CONCEPT KEYWORDS\")\nprint(f\"{'='*80}\")\nprint(f\"Total concepts: {len(concept_keywords)}\\n\")\n\n# Show first 3 examples\nfor i, (topic, keywords) in enumerate(list(concept_keywords.items())[:3]):\n    print(f\"{i+1}. {topic}:\")\n    print(f\"   Keywords: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: BUILD CONCEPT KNOWLEDGE GRAPHS\n# ============================================================================\n\nclass ConceptKGBuilder:\n    \"\"\"Build knowledge graphs for concepts using topic names and keywords.\"\"\"\n    \n    def __init__(self):\n        self.concept_kgs = {}\n    \n    def build_concept_kg(self, topic_name: str, keywords: List[str]) -> nx.DiGraph:\n        \"\"\"Build a KG for a single concept.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add topic as central node\n        G.add_node(topic_name, node_type='topic')\n        \n        # Add keywords and connect to topic\n        for keyword in keywords:\n            G.add_node(keyword, node_type='keyword')\n            G.add_edge(topic_name, keyword, relation='has_keyword')\n        \n        # Connect keywords to each other (co-occurrence)\n        for i, kw1 in enumerate(keywords):\n            for kw2 in keywords[i+1:]:\n                G.add_edge(kw1, kw2, relation='co_occurs')\n        \n        return G\n    \n    def build_all_concept_kgs(self, concept_keywords: Dict[str, List[str]]) -> Dict[str, nx.DiGraph]:\n        \"\"\"Build KGs for all concepts.\"\"\"\n        for topic, keywords in concept_keywords.items():\n            self.concept_kgs[topic] = self.build_concept_kg(topic, keywords)\n        \n        print(f\"\\n{'='*80}\")\n        print(\"BUILT CONCEPT KNOWLEDGE GRAPHS\")\n        print(f\"{'='*80}\")\n        print(f\"Total concept KGs: {len(self.concept_kgs)}\")\n        \n        # Show stats\n        for topic, kg in list(self.concept_kgs.items())[:3]:\n            print(f\"  {topic}: {kg.number_of_nodes()} nodes, {kg.number_of_edges()} edges\")\n        \n        return self.concept_kgs\n\n# Build concept KGs\nkg_builder = ConceptKGBuilder()\nconcept_kgs = kg_builder.build_all_concept_kgs(concept_keywords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:20:31.110877Z","iopub.execute_input":"2025-10-28T15:20:31.111311Z","iopub.status.idle":"2025-10-28T15:20:31.143034Z","shell.execute_reply.started":"2025-10-28T15:20:31.111289Z","shell.execute_reply":"2025-10-28T15:20:31.142054Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_182/1619011266.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Build concept KGs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mkg_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConceptKGBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mconcept_kgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkg_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_all_concept_kgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'concept_keywords' is not defined"],"ename":"NameError","evalue":"name 'concept_keywords' is not defined","output_type":"error"}],"execution_count":64},{"cell_type":"code","source":"# ============================================================================\n# VISUALIZE CONCEPT KG EDGES\n# ============================================================================\n\n# Pick one concept KG to inspect\nconcept_name = 'addiction'\nG = concept_kgs[concept_name]\n\nprint(f\"\\n{'='*80}\")\nprint(f\"EDGES IN '{concept_name.upper()}' KNOWLEDGE GRAPH\")\nprint(f\"{'='*80}\")\nprint(f\"Total nodes: {G.number_of_nodes()}\")\nprint(f\"Total edges: {G.number_of_edges()}\\n\")\n\n# Show all edges with their relationship types\nedges_list = list(G.edges(data=True))\n\nprint(\"TOPIC → KEYWORD EDGES:\")\nprint(\"-\" * 80)\ntopic_edges = [e for e in edges_list if e[2].get('relation') == 'has_keyword']\nfor source, target, data in topic_edges[:5]:  # Show first 5\n    print(f\"  {source} --[{data['relation']}]--> {target}\")\nprint(f\"  ... ({len(topic_edges)} total topic→keyword edges)\\n\")\n\nprint(\"KEYWORD ↔ KEYWORD EDGES (Co-occurrence):\")\nprint(\"-\" * 80)\nkeyword_edges = [e for e in edges_list if e[2].get('relation') == 'co_occurs']\nfor source, target, data in keyword_edges[:10]:  # Show first 10\n    print(f\"  {source} --[{data['relation']}]--> {target}\")\nprint(f\"  ... ({len(keyword_edges)} total keyword↔keyword edges)\")\n\n# Show the nodes\nprint(f\"\\n{'='*80}\")\nprint(\"ALL NODES:\")\nprint(\"-\" * 80)\nfor node, data in G.nodes(data=True):\n    print(f\"  {node} (type: {data.get('node_type', 'unknown')})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: GRAPH EMBEDDER\n# ============================================================================\n\nclass GraphEmbedder:\n    \"\"\"Embed graphs using Graph2Vec.\"\"\"\n    \n    def __init__(self, dimensions=128, wl_iterations=3):\n        \"\"\"\n        Args:\n            dimensions: Embedding dimension\n            wl_iterations: Weisfeiler-Lehman iterations\n        \"\"\"\n        self.dimensions = dimensions\n        self.model = Graph2Vec(\n            dimensions=dimensions,\n            wl_iterations=wl_iterations,\n            epochs=100,\n            min_count=1\n        )\n        print(f\"\\n✓ Initialized Graph2Vec (dim={dimensions}, WL={wl_iterations})\")\n    \n    def embed_graphs(self, graphs: List[nx.Graph]) -> np.ndarray:\n        \"\"\"\n        Embed multiple graphs.\n        \n        Args:\n            graphs: List of NetworkX graphs\n        \n        Returns:\n            Embedding matrix (n_graphs × dimensions)\n        \"\"\"\n        # Convert to undirected (Graph2Vec requires undirected)\n        undirected = [G.to_undirected() if G.is_directed() else G for G in graphs]\n        \n        # Fit and get embeddings\n        self.model.fit(undirected)\n        embeddings = self.model.get_embedding()\n        \n        return embeddings\n    \n    def embed_single_graph(self, G: nx.Graph, reference_graphs: List[nx.Graph]) -> np.ndarray:\n        \"\"\"\n        Embed a single new graph using reference graphs.\n        \n        Args:\n            G: Graph to embed\n            reference_graphs: Graphs to fit model on\n        \n        Returns:\n            Single embedding vector\n        \"\"\"\n        # Combine reference graphs with new graph\n        all_graphs = reference_graphs + [G]\n        embeddings = self.embed_graphs(all_graphs)\n        \n        # Return embedding of the last graph (the new one)\n        return embeddings[-1]\n\nprint(\"✓ GraphEmbedder class defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: GRAPH-BASED TOPIC CLASSIFIER\n# ============================================================================\n\nclass GraphTopicClassifier:\n    \"\"\"Classify articles using graph embeddings.\"\"\"\n    \n    def __init__(self, \n                 concept_kgs: Dict[str, nx.DiGraph],\n                 embedding_dim: int = 128,\n                 threshold: float = 0.3):\n        \"\"\"\n        Initialize classifier.\n        \n        Args:\n            concept_kgs: Dictionary of concept KGs\n            embedding_dim: Embedding dimension\n            threshold: Similarity threshold for topic assignment\n        \"\"\"\n        self.concept_kgs = concept_kgs\n        self.threshold = threshold\n        \n        print(f\"\\n{'='*80}\")\n        print(\"INITIALIZING GRAPH-BASED CLASSIFIER\")\n        print(f\"{'='*80}\")\n        \n        # Initialize embedder\n        self.embedder = GraphEmbedder(dimensions=embedding_dim)\n        \n        # Prepare concept graphs\n        self.concept_names = list(concept_kgs.keys())\n        concept_graphs = [concept_kgs[name] for name in self.concept_names]\n        \n        # Embed all concept graphs\n        print(\"\\nEmbedding concept knowledge graphs...\")\n        self.concept_embeddings = self.embedder.embed_graphs(concept_graphs)\n        \n        print(f\"✓ Embedded {len(self.concept_names)} concept graphs\")\n        print(f\"  Embedding shape: {self.concept_embeddings.shape}\")\n        print(f\"  Similarity threshold: {threshold}\")\n        \n        # Store as dictionary\n        self.concept_embedding_dict = {\n            name: self.concept_embeddings[i] \n            for i, name in enumerate(self.concept_names)\n        }\n    \n    def _dict_to_graph(self, kg_dict: Dict) -> nx.DiGraph:\n        \"\"\"Convert KG dictionary to NetworkX graph.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add nodes from entities\n        entities = kg_dict.get('entities', [])\n        for entity in entities:\n            G.add_node(entity)\n        \n        # Add edges from relations\n        relations = kg_dict.get('relations', [])\n        for relation in relations:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                G.add_edge(source, target, relation=rel_type)\n        \n        return G\n    \n    def classify_article(self, article_kg: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        # Convert to graph\n        G_article = self._dict_to_graph(article_kg)\n        \n        # Skip if empty\n        if G_article.number_of_nodes() == 0:\n            return {\n                'predicted_topics': [],\n                'similarities': {},\n                'max_similarity': 0.0\n            }\n        \n        # Embed article graph\n        reference_graphs = [self.concept_kgs[name] for name in self.concept_names]\n        article_embedding = self.embedder.embed_single_graph(\n            G_article, \n            reference_graphs=reference_graphs\n        )\n        \n        # Compute similarities with all concepts\n        similarities = {}\n        for topic_name in self.concept_names:\n            concept_emb = self.concept_embedding_dict[topic_name]\n            sim = cosine_similarity([article_embedding], [concept_emb])[0][0]\n            similarities[topic_name] = float(sim)\n        \n        # Assign topics above threshold\n        predicted_topics = [\n            topic for topic, sim in similarities.items() \n            if sim >= self.threshold\n        ]\n        \n        # Sort by similarity\n        predicted_topics = sorted(\n            predicted_topics, \n            key=lambda t: similarities[t], \n            reverse=True\n        )\n        \n        return {\n            'predicted_topics': predicted_topics,\n            'similarities': similarities,\n            'max_similarity': max(similarities.values()) if similarities else 0.0\n        }\n    \n    def classify_dataset(self, \n                        articles: List[Dict],\n                        knowledge_graphs: List[Dict]) -> List[Dict]:\n        \"\"\"Classify all articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(knowledge_graphs)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx in range(len(knowledge_graphs)):\n            if idx % 20 == 0:\n                print(f\"Processing articles {idx}-{min(idx+20, len(knowledge_graphs))}...\")\n            \n            article = articles[idx] if idx < len(articles) else {}\n            article_kg = kg_dict.get(idx, {'entities': [], 'relations': []})\n            \n            # Skip if no KG\n            if article_kg.get('num_edges', 0) == 0:\n                results.append({\n                    'article_id': idx,\n                    'predicted_topics': [],\n                    'similarities': {},\n                    'max_similarity': 0.0,\n                    'true_concepts': article.get('Concept', [])\n                })\n                continue\n            \n            # Classify\n            classification = self.classify_article(article_kg)\n            \n            result = {\n                'article_id': idx,\n                'predicted_topics': classification['predicted_topics'],\n                'similarities': classification['similarities'],\n                'max_similarity': classification['max_similarity'],\n                'true_concepts': article.get('Concept', [])\n            }\n            \n            results.append(result)\n        \n        print(\"\\n✓ Classification complete!\")\n        return results\n\nprint(\"✓ GraphTopicClassifier class defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: GRAPH-BASED TOPIC CLASSIFIER\n# ============================================================================\n\nclass GraphTopicClassifier:\n    \"\"\"Classify articles using graph embeddings.\"\"\"\n    \n    def __init__(self, \n                 concept_kgs: Dict[str, nx.DiGraph],\n                 embedding_dim: int = 128,\n                 threshold: float = 0.3):\n        \"\"\"\n        Initialize classifier.\n        \n        Args:\n            concept_kgs: Dictionary of concept KGs\n            embedding_dim: Embedding dimension\n            threshold: Similarity threshold for topic assignment\n        \"\"\"\n        self.concept_kgs = concept_kgs\n        self.threshold = threshold\n        \n        print(f\"\\n{'='*80}\")\n        print(\"INITIALIZING GRAPH-BASED CLASSIFIER\")\n        print(f\"{'='*80}\")\n        \n        # Initialize embedder\n        self.embedder = GraphEmbedder(dimensions=embedding_dim)\n        \n        # Prepare concept graphs\n        self.concept_names = list(concept_kgs.keys())\n        concept_graphs = [concept_kgs[name] for name in self.concept_names]\n        \n        # Embed all concept graphs\n        print(\"\\nEmbedding concept knowledge graphs...\")\n        self.concept_embeddings = self.embedder.embed_graphs(concept_graphs)\n        \n        print(f\"✓ Embedded {len(self.concept_names)} concept graphs\")\n        print(f\"  Embedding shape: {self.concept_embeddings.shape}\")\n        print(f\"  Similarity threshold: {threshold}\")\n        \n        # Store as dictionary\n        self.concept_embedding_dict = {\n            name: self.concept_embeddings[i] \n            for i, name in enumerate(self.concept_names)\n        }\n    \n    def _dict_to_graph(self, kg_dict: Dict) -> nx.DiGraph:\n        \"\"\"Convert KG dictionary to NetworkX graph.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add nodes from entities\n        entities = kg_dict.get('entities', [])\n        for entity in entities:\n            G.add_node(entity)\n        \n        # Add edges from relations\n        relations = kg_dict.get('relations', [])\n        for relation in relations:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                G.add_edge(source, target, relation=rel_type)\n        \n        return G\n    \n    def classify_article(self, article_kg: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        # Convert to graph\n        G_article = self._dict_to_graph(article_kg)\n        \n        # Skip if empty\n        if G_article.number_of_nodes() == 0:\n            return {\n                'predicted_topics': [],\n                'similarities': {},\n                'max_similarity': 0.0\n            }\n        \n        # Embed article graph\n        reference_graphs = [self.concept_kgs[name] for name in self.concept_names]\n        article_embedding = self.embedder.embed_single_graph(\n            G_article, \n            reference_graphs=reference_graphs\n        )\n        \n        # Compute similarities with all concepts\n        similarities = {}\n        for topic_name in self.concept_names:\n            concept_emb = self.concept_embedding_dict[topic_name]\n            sim = cosine_similarity([article_embedding], [concept_emb])[0][0]\n            similarities[topic_name] = float(sim)\n        \n        # Assign topics above threshold\n        predicted_topics = [\n            topic for topic, sim in similarities.items() \n            if sim >= self.threshold\n        ]\n        \n        # Sort by similarity\n        predicted_topics = sorted(\n            predicted_topics, \n            key=lambda t: similarities[t], \n            reverse=True\n        )\n        \n        return {\n            'predicted_topics': predicted_topics,\n            'similarities': similarities,\n            'max_similarity': max(similarities.values()) if similarities else 0.0\n        }\n    \n    def classify_dataset(self, \n                        articles: List[Dict],\n                        knowledge_graphs: List[Dict]) -> List[Dict]:\n        \"\"\"Classify all articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(knowledge_graphs)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx in range(len(knowledge_graphs)):\n            if idx % 20 == 0:\n                print(f\"Processing articles {idx}-{min(idx+20, len(knowledge_graphs))}...\")\n            \n            article = articles[idx] if idx < len(articles) else {}\n            article_kg = kg_dict.get(idx, {'entities': [], 'relations': []})\n            \n            # Skip if no KG\n            if article_kg.get('num_edges', 0) == 0:\n                results.append({\n                    'article_id': idx,\n                    'predicted_topics': [],\n                    'similarities': {},\n                    'max_similarity': 0.0,\n                    'true_concepts': article.get('Concept', [])\n                })\n                continue\n            \n            # Classify\n            classification = self.classify_article(article_kg)\n            \n            result = {\n                'article_id': idx,\n                'predicted_topics': classification['predicted_topics'],\n                'similarities': classification['similarities'],\n                'max_similarity': classification['max_similarity'],\n                'true_concepts': article.get('Concept', [])\n            }\n            \n            results.append(result)\n        \n        print(\"\\n✓ Classification complete!\")\n        return results\n\nprint(\"✓ GraphTopicClassifier class defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First, uninstall the problematic packages\n!pip uninstall -y node2vec gensim smart-open\n\n# Then reinstall with compatible versions\n!pip install gensim==4.3.2 smart-open==6.4.0 --break-system-packages\n!pip install node2vec==0.4.6 --break-system-packages","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scipy==1.13.1 --force-reinstall --break-system-packages","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install numpy==1.26.4 --force-reinstall --break-system-packages","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy, scipy, gensim\nprint(\"NumPy:\", numpy.__version__)\nprint(\"SciPy:\", scipy.__version__)\nprint(\"Gensim:\", gensim.__version__)\n\nfrom node2vec import Node2Vec","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scipy==1.12.0 --force-reinstall --break-system-packages\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# ALTERNATIVE: GRAPH EMBEDDING WITH NODE2VEC\n# ============================================================================\n\nimport json\nimport numpy as np\nimport networkx as nx\nfrom typing import List, Dict\nfrom node2vec import Node2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\n\nprint(\"✓ Imports complete\\n\")\n\n# Alternative GraphEmbedder using Node2Vec\nclass GraphEmbedder:\n    \"\"\"Embed graphs using Node2Vec.\"\"\"\n    \n    def __init__(self, dimensions=128, walk_length=30, num_walks=200):\n        self.dimensions = dimensions\n        self.walk_length = walk_length\n        self.num_walks = num_walks\n        print(f\"✓ Initialized Node2Vec embedder (dim={dimensions})\")\n    \n    def embed_graph(self, G: nx.Graph) -> np.ndarray:\n        \"\"\"Embed a single graph.\"\"\"\n        # Convert to undirected\n        G_undirected = G.to_undirected() if G.is_directed() else G\n        \n        # Skip if too small\n        if G_undirected.number_of_nodes() < 2:\n            return np.zeros(self.dimensions)\n        \n        # Fit Node2Vec\n        node2vec = Node2Vec(\n            G_undirected, \n            dimensions=self.dimensions,\n            walk_length=self.walk_length,\n            num_walks=self.num_walks,\n            workers=1,\n            quiet=True\n        )\n        \n        model = node2vec.fit(window=10, min_count=1, batch_words=4)\n        \n        # Get embeddings for all nodes\n        node_embeddings = []\n        for node in G_undirected.nodes():\n            node_embeddings.append(model.wv[str(node)])\n        \n        # Average all node embeddings to get graph embedding\n        graph_embedding = np.mean(node_embeddings, axis=0)\n        return graph_embedding\n    \n    def embed_graphs(self, graphs: List[nx.Graph]) -> np.ndarray:\n        \"\"\"Embed multiple graphs.\"\"\"\n        embeddings = []\n        for i, G in enumerate(graphs):\n            if i % 5 == 0:\n                print(f\"  Embedding graph {i+1}/{len(graphs)}...\")\n            emb = self.embed_graph(G)\n            embeddings.append(emb)\n        return np.array(embeddings)\n    \n    def embed_single_graph(self, G: nx.Graph, reference_graphs: List[nx.Graph] = None) -> np.ndarray:\n        \"\"\"Embed a single graph.\"\"\"\n        return self.embed_graph(G)\n\nprint(\"✓ GraphEmbedder defined\\n\")\n\n# Now initialize the classifier\ngraph_classifier = GraphTopicClassifier(\n    concept_kgs=concept_kgs,\n    embedding_dim=128,\n    threshold=0.3\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}