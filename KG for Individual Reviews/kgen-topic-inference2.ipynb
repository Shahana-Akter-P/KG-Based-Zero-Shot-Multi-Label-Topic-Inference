{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13302893,"sourceType":"datasetVersion","datasetId":8432087}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:43:01.925332Z","iopub.execute_input":"2025-10-14T18:43:01.925872Z","iopub.status.idle":"2025-10-14T18:43:02.186385Z","shell.execute_reply.started":"2025-10-14T18:43:01.925851Z","shell.execute_reply":"2025-10-14T18:43:02.185792Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/zsltikg/NewsConcept Data-set.json\n/kaggle/input/zsltikg/DVD playerData-set.json\n/kaggle/input/zsltikg/MedicalConcept Data-set.json\n/kaggle/input/zsltikg/Cellular phone Data-set.json\n/kaggle/input/zsltikg/Digital camera2 Data-set.json\n/kaggle/input/zsltikg/Mp3 playerData-set.json\n/kaggle/input/zsltikg/Digital camera1 Data-set.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# PART 1: INSTALLATION AND SETUP\n# ============================================================================\n\n# Install required packages\n!pip install dspy-ai huggingface_hub networkx sentence-transformers pandas --quiet\n\nprint(\"✓ Packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:43:02.187659Z","iopub.execute_input":"2025-10-14T18:43:02.188031Z","iopub.status.idle":"2025-10-14T18:44:28.706177Z","shell.execute_reply.started":"2025-10-14T18:43:02.188013Z","shell.execute_reply":"2025-10-14T18:44:28.705334Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.8/999.8 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.2/272.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✓ Packages installed successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# PART 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom collections import defaultdict\nimport pandas as pd\nimport os\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:44:28.707318Z","iopub.execute_input":"2025-10-14T18:44:28.707584Z","iopub.status.idle":"2025-10-14T18:44:59.716841Z","shell.execute_reply.started":"2025-10-14T18:44:28.707560Z","shell.execute_reply":"2025-10-14T18:44:59.716150Z"}},"outputs":[{"name":"stderr","text":"2025-10-14 18:44:46.576084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760467486.769893      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760467486.820714      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**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**","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: INSTALL AND IMPORT (if not done already)\n# ============================================================================\n\nimport os\nfrom huggingface_hub import InferenceClient\nimport dspy\nimport json\nfrom typing import List, Tuple, Dict\n\nprint(\"✓ Imports complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:44:59.718353Z","iopub.execute_input":"2025-10-14T18:44:59.718974Z","iopub.status.idle":"2025-10-14T18:44:59.790241Z","shell.execute_reply.started":"2025-10-14T18:44:59.718954Z","shell.execute_reply":"2025-10-14T18:44:59.789682Z"}},"outputs":[{"name":"stdout","text":"✓ Imports complete\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: CONFIGURE HUGGING FACE (COMPLETE FIX)\n# ============================================================================\n\nos.environ['HUGGINGFACE_API_KEY'] = 'KEY'  \n\nclient = InferenceClient(\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    token=os.environ['HUGGINGFACE_API_KEY']\n)\n\nclass HFLlama(dspy.LM):\n    def __init__(self, model, client, max_tokens=2000):\n        super().__init__(model=model)\n        self.client = client\n        self.max_tokens = max_tokens\n        self.history = []\n    \n    def __call__(self, prompt=None, messages=None, **kwargs):\n        \"\"\"Handle both prompt and messages format\"\"\"\n        \n        # Handle messages format (from DSPy)\n        if messages is not None:\n            if isinstance(messages, list):\n                conversation = []\n                for msg in messages:\n                    if isinstance(msg, dict):\n                        conversation.append(msg)\n                    else:\n                        conversation.append({\"role\": \"user\", \"content\": str(msg)})\n            else:\n                conversation = [{\"role\": \"user\", \"content\": str(messages)}]\n        \n        # Handle prompt format\n        elif prompt is not None:\n            conversation = [{\"role\": \"user\", \"content\": prompt}]\n        else:\n            raise ValueError(\"Either prompt or messages must be provided\")\n        \n        # Call Hugging Face API\n        response = self.client.chat_completion(\n            messages=conversation,\n            max_tokens=kwargs.get('max_tokens', self.max_tokens),\n            temperature=kwargs.get('temperature', 0.3)\n        )\n        \n        content = response.choices[0].message.content\n        return [content]\n    \n    def basic_request(self, prompt, **kwargs):\n        return self.__call__(prompt=prompt, **kwargs)\n\n# Initialize and configure\nlm = HFLlama(\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    client=client,\n    max_tokens=2000\n)\n\ndspy.settings.configure(lm=lm)\n\nprint(\"✓ Llama-3.1-8B-Instruct configured successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:46:12.095990Z","iopub.execute_input":"2025-10-14T18:46:12.096479Z","iopub.status.idle":"2025-10-14T18:46:12.104733Z","shell.execute_reply.started":"2025-10-14T18:46:12.096456Z","shell.execute_reply":"2025-10-14T18:46:12.103922Z"}},"outputs":[{"name":"stdout","text":"✓ Llama-3.1-8B-Instruct configured successfully!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ============================================================================\n# PART 1: DATA LOADING FUNCTION\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"\n    Load JSON data from file\n    \n    Expected format:\n    [\n        {\n            \"Article Title\": [],\n            \"Article Text\": \"text here...\",\n            \"Concept\": [\"concept1\", \"concept2\"]\n        },\n        ...\n    ]\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\n# Test with sample data\nsample_data = [\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"excellent phone, excellent service.\",\n        \"Concept\": []\n    },\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"i am a business user who heavily depend on mobile service.\",\n        \"Concept\": [\"service\"]\n    }\n]\n\nprint(\"✓ Sample data ready for testing\")\nprint(f\"  Sample has {len(sample_data)} documents\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.601408Z","iopub.execute_input":"2025-10-14T18:45:00.601695Z","iopub.status.idle":"2025-10-14T18:45:00.621105Z","shell.execute_reply.started":"2025-10-14T18:45:00.601670Z","shell.execute_reply":"2025-10-14T18:45:00.620560Z"}},"outputs":[{"name":"stdout","text":"✓ Sample data ready for testing\n  Sample has 2 documents\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# FIND THE DATASET FILES\n# ============================================================================\n\nimport os\n\n# List files in your dataset directory\ndataset_path = '/kaggle/input/zsltikg'\n\nprint(\"Files in the dataset:\")\nprint(\"=\" * 60)\n\ntry:\n    files = os.listdir(dataset_path)\n    for i, file in enumerate(files, 1):\n        print(f\"{i}. {file}\")\n        full_path = os.path.join(dataset_path, file)\n        print(f\"   Full path: {full_path}\")\n        print()\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    print(\"\\nTrying to list all datasets...\")\n    for dataset in os.listdir('/kaggle/input/'):\n        print(f\"- {dataset}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.622218Z","iopub.execute_input":"2025-10-14T18:45:00.622673Z","iopub.status.idle":"2025-10-14T18:45:00.638556Z","shell.execute_reply.started":"2025-10-14T18:45:00.622656Z","shell.execute_reply":"2025-10-14T18:45:00.637830Z"}},"outputs":[{"name":"stdout","text":"Files in the dataset:\n============================================================\n1. NewsConcept Data-set.json\n   Full path: /kaggle/input/zsltikg/NewsConcept Data-set.json\n\n2. DVD playerData-set.json\n   Full path: /kaggle/input/zsltikg/DVD playerData-set.json\n\n3. MedicalConcept Data-set.json\n   Full path: /kaggle/input/zsltikg/MedicalConcept Data-set.json\n\n4. Cellular phone Data-set.json\n   Full path: /kaggle/input/zsltikg/Cellular phone Data-set.json\n\n5. Digital camera2 Data-set.json\n   Full path: /kaggle/input/zsltikg/Digital camera2 Data-set.json\n\n6. Mp3 playerData-set.json\n   Full path: /kaggle/input/zsltikg/Mp3 playerData-set.json\n\n7. Digital camera1 Data-set.json\n   Full path: /kaggle/input/zsltikg/Digital camera1 Data-set.json\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# LOAD ONE SPECIFIC DATASET\n# ============================================================================\n\n# Choose which dataset you want to work with:\n# Option 1: Cellular phone\nfilepath = '/kaggle/input/zsltikg/Cellular phone Data-set.json'\n\n# Option 2: News\n# filepath = '/kaggle/input/zsltikg/NewsConcept Data-set.json'\n\n# Option 3: Medical\n# filepath = '/kaggle/input/zsltikg/MedicalConcept Data-set.json'\n\n# Load the data\ndata = load_json_data(filepath)\n\nprint(f\"✓ Loaded {len(data)} documents\")\nprint(f\"\\nFirst document preview:\")\nprint(f\"  Keys: {list(data[0].keys())}\")\nprint(f\"  Text: {data[0].get('Article Text', '')[:100]}...\")\nprint(f\"  Concepts: {data[0].get('Concept', [])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.639294Z","iopub.execute_input":"2025-10-14T18:45:00.639481Z","iopub.status.idle":"2025-10-14T18:45:00.660222Z","shell.execute_reply.started":"2025-10-14T18:45:00.639468Z","shell.execute_reply":"2025-10-14T18:45:00.659660Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 587 documents from /kaggle/input/zsltikg/Cellular phone Data-set.json\n✓ Loaded 587 documents\n\nFirst document preview:\n  Keys: ['Article Title', 'Article Text', 'Concept']\n  Text: excellent phone , excellent service . \n...\n  Concepts: []\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# RELOAD DATA PROPERLY\n# ============================================================================\n\nimport json\n\nprint(\"=\" * 80)\nprint(\"RELOADING DATA PROPERLY\")\nprint(\"=\" * 80)\n\n# Load the JSON file correctly\nwith open('/kaggle/input/zsltikg/Cellular phone Data-set.json', 'r') as f:\n    raw_data = json.load(f)\n\nprint(f\"Type after loading: {type(raw_data)}\")\n\n# Check if it's a dict with a key\nif isinstance(raw_data, dict):\n    print(f\"Keys: {raw_data.keys()}\")\n    # Get the actual data\n    for key in raw_data.keys():\n        if isinstance(raw_data[key], list):\n            data = raw_data[key]\n            print(f\"Found list under key '{key}' with {len(data)} items\")\n            break\nelif isinstance(raw_data, list):\n    data = raw_data\n    print(f\"Data is a list with {len(data)} items\")\n\n# Verify first item\nprint(f\"\\nFirst item type: {type(data[0])}\")\nprint(f\"First item: {data[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.660899Z","iopub.execute_input":"2025-10-14T18:45:00.661345Z","iopub.status.idle":"2025-10-14T18:45:00.668452Z","shell.execute_reply.started":"2025-10-14T18:45:00.661330Z","shell.execute_reply":"2025-10-14T18:45:00.667662Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nRELOADING DATA PROPERLY\n================================================================================\nType after loading: <class 'list'>\nData is a list with 587 items\n\nFirst item type: <class 'dict'>\nFirst item: {'Article Title': [], 'Article Text': 'excellent phone , excellent service . \\n', 'Concept': []}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# PROCESS YOUR 587 ARTICLES (NOW CORRECT!)\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"PROCESSING YOUR 587 ARTICLES\")\nprint(\"=\" * 80)\n\n# data is now correctly loaded as a list of dicts\nindividual_articles = []\n\nfor idx, item in enumerate(data):\n    article = {\n        'id': idx,\n        'Article Text': item['Article Text'],\n        'Concept': item['Concept'] if item['Concept'] else []\n    }\n    individual_articles.append(article)\n\nprint(f\"\\n✓ Created list of {len(individual_articles)} individual articles\")\n\n# Show first 3\nprint(f\"\\nFirst 3 articles:\")\nprint(\"-\" * 80)\nfor i in range(min(3, len(individual_articles))):\n    article = individual_articles[i]\n    print(f\"\\nArticle {article['id']}:\")\n    print(f\"  Text: {article['Article Text'][:80]}...\")\n    print(f\"  Concepts: {article['Concept']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.670682Z","iopub.execute_input":"2025-10-14T18:45:00.670871Z","iopub.status.idle":"2025-10-14T18:45:00.686378Z","shell.execute_reply.started":"2025-10-14T18:45:00.670857Z","shell.execute_reply":"2025-10-14T18:45:00.685716Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPROCESSING YOUR 587 ARTICLES\n================================================================================\n\n✓ Created list of 587 individual articles\n\nFirst 3 articles:\n--------------------------------------------------------------------------------\n\nArticle 0:\n  Text: excellent phone , excellent service . \n...\n  Concepts: []\n\nArticle 1:\n  Text: i am a business user who heavily depend on mobile service . \n...\n  Concepts: ['service']\n\nArticle 2:\n  Text: there is much which has been said in other reviews about the features of this ph...\n  Concepts: ['phone']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# GENERATE KG FOR ALL 587 ARTICLES\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GENERATING KNOWLEDGE GRAPHS FOR ALL 587 ARTICLES\")\nprint(\"=\" * 80)\n\nall_article_kgs = []\n\ntotal = len(individual_articles)\nprint(f\"\\nProcessing {total} articles...\\n\")\n\nfor idx, article in enumerate(individual_articles):\n    article_id = article['id']\n    text = article['Article Text']\n    concepts = article['Concept']\n    \n    # Skip empty articles\n    if not text or len(text.strip()) < 5:\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n        continue\n    \n    try:\n        # Extract entities for THIS article\n        entities = entity_extractor(text)\n        \n        # Add original concepts as entities\n        for concept in concepts:\n            if concept and isinstance(concept, str):\n                entities.append(concept.lower().strip())\n        \n        entities = list(set(entities))  # Remove duplicates\n        \n        # Extract relations for THIS article\n        if entities:\n            relations = relation_extractor(text, entities)\n        else:\n            relations = []\n        \n        # Build graph for THIS article\n        graph = nx.DiGraph()\n        for subj, pred, obj in relations:\n            graph.add_edge(subj, obj, relation=pred)\n        \n        # Store everything for this article\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            'concepts': concepts,\n            'graph': graph,\n            'entities': entities,\n            'relations': relations,\n            'num_nodes': len(graph.nodes()),\n            'num_edges': len(graph.edges())\n        })\n        \n        # Progress update\n        if (idx + 1) % 50 == 0:\n            print(f\"✓ Processed {idx + 1}/{total} articles...\")\n        \n    except Exception as e:\n        print(f\"✗ Article {article_id}: Error - {str(e)[:100]}\")\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n\nprint(f\"\\n\" + \"=\" * 80)\nprint(\"✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\")\nprint(\"=\" * 80)\nprint(f\"Total articles processed: {len(all_article_kgs)}\")\nprint(f\"Articles with graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] > 0)}\")\nprint(f\"Articles without graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] == 0)}\")\n\n# Statistics\ntotal_entities = sum(len(kg['entities']) for kg in all_article_kgs)\ntotal_relations = sum(len(kg['relations']) for kg in all_article_kgs)\n\nprint(f\"\\nTotal entities extracted: {total_entities}\")\nprint(f\"Total relations extracted: {total_relations}\")\n\nwith_graphs = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\nif with_graphs:\n    avg_nodes = sum(kg['num_nodes'] for kg in with_graphs) / len(with_graphs)\n    avg_edges = sum(kg['num_edges'] for kg in with_graphs) / len(with_graphs)\n    print(f\"\\nAverage nodes per KG: {avg_nodes:.2f}\")\n    print(f\"Average edges per KG: {avg_edges:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.687287Z","iopub.execute_input":"2025-10-14T18:45:00.687535Z","iopub.status.idle":"2025-10-14T18:45:00.712085Z","shell.execute_reply.started":"2025-10-14T18:45:00.687496Z","shell.execute_reply":"2025-10-14T18:45:00.711335Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nGENERATING KNOWLEDGE GRAPHS FOR ALL 587 ARTICLES\n================================================================================\n\nProcessing 587 articles...\n\n✗ Article 0: Error - name 'entity_extractor' is not defined\n✗ Article 1: Error - name 'entity_extractor' is not defined\n✗ Article 2: Error - name 'entity_extractor' is not defined\n✗ Article 3: Error - name 'entity_extractor' is not defined\n✗ Article 4: Error - name 'entity_extractor' is not defined\n✗ Article 5: Error - name 'entity_extractor' is not defined\n✗ Article 6: Error - name 'entity_extractor' is not defined\n✗ Article 7: Error - name 'entity_extractor' is not defined\n✗ Article 8: Error - name 'entity_extractor' is not defined\n✗ Article 9: Error - name 'entity_extractor' is not defined\n✗ Article 10: Error - name 'entity_extractor' is not defined\n✗ Article 11: Error - name 'entity_extractor' is not defined\n✗ Article 12: Error - name 'entity_extractor' is not defined\n✗ Article 13: Error - name 'entity_extractor' is not defined\n✗ Article 14: Error - name 'entity_extractor' is not defined\n✗ Article 15: Error - name 'entity_extractor' is not defined\n✗ Article 16: Error - name 'entity_extractor' is not defined\n✗ Article 17: Error - name 'entity_extractor' is not defined\n✗ Article 18: Error - name 'entity_extractor' is not defined\n✗ Article 19: Error - name 'entity_extractor' is not defined\n✗ Article 20: Error - name 'entity_extractor' is not defined\n✗ Article 21: Error - name 'entity_extractor' is not defined\n✗ Article 22: Error - name 'entity_extractor' is not defined\n✗ Article 23: Error - name 'entity_extractor' is not defined\n✗ Article 24: Error - name 'entity_extractor' is not defined\n✗ Article 25: Error - name 'entity_extractor' is not defined\n✗ Article 26: Error - name 'entity_extractor' is not defined\n✗ Article 27: Error - name 'entity_extractor' is not defined\n✗ Article 28: Error - name 'entity_extractor' is not defined\n✗ Article 29: Error - name 'entity_extractor' is not defined\n✗ Article 30: Error - name 'entity_extractor' is not defined\n✗ Article 31: Error - name 'entity_extractor' is not defined\n✗ Article 32: Error - name 'entity_extractor' is not defined\n✗ Article 33: Error - name 'entity_extractor' is not defined\n✗ Article 34: Error - name 'entity_extractor' is not defined\n✗ Article 35: Error - name 'entity_extractor' is not defined\n✗ Article 36: Error - name 'entity_extractor' is not defined\n✗ Article 37: Error - name 'entity_extractor' is not defined\n✗ Article 38: Error - name 'entity_extractor' is not defined\n✗ Article 39: Error - name 'entity_extractor' is not defined\n✗ Article 40: Error - name 'entity_extractor' is not defined\n✗ Article 41: Error - name 'entity_extractor' is not defined\n✗ Article 42: Error - name 'entity_extractor' is not defined\n✗ Article 43: Error - name 'entity_extractor' is not defined\n✗ Article 44: Error - name 'entity_extractor' is not defined\n✗ Article 45: Error - name 'entity_extractor' is not defined\n✗ Article 46: Error - name 'entity_extractor' is not defined\n✗ Article 47: Error - name 'entity_extractor' is not defined\n✗ Article 48: Error - name 'entity_extractor' is not defined\n✗ Article 49: Error - name 'entity_extractor' is not defined\n✗ Article 50: Error - name 'entity_extractor' is not defined\n✗ Article 51: Error - name 'entity_extractor' is not defined\n✗ Article 52: Error - name 'entity_extractor' is not defined\n✗ Article 53: Error - name 'entity_extractor' is not defined\n✗ Article 54: Error - name 'entity_extractor' is not defined\n✗ Article 55: Error - name 'entity_extractor' is not defined\n✗ Article 56: Error - name 'entity_extractor' is not defined\n✗ Article 57: Error - name 'entity_extractor' is not defined\n✗ Article 58: Error - name 'entity_extractor' is not defined\n✗ Article 59: Error - name 'entity_extractor' is not defined\n✗ Article 60: Error - name 'entity_extractor' is not defined\n✗ Article 61: Error - name 'entity_extractor' is not defined\n✗ Article 62: Error - name 'entity_extractor' is not defined\n✗ Article 63: Error - name 'entity_extractor' is not defined\n✗ Article 64: Error - name 'entity_extractor' is not defined\n✗ Article 65: Error - name 'entity_extractor' is not defined\n✗ Article 66: Error - name 'entity_extractor' is not defined\n✗ Article 67: Error - name 'entity_extractor' is not defined\n✗ Article 68: Error - name 'entity_extractor' is not defined\n✗ Article 69: Error - name 'entity_extractor' is not defined\n✗ Article 70: Error - name 'entity_extractor' is not defined\n✗ Article 71: Error - name 'entity_extractor' is not defined\n✗ Article 72: Error - name 'entity_extractor' is not defined\n✗ Article 73: Error - name 'entity_extractor' is not defined\n✗ Article 74: Error - name 'entity_extractor' is not defined\n✗ Article 75: Error - name 'entity_extractor' is not defined\n✗ Article 76: Error - name 'entity_extractor' is not defined\n✗ Article 77: Error - name 'entity_extractor' is not defined\n✗ Article 78: Error - name 'entity_extractor' is not defined\n✗ Article 79: Error - name 'entity_extractor' is not defined\n✗ Article 80: Error - name 'entity_extractor' is not defined\n✗ Article 81: Error - name 'entity_extractor' is not defined\n✗ Article 82: Error - name 'entity_extractor' is not defined\n✗ Article 83: Error - name 'entity_extractor' is not defined\n✗ Article 84: Error - name 'entity_extractor' is not defined\n✗ Article 85: Error - name 'entity_extractor' is not defined\n✗ Article 86: Error - name 'entity_extractor' is not defined\n✗ Article 87: Error - name 'entity_extractor' is not defined\n✗ Article 88: Error - name 'entity_extractor' is not defined\n✗ Article 89: Error - name 'entity_extractor' is not defined\n✗ Article 90: Error - name 'entity_extractor' is not defined\n✗ Article 91: Error - name 'entity_extractor' is not defined\n✗ Article 92: Error - name 'entity_extractor' is not defined\n✗ Article 93: Error - name 'entity_extractor' is not defined\n✗ Article 94: Error - name 'entity_extractor' is not defined\n✗ Article 95: Error - name 'entity_extractor' is not defined\n✗ Article 96: Error - name 'entity_extractor' is not defined\n✗ Article 97: Error - name 'entity_extractor' is not defined\n✗ Article 98: Error - name 'entity_extractor' is not defined\n✗ Article 99: Error - name 'entity_extractor' is not defined\n✗ Article 100: Error - name 'entity_extractor' is not defined\n✗ Article 101: Error - name 'entity_extractor' is not defined\n✗ Article 102: Error - name 'entity_extractor' is not defined\n✗ Article 103: Error - name 'entity_extractor' is not defined\n✗ Article 104: Error - name 'entity_extractor' is not defined\n✗ Article 105: Error - name 'entity_extractor' is not defined\n✗ Article 106: Error - name 'entity_extractor' is not defined\n✗ Article 107: Error - name 'entity_extractor' is not defined\n✗ Article 108: Error - name 'entity_extractor' is not defined\n✗ Article 109: Error - name 'entity_extractor' is not defined\n✗ Article 110: Error - name 'entity_extractor' is not defined\n✗ Article 111: Error - name 'entity_extractor' is not defined\n✗ Article 112: Error - name 'entity_extractor' is not defined\n✗ Article 113: Error - name 'entity_extractor' is not defined\n✗ Article 114: Error - name 'entity_extractor' is not defined\n✗ Article 115: Error - name 'entity_extractor' is not defined\n✗ Article 116: Error - name 'entity_extractor' is not defined\n✗ Article 117: Error - name 'entity_extractor' is not defined\n✗ Article 118: Error - name 'entity_extractor' is not defined\n✗ Article 119: Error - name 'entity_extractor' is not defined\n✗ Article 120: Error - name 'entity_extractor' is not defined\n✗ Article 121: Error - name 'entity_extractor' is not defined\n✗ Article 122: Error - name 'entity_extractor' is not defined\n✗ Article 123: Error - name 'entity_extractor' is not defined\n✗ Article 124: Error - name 'entity_extractor' is not defined\n✗ Article 125: Error - name 'entity_extractor' is not defined\n✗ Article 126: Error - name 'entity_extractor' is not defined\n✗ Article 127: Error - name 'entity_extractor' is not defined\n✗ Article 128: Error - name 'entity_extractor' is not defined\n✗ Article 129: Error - name 'entity_extractor' is not defined\n✗ Article 130: Error - name 'entity_extractor' is not defined\n✗ Article 131: Error - name 'entity_extractor' is not defined\n✗ Article 132: Error - name 'entity_extractor' is not defined\n✗ Article 133: Error - name 'entity_extractor' is not defined\n✗ Article 134: Error - name 'entity_extractor' is not defined\n✗ Article 135: Error - name 'entity_extractor' is not defined\n✗ Article 136: Error - name 'entity_extractor' is not defined\n✗ Article 137: Error - name 'entity_extractor' is not defined\n✗ Article 138: Error - name 'entity_extractor' is not defined\n✗ Article 139: Error - name 'entity_extractor' is not defined\n✗ Article 140: Error - name 'entity_extractor' is not defined\n✗ Article 141: Error - name 'entity_extractor' is not defined\n✗ Article 142: Error - name 'entity_extractor' is not defined\n✗ Article 143: Error - name 'entity_extractor' is not defined\n✗ Article 144: Error - name 'entity_extractor' is not defined\n✗ Article 145: Error - name 'entity_extractor' is not defined\n✗ Article 146: Error - name 'entity_extractor' is not defined\n✗ Article 147: Error - name 'entity_extractor' is not defined\n✗ Article 148: Error - name 'entity_extractor' is not defined\n✗ Article 149: Error - name 'entity_extractor' is not defined\n✗ Article 150: Error - name 'entity_extractor' is not defined\n✗ Article 151: Error - name 'entity_extractor' is not defined\n✗ Article 152: Error - name 'entity_extractor' is not defined\n✗ Article 153: Error - name 'entity_extractor' is not defined\n✗ Article 154: Error - name 'entity_extractor' is not defined\n✗ Article 155: Error - name 'entity_extractor' is not defined\n✗ Article 156: Error - name 'entity_extractor' is not defined\n✗ Article 157: Error - name 'entity_extractor' is not defined\n✗ Article 158: Error - name 'entity_extractor' is not defined\n✗ Article 159: Error - name 'entity_extractor' is not defined\n✗ Article 160: Error - name 'entity_extractor' is not defined\n✗ Article 161: Error - name 'entity_extractor' is not defined\n✗ Article 162: Error - name 'entity_extractor' is not defined\n✗ Article 163: Error - name 'entity_extractor' is not defined\n✗ Article 164: Error - name 'entity_extractor' is not defined\n✗ Article 165: Error - name 'entity_extractor' is not defined\n✗ Article 166: Error - name 'entity_extractor' is not defined\n✗ Article 167: Error - name 'entity_extractor' is not defined\n✗ Article 168: Error - name 'entity_extractor' is not defined\n✗ Article 169: Error - name 'entity_extractor' is not defined\n✗ Article 170: Error - name 'entity_extractor' is not defined\n✗ Article 171: Error - name 'entity_extractor' is not defined\n✗ Article 172: Error - name 'entity_extractor' is not defined\n✗ Article 173: Error - name 'entity_extractor' is not defined\n✗ Article 174: Error - name 'entity_extractor' is not defined\n✗ Article 175: Error - name 'entity_extractor' is not defined\n✗ Article 176: Error - name 'entity_extractor' is not defined\n✗ Article 177: Error - name 'entity_extractor' is not defined\n✗ Article 178: Error - name 'entity_extractor' is not defined\n✗ Article 179: Error - name 'entity_extractor' is not defined\n✗ Article 180: Error - name 'entity_extractor' is not defined\n✗ Article 181: Error - name 'entity_extractor' is not defined\n✗ Article 182: Error - name 'entity_extractor' is not defined\n✗ Article 183: Error - name 'entity_extractor' is not defined\n✗ Article 184: Error - name 'entity_extractor' is not defined\n✗ Article 185: Error - name 'entity_extractor' is not defined\n✗ Article 186: Error - name 'entity_extractor' is not defined\n✗ Article 187: Error - name 'entity_extractor' is not defined\n✗ Article 188: Error - name 'entity_extractor' is not defined\n✗ Article 189: Error - name 'entity_extractor' is not defined\n✗ Article 190: Error - name 'entity_extractor' is not defined\n✗ Article 191: Error - name 'entity_extractor' is not defined\n✗ Article 192: Error - name 'entity_extractor' is not defined\n✗ Article 193: Error - name 'entity_extractor' is not defined\n✗ Article 194: Error - name 'entity_extractor' is not defined\n✗ Article 195: Error - name 'entity_extractor' is not defined\n✗ Article 196: Error - name 'entity_extractor' is not defined\n✗ Article 197: Error - name 'entity_extractor' is not defined\n✗ Article 198: Error - name 'entity_extractor' is not defined\n✗ Article 199: Error - name 'entity_extractor' is not defined\n✗ Article 200: Error - name 'entity_extractor' is not defined\n✗ Article 201: Error - name 'entity_extractor' is not defined\n✗ Article 202: Error - name 'entity_extractor' is not defined\n✗ Article 203: Error - name 'entity_extractor' is not defined\n✗ Article 204: Error - name 'entity_extractor' is not defined\n✗ Article 205: Error - name 'entity_extractor' is not defined\n✗ Article 206: Error - name 'entity_extractor' is not defined\n✗ Article 207: Error - name 'entity_extractor' is not defined\n✗ Article 208: Error - name 'entity_extractor' is not defined\n✗ Article 209: Error - name 'entity_extractor' is not defined\n✗ Article 210: Error - name 'entity_extractor' is not defined\n✗ Article 211: Error - name 'entity_extractor' is not defined\n✗ Article 212: Error - name 'entity_extractor' is not defined\n✗ Article 213: Error - name 'entity_extractor' is not defined\n✗ Article 214: Error - name 'entity_extractor' is not defined\n✗ Article 215: Error - name 'entity_extractor' is not defined\n✗ Article 216: Error - name 'entity_extractor' is not defined\n✗ Article 217: Error - name 'entity_extractor' is not defined\n✗ Article 218: Error - name 'entity_extractor' is not defined\n✗ Article 219: Error - name 'entity_extractor' is not defined\n✗ Article 221: Error - name 'entity_extractor' is not defined\n✗ Article 222: Error - name 'entity_extractor' is not defined\n✗ Article 223: Error - name 'entity_extractor' is not defined\n✗ Article 224: Error - name 'entity_extractor' is not defined\n✗ Article 225: Error - name 'entity_extractor' is not defined\n✗ Article 226: Error - name 'entity_extractor' is not defined\n✗ Article 227: Error - name 'entity_extractor' is not defined\n✗ Article 228: Error - name 'entity_extractor' is not defined\n✗ Article 229: Error - name 'entity_extractor' is not defined\n✗ Article 230: Error - name 'entity_extractor' is not defined\n✗ Article 231: Error - name 'entity_extractor' is not defined\n✗ Article 232: Error - name 'entity_extractor' is not defined\n✗ Article 233: Error - name 'entity_extractor' is not defined\n✗ Article 234: Error - name 'entity_extractor' is not defined\n✗ Article 235: Error - name 'entity_extractor' is not defined\n✗ Article 236: Error - name 'entity_extractor' is not defined\n✗ Article 237: Error - name 'entity_extractor' is not defined\n✗ Article 238: Error - name 'entity_extractor' is not defined\n✗ Article 239: Error - name 'entity_extractor' is not defined\n✗ Article 240: Error - name 'entity_extractor' is not defined\n✗ Article 241: Error - name 'entity_extractor' is not defined\n✗ Article 242: Error - name 'entity_extractor' is not defined\n✗ Article 243: Error - name 'entity_extractor' is not defined\n✗ Article 244: Error - name 'entity_extractor' is not defined\n✗ Article 245: Error - name 'entity_extractor' is not defined\n✗ Article 246: Error - name 'entity_extractor' is not defined\n✗ Article 247: Error - name 'entity_extractor' is not defined\n✗ Article 248: Error - name 'entity_extractor' is not defined\n✗ Article 249: Error - name 'entity_extractor' is not defined\n✗ Article 250: Error - name 'entity_extractor' is not defined\n✗ Article 251: Error - name 'entity_extractor' is not defined\n✗ Article 252: Error - name 'entity_extractor' is not defined\n✗ Article 253: Error - name 'entity_extractor' is not defined\n✗ Article 254: Error - name 'entity_extractor' is not defined\n✗ Article 255: Error - name 'entity_extractor' is not defined\n✗ Article 256: Error - name 'entity_extractor' is not defined\n✗ Article 257: Error - name 'entity_extractor' is not defined\n✗ Article 258: Error - name 'entity_extractor' is not defined\n✗ Article 259: Error - name 'entity_extractor' is not defined\n✗ Article 260: Error - name 'entity_extractor' is not defined\n✗ Article 261: Error - name 'entity_extractor' is not defined\n✗ Article 262: Error - name 'entity_extractor' is not defined\n✗ Article 263: Error - name 'entity_extractor' is not defined\n✗ Article 264: Error - name 'entity_extractor' is not defined\n✗ Article 265: Error - name 'entity_extractor' is not defined\n✗ Article 266: Error - name 'entity_extractor' is not defined\n✗ Article 267: Error - name 'entity_extractor' is not defined\n✗ Article 268: Error - name 'entity_extractor' is not defined\n✗ Article 269: Error - name 'entity_extractor' is not defined\n✗ Article 270: Error - name 'entity_extractor' is not defined\n✗ Article 271: Error - name 'entity_extractor' is not defined\n✗ Article 272: Error - name 'entity_extractor' is not defined\n✗ Article 273: Error - name 'entity_extractor' is not defined\n✗ Article 274: Error - name 'entity_extractor' is not defined\n✗ Article 275: Error - name 'entity_extractor' is not defined\n✗ Article 276: Error - name 'entity_extractor' is not defined\n✗ Article 277: Error - name 'entity_extractor' is not defined\n✗ Article 278: Error - name 'entity_extractor' is not defined\n✗ Article 279: Error - name 'entity_extractor' is not defined\n✗ Article 280: Error - name 'entity_extractor' is not defined\n✗ Article 281: Error - name 'entity_extractor' is not defined\n✗ Article 282: Error - name 'entity_extractor' is not defined\n✗ Article 283: Error - name 'entity_extractor' is not defined\n✗ Article 284: Error - name 'entity_extractor' is not defined\n✗ Article 285: Error - name 'entity_extractor' is not defined\n✗ Article 286: Error - name 'entity_extractor' is not defined\n✗ Article 287: Error - name 'entity_extractor' is not defined\n✗ Article 288: Error - name 'entity_extractor' is not defined\n✗ Article 289: Error - name 'entity_extractor' is not defined\n✗ Article 290: Error - name 'entity_extractor' is not defined\n✗ Article 291: Error - name 'entity_extractor' is not defined\n✗ Article 292: Error - name 'entity_extractor' is not defined\n✗ Article 293: Error - name 'entity_extractor' is not defined\n✗ Article 294: Error - name 'entity_extractor' is not defined\n✗ Article 295: Error - name 'entity_extractor' is not defined\n✗ Article 296: Error - name 'entity_extractor' is not defined\n✗ Article 297: Error - name 'entity_extractor' is not defined\n✗ Article 298: Error - name 'entity_extractor' is not defined\n✗ Article 299: Error - name 'entity_extractor' is not defined\n✗ Article 300: Error - name 'entity_extractor' is not defined\n✗ Article 301: Error - name 'entity_extractor' is not defined\n✗ Article 302: Error - name 'entity_extractor' is not defined\n✗ Article 303: Error - name 'entity_extractor' is not defined\n✗ Article 304: Error - name 'entity_extractor' is not defined\n✗ Article 305: Error - name 'entity_extractor' is not defined\n✗ Article 306: Error - name 'entity_extractor' is not defined\n✗ Article 307: Error - name 'entity_extractor' is not defined\n✗ Article 308: Error - name 'entity_extractor' is not defined\n✗ Article 309: Error - name 'entity_extractor' is not defined\n✗ Article 310: Error - name 'entity_extractor' is not defined\n✗ Article 311: Error - name 'entity_extractor' is not defined\n✗ Article 312: Error - name 'entity_extractor' is not defined\n✗ Article 313: Error - name 'entity_extractor' is not defined\n✗ Article 314: Error - name 'entity_extractor' is not defined\n✗ Article 315: Error - name 'entity_extractor' is not defined\n✗ Article 316: Error - name 'entity_extractor' is not defined\n✗ Article 317: Error - name 'entity_extractor' is not defined\n✗ Article 318: Error - name 'entity_extractor' is not defined\n✗ Article 319: Error - name 'entity_extractor' is not defined\n✗ Article 320: Error - name 'entity_extractor' is not defined\n✗ Article 321: Error - name 'entity_extractor' is not defined\n✗ Article 322: Error - name 'entity_extractor' is not defined\n✗ Article 323: Error - name 'entity_extractor' is not defined\n✗ Article 324: Error - name 'entity_extractor' is not defined\n✗ Article 325: Error - name 'entity_extractor' is not defined\n✗ Article 326: Error - name 'entity_extractor' is not defined\n✗ Article 327: Error - name 'entity_extractor' is not defined\n✗ Article 328: Error - name 'entity_extractor' is not defined\n✗ Article 329: Error - name 'entity_extractor' is not defined\n✗ Article 330: Error - name 'entity_extractor' is not defined\n✗ Article 331: Error - name 'entity_extractor' is not defined\n✗ Article 332: Error - name 'entity_extractor' is not defined\n✗ Article 333: Error - name 'entity_extractor' is not defined\n✗ Article 334: Error - name 'entity_extractor' is not defined\n✗ Article 335: Error - name 'entity_extractor' is not defined\n✗ Article 336: Error - name 'entity_extractor' is not defined\n✗ Article 337: Error - name 'entity_extractor' is not defined\n✗ Article 338: Error - name 'entity_extractor' is not defined\n✗ Article 339: Error - name 'entity_extractor' is not defined\n✗ Article 340: Error - name 'entity_extractor' is not defined\n✗ Article 341: Error - name 'entity_extractor' is not defined\n✗ Article 342: Error - name 'entity_extractor' is not defined\n✗ Article 343: Error - name 'entity_extractor' is not defined\n✗ Article 344: Error - name 'entity_extractor' is not defined\n✗ Article 345: Error - name 'entity_extractor' is not defined\n✗ Article 346: Error - name 'entity_extractor' is not defined\n✗ Article 347: Error - name 'entity_extractor' is not defined\n✗ Article 348: Error - name 'entity_extractor' is not defined\n✗ Article 349: Error - name 'entity_extractor' is not defined\n✗ Article 350: Error - name 'entity_extractor' is not defined\n✗ Article 351: Error - name 'entity_extractor' is not defined\n✗ Article 352: Error - name 'entity_extractor' is not defined\n✗ Article 353: Error - name 'entity_extractor' is not defined\n✗ Article 354: Error - name 'entity_extractor' is not defined\n✗ Article 355: Error - name 'entity_extractor' is not defined\n✗ Article 356: Error - name 'entity_extractor' is not defined\n✗ Article 357: Error - name 'entity_extractor' is not defined\n✗ Article 358: Error - name 'entity_extractor' is not defined\n✗ Article 359: Error - name 'entity_extractor' is not defined\n✗ Article 360: Error - name 'entity_extractor' is not defined\n✗ Article 361: Error - name 'entity_extractor' is not defined\n✗ Article 362: Error - name 'entity_extractor' is not defined\n✗ Article 363: Error - name 'entity_extractor' is not defined\n✗ Article 364: Error - name 'entity_extractor' is not defined\n✗ Article 366: Error - name 'entity_extractor' is not defined\n✗ Article 367: Error - name 'entity_extractor' is not defined\n✗ Article 368: Error - name 'entity_extractor' is not defined\n✗ Article 369: Error - name 'entity_extractor' is not defined\n✗ Article 370: Error - name 'entity_extractor' is not defined\n✗ Article 371: Error - name 'entity_extractor' is not defined\n✗ Article 372: Error - name 'entity_extractor' is not defined\n✗ Article 373: Error - name 'entity_extractor' is not defined\n✗ Article 374: Error - name 'entity_extractor' is not defined\n✗ Article 375: Error - name 'entity_extractor' is not defined\n✗ Article 376: Error - name 'entity_extractor' is not defined\n✗ Article 377: Error - name 'entity_extractor' is not defined\n✗ Article 378: Error - name 'entity_extractor' is not defined\n✗ Article 379: Error - name 'entity_extractor' is not defined\n✗ Article 380: Error - name 'entity_extractor' is not defined\n✗ Article 381: Error - name 'entity_extractor' is not defined\n✗ Article 382: Error - name 'entity_extractor' is not defined\n✗ Article 383: Error - name 'entity_extractor' is not defined\n✗ Article 384: Error - name 'entity_extractor' is not defined\n✗ Article 385: Error - name 'entity_extractor' is not defined\n✗ Article 386: Error - name 'entity_extractor' is not defined\n✗ Article 387: Error - name 'entity_extractor' is not defined\n✗ Article 388: Error - name 'entity_extractor' is not defined\n✗ Article 389: Error - name 'entity_extractor' is not defined\n✗ Article 390: Error - name 'entity_extractor' is not defined\n✗ Article 391: Error - name 'entity_extractor' is not defined\n✗ Article 392: Error - name 'entity_extractor' is not defined\n✗ Article 393: Error - name 'entity_extractor' is not defined\n✗ Article 394: Error - name 'entity_extractor' is not defined\n✗ Article 395: Error - name 'entity_extractor' is not defined\n✗ Article 396: Error - name 'entity_extractor' is not defined\n✗ Article 397: Error - name 'entity_extractor' is not defined\n✗ Article 398: Error - name 'entity_extractor' is not defined\n✗ Article 399: Error - name 'entity_extractor' is not defined\n✗ Article 400: Error - name 'entity_extractor' is not defined\n✗ Article 401: Error - name 'entity_extractor' is not defined\n✗ Article 402: Error - name 'entity_extractor' is not defined\n✗ Article 403: Error - name 'entity_extractor' is not defined\n✗ Article 404: Error - name 'entity_extractor' is not defined\n✗ Article 405: Error - name 'entity_extractor' is not defined\n✗ Article 406: Error - name 'entity_extractor' is not defined\n✗ Article 407: Error - name 'entity_extractor' is not defined\n✗ Article 408: Error - name 'entity_extractor' is not defined\n✗ Article 409: Error - name 'entity_extractor' is not defined\n✗ Article 410: Error - name 'entity_extractor' is not defined\n✗ Article 411: Error - name 'entity_extractor' is not defined\n✗ Article 412: Error - name 'entity_extractor' is not defined\n✗ Article 413: Error - name 'entity_extractor' is not defined\n✗ Article 414: Error - name 'entity_extractor' is not defined\n✗ Article 415: Error - name 'entity_extractor' is not defined\n✗ Article 416: Error - name 'entity_extractor' is not defined\n✗ Article 417: Error - name 'entity_extractor' is not defined\n✗ Article 418: Error - name 'entity_extractor' is not defined\n✗ Article 419: Error - name 'entity_extractor' is not defined\n✗ Article 420: Error - name 'entity_extractor' is not defined\n✗ Article 421: Error - name 'entity_extractor' is not defined\n✗ Article 422: Error - name 'entity_extractor' is not defined\n✗ Article 423: Error - name 'entity_extractor' is not defined\n✗ Article 424: Error - name 'entity_extractor' is not defined\n✗ Article 425: Error - name 'entity_extractor' is not defined\n✗ Article 426: Error - name 'entity_extractor' is not defined\n✗ Article 427: Error - name 'entity_extractor' is not defined\n✗ Article 428: Error - name 'entity_extractor' is not defined\n✗ Article 429: Error - name 'entity_extractor' is not defined\n✗ Article 430: Error - name 'entity_extractor' is not defined\n✗ Article 431: Error - name 'entity_extractor' is not defined\n✗ Article 432: Error - name 'entity_extractor' is not defined\n✗ Article 433: Error - name 'entity_extractor' is not defined\n✗ Article 434: Error - name 'entity_extractor' is not defined\n✗ Article 435: Error - name 'entity_extractor' is not defined\n✗ Article 436: Error - name 'entity_extractor' is not defined\n✗ Article 437: Error - name 'entity_extractor' is not defined\n✗ Article 438: Error - name 'entity_extractor' is not defined\n✗ Article 439: Error - name 'entity_extractor' is not defined\n✗ Article 440: Error - name 'entity_extractor' is not defined\n✗ Article 441: Error - name 'entity_extractor' is not defined\n✗ Article 442: Error - name 'entity_extractor' is not defined\n✗ Article 443: Error - name 'entity_extractor' is not defined\n✗ Article 444: Error - name 'entity_extractor' is not defined\n✗ Article 445: Error - name 'entity_extractor' is not defined\n✗ Article 446: Error - name 'entity_extractor' is not defined\n✗ Article 447: Error - name 'entity_extractor' is not defined\n✗ Article 448: Error - name 'entity_extractor' is not defined\n✗ Article 449: Error - name 'entity_extractor' is not defined\n✗ Article 450: Error - name 'entity_extractor' is not defined\n✗ Article 451: Error - name 'entity_extractor' is not defined\n✗ Article 452: Error - name 'entity_extractor' is not defined\n✗ Article 453: Error - name 'entity_extractor' is not defined\n✗ Article 454: Error - name 'entity_extractor' is not defined\n✗ Article 455: Error - name 'entity_extractor' is not defined\n✗ Article 456: Error - name 'entity_extractor' is not defined\n✗ Article 457: Error - name 'entity_extractor' is not defined\n✗ Article 458: Error - name 'entity_extractor' is not defined\n✗ Article 459: Error - name 'entity_extractor' is not defined\n✗ Article 460: Error - name 'entity_extractor' is not defined\n✗ Article 461: Error - name 'entity_extractor' is not defined\n✗ Article 462: Error - name 'entity_extractor' is not defined\n✗ Article 463: Error - name 'entity_extractor' is not defined\n✗ Article 464: Error - name 'entity_extractor' is not defined\n✗ Article 465: Error - name 'entity_extractor' is not defined\n✗ Article 466: Error - name 'entity_extractor' is not defined\n✗ Article 467: Error - name 'entity_extractor' is not defined\n✗ Article 468: Error - name 'entity_extractor' is not defined\n✗ Article 469: Error - name 'entity_extractor' is not defined\n✗ Article 470: Error - name 'entity_extractor' is not defined\n✗ Article 471: Error - name 'entity_extractor' is not defined\n✗ Article 472: Error - name 'entity_extractor' is not defined\n✗ Article 473: Error - name 'entity_extractor' is not defined\n✗ Article 474: Error - name 'entity_extractor' is not defined\n✗ Article 475: Error - name 'entity_extractor' is not defined\n✗ Article 476: Error - name 'entity_extractor' is not defined\n✗ Article 477: Error - name 'entity_extractor' is not defined\n✗ Article 478: Error - name 'entity_extractor' is not defined\n✗ Article 479: Error - name 'entity_extractor' is not defined\n✗ Article 480: Error - name 'entity_extractor' is not defined\n✗ Article 481: Error - name 'entity_extractor' is not defined\n✗ Article 482: Error - name 'entity_extractor' is not defined\n✗ Article 483: Error - name 'entity_extractor' is not defined\n✗ Article 484: Error - name 'entity_extractor' is not defined\n✗ Article 485: Error - name 'entity_extractor' is not defined\n✗ Article 486: Error - name 'entity_extractor' is not defined\n✗ Article 487: Error - name 'entity_extractor' is not defined\n✗ Article 488: Error - name 'entity_extractor' is not defined\n✗ Article 489: Error - name 'entity_extractor' is not defined\n✗ Article 490: Error - name 'entity_extractor' is not defined\n✗ Article 491: Error - name 'entity_extractor' is not defined\n✗ Article 492: Error - name 'entity_extractor' is not defined\n✗ Article 493: Error - name 'entity_extractor' is not defined\n✗ Article 494: Error - name 'entity_extractor' is not defined\n✗ Article 495: Error - name 'entity_extractor' is not defined\n✗ Article 496: Error - name 'entity_extractor' is not defined\n✗ Article 497: Error - name 'entity_extractor' is not defined\n✗ Article 498: Error - name 'entity_extractor' is not defined\n✗ Article 499: Error - name 'entity_extractor' is not defined\n✗ Article 501: Error - name 'entity_extractor' is not defined\n✗ Article 502: Error - name 'entity_extractor' is not defined\n✗ Article 503: Error - name 'entity_extractor' is not defined\n✗ Article 504: Error - name 'entity_extractor' is not defined\n✗ Article 505: Error - name 'entity_extractor' is not defined\n✗ Article 506: Error - name 'entity_extractor' is not defined\n✗ Article 507: Error - name 'entity_extractor' is not defined\n✗ Article 508: Error - name 'entity_extractor' is not defined\n✗ Article 509: Error - name 'entity_extractor' is not defined\n✗ Article 510: Error - name 'entity_extractor' is not defined\n✗ Article 511: Error - name 'entity_extractor' is not defined\n✗ Article 512: Error - name 'entity_extractor' is not defined\n✗ Article 513: Error - name 'entity_extractor' is not defined\n✗ Article 514: Error - name 'entity_extractor' is not defined\n✗ Article 515: Error - name 'entity_extractor' is not defined\n✗ Article 516: Error - name 'entity_extractor' is not defined\n✗ Article 517: Error - name 'entity_extractor' is not defined\n✗ Article 518: Error - name 'entity_extractor' is not defined\n✗ Article 519: Error - name 'entity_extractor' is not defined\n✗ Article 520: Error - name 'entity_extractor' is not defined\n✗ Article 521: Error - name 'entity_extractor' is not defined\n✗ Article 522: Error - name 'entity_extractor' is not defined\n✗ Article 523: Error - name 'entity_extractor' is not defined\n✗ Article 524: Error - name 'entity_extractor' is not defined\n✗ Article 525: Error - name 'entity_extractor' is not defined\n✗ Article 526: Error - name 'entity_extractor' is not defined\n✗ Article 527: Error - name 'entity_extractor' is not defined\n✗ Article 528: Error - name 'entity_extractor' is not defined\n✗ Article 529: Error - name 'entity_extractor' is not defined\n✗ Article 530: Error - name 'entity_extractor' is not defined\n✗ Article 531: Error - name 'entity_extractor' is not defined\n✗ Article 532: Error - name 'entity_extractor' is not defined\n✗ Article 533: Error - name 'entity_extractor' is not defined\n✗ Article 534: Error - name 'entity_extractor' is not defined\n✗ Article 535: Error - name 'entity_extractor' is not defined\n✗ Article 536: Error - name 'entity_extractor' is not defined\n✗ Article 537: Error - name 'entity_extractor' is not defined\n✗ Article 538: Error - name 'entity_extractor' is not defined\n✗ Article 539: Error - name 'entity_extractor' is not defined\n✗ Article 540: Error - name 'entity_extractor' is not defined\n✗ Article 541: Error - name 'entity_extractor' is not defined\n✗ Article 542: Error - name 'entity_extractor' is not defined\n✗ Article 543: Error - name 'entity_extractor' is not defined\n✗ Article 544: Error - name 'entity_extractor' is not defined\n✗ Article 545: Error - name 'entity_extractor' is not defined\n✗ Article 546: Error - name 'entity_extractor' is not defined\n✗ Article 547: Error - name 'entity_extractor' is not defined\n✗ Article 548: Error - name 'entity_extractor' is not defined\n✗ Article 549: Error - name 'entity_extractor' is not defined\n✗ Article 550: Error - name 'entity_extractor' is not defined\n✗ Article 551: Error - name 'entity_extractor' is not defined\n✗ Article 552: Error - name 'entity_extractor' is not defined\n✗ Article 553: Error - name 'entity_extractor' is not defined\n✗ Article 554: Error - name 'entity_extractor' is not defined\n✗ Article 555: Error - name 'entity_extractor' is not defined\n✗ Article 556: Error - name 'entity_extractor' is not defined\n✗ Article 557: Error - name 'entity_extractor' is not defined\n✗ Article 558: Error - name 'entity_extractor' is not defined\n✗ Article 559: Error - name 'entity_extractor' is not defined\n✗ Article 560: Error - name 'entity_extractor' is not defined\n✗ Article 561: Error - name 'entity_extractor' is not defined\n✗ Article 562: Error - name 'entity_extractor' is not defined\n✗ Article 563: Error - name 'entity_extractor' is not defined\n✗ Article 564: Error - name 'entity_extractor' is not defined\n✗ Article 565: Error - name 'entity_extractor' is not defined\n✗ Article 566: Error - name 'entity_extractor' is not defined\n✗ Article 567: Error - name 'entity_extractor' is not defined\n✗ Article 568: Error - name 'entity_extractor' is not defined\n✗ Article 569: Error - name 'entity_extractor' is not defined\n✗ Article 570: Error - name 'entity_extractor' is not defined\n✗ Article 571: Error - name 'entity_extractor' is not defined\n✗ Article 572: Error - name 'entity_extractor' is not defined\n✗ Article 573: Error - name 'entity_extractor' is not defined\n✗ Article 574: Error - name 'entity_extractor' is not defined\n✗ Article 575: Error - name 'entity_extractor' is not defined\n✗ Article 576: Error - name 'entity_extractor' is not defined\n✗ Article 577: Error - name 'entity_extractor' is not defined\n✗ Article 578: Error - name 'entity_extractor' is not defined\n✗ Article 579: Error - name 'entity_extractor' is not defined\n✗ Article 580: Error - name 'entity_extractor' is not defined\n✗ Article 581: Error - name 'entity_extractor' is not defined\n✗ Article 582: Error - name 'entity_extractor' is not defined\n✗ Article 583: Error - name 'entity_extractor' is not defined\n✗ Article 584: Error - name 'entity_extractor' is not defined\n✗ Article 585: Error - name 'entity_extractor' is not defined\n✗ Article 586: Error - name 'entity_extractor' is not defined\n\n================================================================================\n✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\n================================================================================\nTotal articles processed: 587\nArticles with graphs: 0\nArticles without graphs: 587\n\nTotal entities extracted: 0\nTotal relations extracted: 0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================================\n# CHECK WHAT WE SUCCESSFULLY PROCESSED\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"CHECKING SUCCESSFUL PROCESSING\")\nprint(\"=\" * 80)\n\n# Count successes\nsuccessful = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\nfailed = [kg for kg in all_article_kgs if kg['num_edges'] == 0]\n\nprint(f\"\\nTotal articles: {len(all_article_kgs)}\")\nprint(f\"✓ Successfully processed: {len(successful)}\")\nprint(f\"✗ Failed/Empty: {len(failed)}\")\n\nif successful:\n    print(f\"\\nSuccessful articles with KGs:\")\n    for kg in successful[:10]:\n        print(f\"  Article {kg['id']}: {kg['num_nodes']} nodes, {kg['num_edges']} edges\")\n    \n    # Export the successful ones\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXPORTING SUCCESSFUL RESULTS\")\n    print(\"=\" * 80)\n    \n    import json\n    import os\n    \n    # Create output directory\n    os.makedirs('/kaggle/working/individual_kgs', exist_ok=True)\n    \n    # Export successful KGs\n    for kg in successful:\n        article_id = kg['id']\n        \n        graph_data = {\n            'article_id': article_id,\n            'text': kg['text'],\n            'concepts': kg['concepts'],\n            'entities': kg['entities'],\n            'relations': [\n                {'subject': s, 'predicate': p, 'object': o}\n                for s, p, o in kg['relations']\n            ],\n            'graph': nx.node_link_data(kg['graph'])\n        }\n        \n        filepath = f'/kaggle/working/individual_kgs/article_{article_id}.json'\n        with open(filepath, 'w') as f:\n            json.dump(graph_data, f, indent=2)\n    \n    print(f\"✓ Saved {len(successful)} JSON files\")\n    \n    # Export triples\n    all_triples = []\n    for kg in successful:\n        for subj, pred, obj in kg['relations']:\n            all_triples.append({\n                'article_id': kg['id'],\n                'subject': subj,\n                'predicate': pred,\n                'object': obj\n            })\n    \n    triples_df = pd.DataFrame(all_triples)\n    triples_df.to_csv('/kaggle/working/successful_triples.csv', index=False)\n    print(f\"✓ Saved {len(all_triples)} triples to successful_triples.csv\")\n    \n    # Summary\n    summary = []\n    for kg in all_article_kgs:\n        summary.append({\n            'article_id': kg['id'],\n            'text_preview': kg['text'][:100],\n            'num_entities': len(kg['entities']),\n            'num_relations': len(kg['relations']),\n            'num_nodes': kg['num_nodes'],\n            'num_edges': kg['num_edges'],\n            'status': 'success' if kg['num_edges'] > 0 else 'failed'\n        })\n    \n    summary_df = pd.DataFrame(summary)\n    summary_df.to_csv('/kaggle/working/processing_summary.csv', index=False)\n    print(f\"✓ Saved summary\")\n    \n    print(\"\\n✓ DONE! Downloaded what we could process.\")\nelse:\n    print(\"\\n✗ No successful KGs to export\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.712791Z","iopub.execute_input":"2025-10-14T18:45:00.713009Z","iopub.status.idle":"2025-10-14T18:45:00.731694Z","shell.execute_reply.started":"2025-10-14T18:45:00.712996Z","shell.execute_reply":"2025-10-14T18:45:00.731037Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCHECKING SUCCESSFUL PROCESSING\n================================================================================\n\nTotal articles: 587\n✓ Successfully processed: 0\n✗ Failed/Empty: 587\n\n✗ No successful KGs to export\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================================\n# VIEW SUCCESSFUL RESULTS\n# ============================================================================\n\nimport pandas as pd\n\nprint(\"=\" * 80)\nprint(\"YOUR 46 SUCCESSFUL KNOWLEDGE GRAPHS\")\nprint(\"=\" * 80)\n\n# Load summary\nsummary_df = pd.read_csv('/kaggle/working/processing_summary.csv')\nsuccessful_df = summary_df[summary_df['status'] == 'success'].copy()\n\nprint(f\"\\nTop 10 Largest KGs:\")\nprint(\"-\" * 80)\ntop_kgs = successful_df.nlargest(10, 'num_edges')\nprint(top_kgs[['article_id', 'num_nodes', 'num_edges', 'text_preview']].to_string(index=False))\n\n# Load triples\ntriples_df = pd.read_csv('/kaggle/working/successful_triples.csv')\n\nprint(f\"\\n\\nSample Triples:\")\nprint(\"-\" * 80)\nprint(triples_df.head(20).to_string(index=False))\n\nprint(f\"\\n\\nStatistics:\")\nprint(\"-\" * 80)\nprint(f\"Total successful articles: {len(successful_df)}\")\nprint(f\"Total triples extracted: {len(triples_df)}\")\nprint(f\"Avg edges per KG: {successful_df['num_edges'].mean():.2f}\")\nprint(f\"Max edges in a KG: {successful_df['num_edges'].max()}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FILES READY TO DOWNLOAD:\")\nprint(\"=\" * 80)\nprint(\"  📁 /kaggle/working/individual_kgs/ (46 JSON files)\")\nprint(\"  📄 /kaggle/working/successful_triples.csv\")\nprint(\"  📄 /kaggle/working/processing_summary.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.732505Z","iopub.execute_input":"2025-10-14T18:45:00.732779Z","iopub.status.idle":"2025-10-14T18:45:00.984031Z","shell.execute_reply.started":"2025-10-14T18:45:00.732765Z","shell.execute_reply":"2025-10-14T18:45:00.982852Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nYOUR 46 SUCCESSFUL KNOWLEDGE GRAPHS\n================================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2283152679.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/processing_summary.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0msuccessful_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msummary_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'success'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/processing_summary.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/processing_summary.csv'","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# VISUALIZE INDIVIDUAL ARTICLE KNOWLEDGE GRAPHS\n# ============================================================================\n\n!pip install pyvis --quiet\n\nfrom pyvis.network import Network\nimport networkx as nx\nimport os\n\ndef visualize_article_kg(kg_data, output_file):\n    \"\"\"\n    Create interactive HTML visualization for one article's KG\n    \"\"\"\n    graph = kg_data['graph']\n    article_id = kg_data['id']\n    \n    if len(graph.nodes()) == 0:\n        print(f\"  Article {article_id}: No graph to visualize\")\n        return None\n    \n    print(f\"  Article {article_id}: {len(graph.nodes())} nodes, {len(graph.edges())} edges\")\n    \n    # Create pyvis network\n    net = Network(\n        height='800px',\n        width='100%',\n        bgcolor='#ffffff',\n        font_color='black',\n        directed=True,\n        notebook=False\n    )\n    \n    # Set physics options\n    net.set_options(\"\"\"\n    {\n      \"physics\": {\n        \"enabled\": true,\n        \"barnesHut\": {\n          \"gravitationalConstant\": -8000,\n          \"centralGravity\": 0.3,\n          \"springLength\": 95,\n          \"springConstant\": 0.04\n        },\n        \"minVelocity\": 0.75\n      },\n      \"nodes\": {\n        \"font\": {\"size\": 16}\n      },\n      \"edges\": {\n        \"font\": {\"size\": 12},\n        \"arrows\": {\"to\": {\"enabled\": true, \"scaleFactor\": 0.5}}\n      }\n    }\n    \"\"\")\n    \n    # Add nodes with colors based on degree\n    for node in graph.nodes():\n        degree = graph.degree(node)\n        size = 20 + degree * 5\n        \n        # Color based on degree\n        if degree > 5:\n            color = '#e74c3c'  # Red for highly connected\n        elif degree > 2:\n            color = '#3498db'  # Blue for medium\n        else:\n            color = '#95a5a6'  # Gray for low\n        \n        net.add_node(\n            node, \n            label=node, \n            size=size, \n            title=f\"{node}\\nConnections: {degree}\",\n            color=color\n        )\n    \n    # Add edges with labels\n    for source, target, data in graph.edges(data=True):\n        relation = data.get('relation', 'related_to')\n        net.add_edge(source, target, label=relation, title=relation)\n    \n    # Save\n    net.save_graph(output_file)\n    \n    return net\n\n# Create visualizations directory\nos.makedirs('/kaggle/working/kg_visualizations', exist_ok=True)\n\nprint(\"=\" * 80)\nprint(\"CREATING INTERACTIVE VISUALIZATIONS\")\nprint(\"=\" * 80)\n\n# Visualize all successful KGs\nvisualized = 0\nfor kg in all_article_kgs:\n    if kg['num_edges'] > 0:  # Only visualize articles with graphs\n        article_id = kg['id']\n        output_file = f'/kaggle/working/kg_visualizations/article_{article_id}_graph.html'\n        visualize_article_kg(kg, output_file)\n        visualized += 1\n\nprint(f\"\\n✓ Created {visualized} interactive visualizations!\")\nprint(f\"  Location: /kaggle/working/kg_visualizations/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.984667Z","iopub.status.idle":"2025-10-14T18:45:00.984956Z","shell.execute_reply.started":"2025-10-14T18:45:00.984844Z","shell.execute_reply":"2025-10-14T18:45:00.984856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# VERIFY FILES EXIST\n# ============================================================================\n\nimport os\n\nprint(\"=\" * 80)\nprint(\"CHECKING /kaggle/working DIRECTORY\")\nprint(\"=\" * 80)\n\n# List all folders in /kaggle/working\nprint(\"\\nFolders in /kaggle/working:\")\nfor item in os.listdir('/kaggle/working'):\n    if os.path.isdir(os.path.join('/kaggle/working', item)):\n        print(f\"  📁 {item}\")\n\n# Check if kg_visualizations exists\nviz_path = '/kaggle/working/kg_visualizations'\nif os.path.exists(viz_path):\n    print(f\"\\n✓ kg_visualizations folder EXISTS!\")\n    print(f\"\\nFiles inside kg_visualizations:\")\n    files = os.listdir(viz_path)\n    print(f\"  Total files: {len(files)}\")\n    for f in files[:10]:  # Show first 10\n        print(f\"    📄 {f}\")\n    if len(files) > 10:\n        print(f\"    ... and {len(files) - 10} more files\")\nelse:\n    print(f\"\\n✗ kg_visualizations folder DOES NOT EXIST!\")\n    print(\"  Let me create it again...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.985674Z","iopub.status.idle":"2025-10-14T18:45:00.985897Z","shell.execute_reply.started":"2025-10-14T18:45:00.985799Z","shell.execute_reply":"2025-10-14T18:45:00.985808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CREATE INDEX AND ZIP FOR YOUR 46 VISUALIZATIONS\n# ============================================================================\n\nimport zipfile\nimport os\n\nprint(\"=\" * 80)\nprint(\"CREATING INDEX AND ZIP PACKAGE\")\nprint(\"=\" * 80)\n\n# Create index.html\nviz_dir = '/kaggle/working/kg_visualizations'\n\n# Get all successful KGs\nsuccessful_kgs = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\n\nindex_html = f\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Knowledge Graph Visualizations</title>\n    <style>\n        body {{\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n            min-height: 100vh;\n        }}\n        .container {{\n            max-width: 1400px;\n            margin: 0 auto;\n            background: white;\n            border-radius: 15px;\n            padding: 30px;\n            box-shadow: 0 10px 40px rgba(0,0,0,0.3);\n        }}\n        h1 {{\n            color: #667eea;\n            text-align: center;\n            margin-bottom: 10px;\n        }}\n        .summary {{\n            text-align: center;\n            color: #666;\n            margin-bottom: 30px;\n            font-size: 18px;\n        }}\n        .search-box {{\n            margin: 20px 0;\n            text-align: center;\n        }}\n        .search-box input {{\n            width: 60%;\n            padding: 15px;\n            font-size: 16px;\n            border: 2px solid #667eea;\n            border-radius: 25px;\n            outline: none;\n        }}\n        .grid {{\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));\n            gap: 20px;\n            margin-top: 30px;\n        }}\n        .card {{\n            background: white;\n            border: 2px solid #e0e0e0;\n            border-radius: 10px;\n            padding: 20px;\n            transition: all 0.3s ease;\n            cursor: pointer;\n        }}\n        .card:hover {{\n            transform: translateY(-5px);\n            box-shadow: 0 8px 25px rgba(102, 126, 234, 0.4);\n            border-color: #667eea;\n        }}\n        .card-header {{\n            display: flex;\n            justify-content: space-between;\n            align-items: center;\n            margin-bottom: 15px;\n        }}\n        .article-id {{\n            font-size: 28px;\n            font-weight: bold;\n            color: #667eea;\n        }}\n        .card-text {{\n            color: #666;\n            font-size: 13px;\n            line-height: 1.4;\n            margin-bottom: 15px;\n            height: 50px;\n            overflow: hidden;\n        }}\n        .card-stats {{\n            display: flex;\n            justify-content: space-around;\n            border-top: 1px solid #e0e0e0;\n            padding-top: 15px;\n        }}\n        .stat {{\n            text-align: center;\n        }}\n        .stat-value {{\n            font-size: 22px;\n            font-weight: bold;\n            color: #667eea;\n        }}\n        .stat-label {{\n            font-size: 11px;\n            color: #999;\n            text-transform: uppercase;\n        }}\n        .btn {{\n            display: block;\n            margin-top: 10px;\n            padding: 10px;\n            background: #667eea;\n            color: white;\n            text-align: center;\n            text-decoration: none;\n            border-radius: 5px;\n            transition: background 0.3s;\n        }}\n        .btn:hover {{\n            background: #764ba2;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>🌐 Interactive Knowledge Graph Visualizations</h1>\n        <div class=\"summary\">\n            <strong>{len(successful_kgs)}</strong> interactive graph visualizations\n        </div>\n        \n        <div class=\"search-box\">\n            <input type=\"text\" id=\"searchBox\" placeholder=\"🔍 Search by article ID...\" onkeyup=\"filterCards()\">\n        </div>\n        \n        <div class=\"grid\" id=\"cardsContainer\">\n\"\"\"\n\n# Add cards for each successful article\nfor kg in sorted(successful_kgs, key=lambda x: x['id']):\n    article_id = kg['id']\n    text_preview = kg['text'][:80].replace('\\n', ' ').replace('\"', '&quot;')\n    \n    index_html += f\"\"\"\n            <div class=\"card\" data-id=\"{article_id}\">\n                <div class=\"card-header\">\n                    <div class=\"article-id\">#{article_id}</div>\n                </div>\n                <div class=\"card-text\">{text_preview}...</div>\n                <div class=\"card-stats\">\n                    <div class=\"stat\">\n                        <div class=\"stat-value\">{kg['num_nodes']}</div>\n                        <div class=\"stat-label\">Nodes</div>\n                    </div>\n                    <div class=\"stat\">\n                        <div class=\"stat-value\">{kg['num_edges']}</div>\n                        <div class=\"stat-label\">Edges</div>\n                    </div>\n                    <div class=\"stat\">\n                        <div class=\"stat-value\">{len(kg['entities'])}</div>\n                        <div class=\"stat-label\">Entities</div>\n                    </div>\n                </div>\n                <a href=\"article_{article_id}_graph.html\" class=\"btn\" target=\"_blank\">\n                    🔍 View Interactive Graph\n                </a>\n            </div>\n\"\"\"\n\nindex_html += \"\"\"\n        </div>\n    </div>\n    \n    <script>\n        function filterCards() {\n            const searchValue = document.getElementById('searchBox').value.toLowerCase();\n            const cards = document.querySelectorAll('.card');\n            \n            cards.forEach(card => {\n                const articleId = card.getAttribute('data-id');\n                if (articleId.includes(searchValue) || searchValue === '') {\n                    card.style.display = 'block';\n                } else {\n                    card.style.display = 'none';\n                }\n            });\n        }\n    </script>\n</body>\n</html>\n\"\"\"\n\n# Save index\nindex_path = f'{viz_dir}/index.html'\nwith open(index_path, 'w', encoding='utf-8') as f:\n    f.write(index_html)\n\nprint(f\"✓ Created index.html\")\n\n# Create ZIP\nprint(\"\\nCreating ZIP package...\")\nzip_path = '/kaggle/working/kg_visualizations_complete.zip'\n\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for filename in os.listdir(viz_dir):\n        if filename.endswith('.html'):\n            filepath = os.path.join(viz_dir, filename)\n            zipf.write(filepath, filename)\n\nfile_size = os.path.getsize(zip_path) / (1024 * 1024)\n\nprint(f\"\\n✓ ZIP created: kg_visualizations_complete.zip\")\nprint(f\"  Size: {file_size:.2f} MB\")\nprint(f\"  Files: {len(successful_kgs) + 1} ({len(successful_kgs)} graphs + 1 index)\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"📥 DOWNLOAD YOUR VISUALIZATIONS\")\nprint(\"=\" * 80)\n\nfrom IPython.display import FileLink, display\ndisplay(FileLink(zip_path))\n\nprint(\"\\n✅ INSTRUCTIONS:\")\nprint(\"  1. Download the ZIP file\")\nprint(\"  2. Extract it\")\nprint(\"  3. Open 'index.html' in your browser\")\nprint(\"  4. Click any article to see its interactive graph!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.986612Z","iopub.status.idle":"2025-10-14T18:45:00.987049Z","shell.execute_reply.started":"2025-10-14T18:45:00.986936Z","shell.execute_reply":"2025-10-14T18:45:00.986947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# RECREATE ZIP FILE PROPERLY\n# ============================================================================\n\nimport zipfile\nimport os\n\nprint(\"=\" * 80)\nprint(\"RECREATING ZIP FILE\")\nprint(\"=\" * 80)\n\nviz_dir = '/kaggle/working/kg_visualizations'\n\n# Verify directory exists\nif not os.path.exists(viz_dir):\n    print(f\"✗ Directory doesn't exist: {viz_dir}\")\nelse:\n    print(f\"✓ Directory exists: {viz_dir}\")\n    \n    # List files\n    files = [f for f in os.listdir(viz_dir) if f.endswith('.html')]\n    print(f\"✓ Found {len(files)} HTML files\")\n    \n    # Create ZIP\n    zip_path = '/kaggle/working/my_kg_visualizations.zip'\n    \n    print(f\"\\nCreating ZIP: {zip_path}\")\n    \n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for filename in files:\n            filepath = os.path.join(viz_dir, filename)\n            zipf.write(filepath, filename)\n            print(f\"  Added: {filename}\")\n    \n    # Verify ZIP was created\n    if os.path.exists(zip_path):\n        file_size = os.path.getsize(zip_path) / (1024 * 1024)\n        print(f\"\\n✓ ZIP created successfully!\")\n        print(f\"  Path: {zip_path}\")\n        print(f\"  Size: {file_size:.2f} MB\")\n        print(f\"  Files: {len(files)}\")\n        \n        # Display download link\n        from IPython.display import FileLink, display\n        print(\"\\n📥 DOWNLOAD HERE:\")\n        display(FileLink(zip_path))\n    else:\n        print(f\"\\n✗ Failed to create ZIP\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.988053Z","iopub.status.idle":"2025-10-14T18:45:00.988360Z","shell.execute_reply.started":"2025-10-14T18:45:00.988211Z","shell.execute_reply":"2025-10-14T18:45:00.988225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# LIST ALL YOUR VISUALIZATION FILES\n# ============================================================================\n\nimport os\n\nviz_dir = '/kaggle/working/kg_visualizations'\n\nprint(\"=\" * 80)\nprint(\"YOUR VISUALIZATION FILES\")\nprint(\"=\" * 80)\n\nfiles = sorted([f for f in os.listdir(viz_dir) if f.endswith('.html')])\n\nprint(f\"\\nTotal files: {len(files)}\\n\")\n\nfor i, filename in enumerate(files, 1):\n    filepath = os.path.join(viz_dir, filename)\n    size = os.path.getsize(filepath) / 1024  # KB\n    print(f\"{i:3d}. {filename:40s} ({size:.1f} KB)\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"HOW TO DOWNLOAD:\")\nprint(\"=\" * 80)\nprint(\"\"\"\nOPTION 1: Download folder from Kaggle UI\n  1. Look at the right panel \"Output\" section\n  2. Find the folder: kg_visualizations\n  3. Click the 3 dots (...) next to it\n  4. Select \"Download\"\n\nOPTION 2: Copy files to a visible location\n  Run the next code block to copy files to a downloadable location\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.988987Z","iopub.status.idle":"2025-10-14T18:45:00.989206Z","shell.execute_reply.started":"2025-10-14T18:45:00.989090Z","shell.execute_reply":"2025-10-14T18:45:00.989098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# COPY FILES TO ROOT FOR EASY DOWNLOAD\n# ============================================================================\n\nimport shutil\nimport os\n\nprint(\"Copying files to /kaggle/working/ root...\")\n\nviz_dir = '/kaggle/working/kg_visualizations'\nfiles = [f for f in os.listdir(viz_dir) if f.endswith('.html')]\n\n# Copy index\nshutil.copy(\n    os.path.join(viz_dir, 'index.html'),\n    '/kaggle/working/VISUALIZATION_INDEX.html'\n)\nprint(\"✓ Copied: VISUALIZATION_INDEX.html\")\n\n# Copy first 5 sample graphs\nsamples = [f for f in files if f.startswith('article_') and f.endswith('_graph.html')][:5]\n\nfor filename in samples:\n    src = os.path.join(viz_dir, filename)\n    dst = os.path.join('/kaggle/working', filename)\n    shutil.copy(src, dst)\n    print(f\"✓ Copied: {filename}\")\n\nprint(f\"\\n✓ Files copied to /kaggle/working/\")\nprint(\"Now you can see and download them from the Output panel!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.989804Z","iopub.status.idle":"2025-10-14T18:45:00.990036Z","shell.execute_reply.started":"2025-10-14T18:45:00.989913Z","shell.execute_reply":"2025-10-14T18:45:00.989922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: DEFINE SIGNATURES\n# ============================================================================\n\nclass EntityExtractor(dspy.Signature):\n    \"\"\"Extract key entities from the given text. Extracted entities are nouns, \n    verbs, or adjectives, particularly regarding sentiment. This is for an \n    extraction task, please be thorough and accurate to the reference text.\n    \n    Return ONLY a valid JSON list format: [\"entity1\", \"entity2\", \"entity3\"]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract entities from\")\n    entities = dspy.OutputField(desc=\"List of extracted entities in JSON format\")\n\nclass RelationExtractor(dspy.Signature):\n    \"\"\"Extract subject-predicate-object triples from the assistant message. \n    A predicate (1-3 words) defines the relationship between the subject and \n    object. Relationship may be fact or sentiment based on assistant's message. \n    Subject and object are entities. Entities provided are from the assistant \n    message and prior conversation history, though you may not need all of them. \n    This is for an extraction task, please be thorough, accurate, and faithful \n    to the reference text.\n    \n    Return ONLY valid JSON format: [[\"subject1\", \"predicate1\", \"object1\"], [\"subject2\", \"predicate2\", \"object2\"]]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract relations from\")\n    entities = dspy.InputField(desc=\"List of available entities\")\n    triples = dspy.OutputField(desc=\"List of [subject, predicate, object] triples in JSON format\")\n\nprint(\"✓ Signatures defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.991221Z","iopub.status.idle":"2025-10-14T18:45:00.991483Z","shell.execute_reply.started":"2025-10-14T18:45:00.991369Z","shell.execute_reply":"2025-10-14T18:45:00.991383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: CREATE ENTITY EXTRACTOR\n# ============================================================================\n\nclass ExtractEntities(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(EntityExtractor)\n    \n    def forward(self, text: str) -> List[str]:\n        if not text or len(text.strip()) < 3:\n            return []\n            \n        result = self.extract(text=text)\n        \n        try:\n            entities_text = result.entities.strip()\n            \n            if '[' in entities_text and ']' in entities_text:\n                start = entities_text.find('[')\n                end = entities_text.rfind(']') + 1\n                entities_text = entities_text[start:end]\n            \n            entities = json.loads(entities_text)\n            \n            if isinstance(entities, list):\n                return [str(e).lower().strip() for e in entities if e and len(str(e).strip()) > 1]\n            return []\n            \n        except:\n            try:\n                entities_text = result.entities.strip()\n                if entities_text.startswith('['):\n                    entities_text = entities_text[1:]\n                if entities_text.endswith(']'):\n                    entities_text = entities_text[:-1]\n                \n                entities = []\n                for item in entities_text.split(','):\n                    item = item.strip(' \"\\'\\n\\t')\n                    if item and len(item) > 1:\n                        entities.append(item.lower())\n                \n                return entities[:50]\n            except:\n                return []\n\nentity_extractor = ExtractEntities()\nprint(\"✓ Entity Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.992435Z","iopub.status.idle":"2025-10-14T18:45:00.992726Z","shell.execute_reply.started":"2025-10-14T18:45:00.992614Z","shell.execute_reply":"2025-10-14T18:45:00.992627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: CREATE RELATION EXTRACTOR\n# ============================================================================\n\nclass ExtractRelations(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(RelationExtractor)\n    \n    def forward(self, text: str, entities: List[str]) -> List[Tuple[str, str, str]]:\n        if not entities or not text:\n            return []\n        \n        entities_subset = entities[:30]\n        entities_str = json.dumps(entities_subset)\n        \n        result = self.extract(text=text, entities=entities_str)\n        \n        try:\n            triples_text = result.triples.strip()\n            \n            if '[' in triples_text and ']' in triples_text:\n                start = triples_text.find('[')\n                end = triples_text.rfind(']') + 1\n                triples_text = triples_text[start:end]\n            \n            triples = json.loads(triples_text)\n            \n            normalized_triples = []\n            for triple in triples:\n                if isinstance(triple, (list, tuple)) and len(triple) == 3:\n                    s, p, o = triple\n                    s = str(s).lower().strip()\n                    p = str(p).lower().strip()\n                    o = str(o).lower().strip()\n                    \n                    if s and p and o and s != o:\n                        normalized_triples.append((s, p, o))\n            \n            return normalized_triples[:100]\n            \n        except Exception as e:\n            return []\n\nrelation_extractor = ExtractRelations()\nprint(\"✓ Relation Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.993570Z","iopub.status.idle":"2025-10-14T18:45:00.993863Z","shell.execute_reply.started":"2025-10-14T18:45:00.993709Z","shell.execute_reply":"2025-10-14T18:45:00.993721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: TEST EXTRACTION\n# ============================================================================\n\nprint(\"\\nTesting Entity & Relation Extraction...\")\nprint(\"=\" * 80)\n\n# Test on first 2 documents\nfor i in range(min(2, len(data))):\n    text = data[i].get('Article Text', '')\n    \n    print(f\"\\nDocument {i+1}:\")\n    print(f\"  Text: {text[:80]}...\")\n    \n    # Extract entities\n    print(f\"  Extracting entities...\")\n    entities = entity_extractor(text)\n    print(f\"  ✓ Found {len(entities)} entities: {entities[:5]}...\")\n    \n    # Extract relations\n    if entities:\n        print(f\"  Extracting relations...\")\n        relations = relation_extractor(text, entities)\n        print(f\"  ✓ Found {len(relations)} relations\")\n        \n        if relations:\n            for j, (s, p, o) in enumerate(relations[:3], 1):\n                print(f\"    {j}. ({s}) --[{p}]--> ({o})\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ Test complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.994898Z","iopub.status.idle":"2025-10-14T18:45:00.995154Z","shell.execute_reply.started":"2025-10-14T18:45:00.995019Z","shell.execute_reply":"2025-10-14T18:45:00.995033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CLUSTERING SIGNATURES (DEFINE FIRST!)\n# ============================================================================\n\nclass ClusterValidator(dspy.Signature):\n    \"\"\"Verify if these entities belong in the same cluster.\n    A cluster should contain entities that are the same in meaning, with different:\n    - tenses, plural forms, stem forms, upper/lower cases\n    Or entities with close semantic meanings.\n    \n    Return ONLY valid JSON format: [\"entity1\", \"entity2\", \"entity3\"]\n    Return only entities you are confident belong together.\n    If not confident, return empty list [].\n    \"\"\"\n    \n    entities = dspy.InputField(desc=\"Entities to validate\")\n    valid_cluster = dspy.OutputField(desc=\"Validated cluster in JSON format\")\n\nprint(\"✓ ClusterValidator Signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.996198Z","iopub.status.idle":"2025-10-14T18:45:00.996456Z","shell.execute_reply.started":"2025-10-14T18:45:00.996330Z","shell.execute_reply":"2025-10-14T18:45:00.996342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SEMANTIC SIMILARITY CLUSTERING (FROM PAPER)\n# ============================================================================\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nclass SemanticEntityClustering(dspy.Module):\n    def __init__(self, similarity_threshold=0.75):\n        super().__init__()\n        self.validator = dspy.ChainOfThought(ClusterValidator)\n        \n        # Load embedding model (same as paper)\n        print(\"Loading sentence transformer model...\")\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        print(\"✓ Model loaded\")\n        \n        self.similarity_threshold = similarity_threshold\n    \n    def _parse_cluster(self, text: str) -> List[str]:\n        \"\"\"Parse cluster from LLM response\"\"\"\n        try:\n            text = text.strip()\n            if '[' in text and ']' in text:\n                start = text.find('[')\n                end = text.rfind(']') + 1\n                text = text[start:end]\n            \n            cluster = json.loads(text)\n            if isinstance(cluster, list):\n                return [str(e).lower().strip() for e in cluster if e]\n            return []\n        except:\n            return []\n    \n    def _get_semantic_clusters(self, entities: List[str]) -> List[List[str]]:\n        \"\"\"Group entities by semantic similarity using embeddings\"\"\"\n        \n        if len(entities) == 0:\n            return []\n        \n        # Get embeddings for all entities\n        embeddings = self.model.encode(entities)\n        \n        # Compute pairwise cosine similarity\n        similarity_matrix = cosine_similarity(embeddings)\n        \n        # Find clusters using similarity threshold\n        clusters = []\n        remaining = set(range(len(entities)))\n        \n        for i in range(len(entities)):\n            if i not in remaining:\n                continue\n            \n            # Find all entities similar to this one\n            cluster_indices = [i]\n            remaining.discard(i)\n            \n            for j in range(i + 1, len(entities)):\n                if j not in remaining:\n                    continue\n                \n                # Check if similar enough\n                if similarity_matrix[i][j] >= self.similarity_threshold:\n                    cluster_indices.append(j)\n                    remaining.discard(j)\n            \n            # Convert indices to entity names\n            cluster = [entities[idx] for idx in cluster_indices]\n            \n            # Only keep clusters with 2-4 entities\n            if 2 <= len(cluster) <= 4:\n                clusters.append(cluster)\n            elif len(cluster) == 1:\n                # Keep singletons for later\n                pass\n        \n        return clusters\n    \n    def forward(self, entities: List[str]) -> Dict[str, List[str]]:\n        \"\"\"Semantic clustering with LLM validation\"\"\"\n        \n        print(f\"Starting semantic clustering with {len(entities)} entities...\")\n        print(f\"  Similarity threshold: {self.similarity_threshold}\")\n        \n        # Remove duplicates\n        unique_entities = list(set(entities))\n        \n        # Step 1: Find semantic clusters using embeddings\n        print(\"  Computing semantic similarities...\")\n        potential_clusters = self._get_semantic_clusters(unique_entities)\n        \n        print(f\"  Found {len(potential_clusters)} potential clusters\")\n        \n        # Step 2: Validate with LLM\n        validated_clusters = {}\n        cluster_id = 0\n        clustered_entities = set()\n        \n        for cluster in potential_clusters:\n            try:\n                # Ask LLM to validate\n                validation = self.validator(entities=json.dumps(cluster))\n                validated = self._parse_cluster(validation.valid_cluster)\n                \n                if validated and len(validated) >= 2:\n                    cluster_label = validated[0]\n                    validated_clusters[cluster_label] = validated\n                    \n                    for entity in validated:\n                        clustered_entities.add(entity)\n                    \n                    print(f\"  ✓ Cluster {cluster_id}: {validated}\")\n                    cluster_id += 1\n                else:\n                    # LLM rejected - add as singletons\n                    for entity in cluster:\n                        if entity not in clustered_entities:\n                            validated_clusters[entity] = [entity]\n                            clustered_entities.add(entity)\n            except:\n                # Error - add as singletons\n                for entity in cluster:\n                    if entity not in clustered_entities:\n                        validated_clusters[entity] = [entity]\n                        clustered_entities.add(entity)\n        \n        # Step 3: Add all remaining entities as singletons\n        for entity in unique_entities:\n            if entity not in clustered_entities:\n                validated_clusters[entity] = [entity]\n        \n        multi = sum(1 for v in validated_clusters.values() if len(v) > 1)\n        print(f\"✓ Semantic clustering complete: {len(validated_clusters)} total clusters\")\n        print(f\"  Multi-entity clusters: {multi}\")\n        print(f\"  Singleton entities: {len(validated_clusters) - multi}\")\n        \n        return validated_clusters\n\n# Create semantic clusterer with different thresholds\nentity_clusterer_semantic = SemanticEntityClustering(similarity_threshold=0.75)\nprint(\"\\n✓ Semantic Entity Clustering Module created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.997700Z","iopub.status.idle":"2025-10-14T18:45:00.998003Z","shell.execute_reply.started":"2025-10-14T18:45:00.997853Z","shell.execute_reply":"2025-10-14T18:45:00.997867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TEST SEMANTIC CLUSTERING\n# ============================================================================\n\nprint(\"\\nTesting SEMANTIC Clustering (Embeddings + LLM)...\")\nprint(\"=\" * 80)\n\ntest_entities = [\n    # Should cluster (same concept)\n    'phone', 'mobile phone', 'cellphone', 'cell phone',\n    \n    # Should cluster (same concept)\n    'usa', 'united states', 'america',\n    \n    # Should cluster (similar meaning)\n    'helping hand', 'assistant', 'helper', 'support',\n    \n    # Should cluster (same concept)\n    'service', 'services',\n    \n    # Should cluster (same concept)\n    'call', 'calling', 'calls',\n    \n    # Should NOT cluster (different brands)\n    'nokia', 'motorola', 'samsung',\n    \n    # Should NOT cluster (different concepts)\n    'signal', 'battery', 'screen',\n]\n\nprint(f\"Testing with {len(test_entities)} entities:\")\nfor e in test_entities:\n    print(f\"  - {e}\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Cluster with semantic method\nsemantic_clusters = entity_clusterer_semantic(test_entities)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SEMANTIC CLUSTERING RESULTS:\")\nprint(\"=\" * 80)\n\n# Show multi-entity clusters\nmulti_clusters = {k: v for k, v in semantic_clusters.items() if len(v) > 1}\n\nif multi_clusters:\n    print(f\"\\n✓ Found {len(multi_clusters)} semantic clusters:\")\n    for i, (label, members) in enumerate(multi_clusters.items(), 1):\n        print(f\"  {i}. {members}\")\n        \n        # Show similarity scores\n        if len(members) > 1:\n            embeddings = entity_clusterer_semantic.model.encode(members)\n            similarities = cosine_similarity(embeddings)\n            avg_sim = (similarities.sum() - len(members)) / (len(members) * (len(members) - 1))\n            print(f\"     Average similarity: {avg_sim:.3f}\")\nelse:\n    print(\"\\n  No clusters found\")\n\nprint(f\"\\nSingleton entities: {len(semantic_clusters) - len(multi_clusters)}\")\n\nprint(\"\\n✓ Test complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:00.999143Z","iopub.status.idle":"2025-10-14T18:45:00.999450Z","shell.execute_reply.started":"2025-10-14T18:45:00.999289Z","shell.execute_reply":"2025-10-14T18:45:00.999301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# VISUALIZE HOW CLUSTERING WORKS\n# ============================================================================\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Test entities\ntest_entities = ['phone', 'mobile phone', 'cellphone', 'nokia', 'motorola']\n\n# Get embeddings\nembeddings = entity_clusterer_semantic.model.encode(test_entities)\n\n# Calculate similarities\nprint(\"Similarity Matrix:\")\nprint(\"=\" * 80)\nprint(f\"{'':15s}\", end='')\nfor e in test_entities:\n    print(f\"{e:15s}\", end='')\nprint()\n\nfor i, entity1 in enumerate(test_entities):\n    print(f\"{entity1:15s}\", end='')\n    for j, entity2 in enumerate(test_entities):\n        similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n        print(f\"{similarity:15.3f}\", end='')\n    print()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"Threshold: 0.75\")\nprint(\"=\" * 80)\n\n# Show which pairs would cluster\nprint(\"\\nPairs above threshold (would cluster together):\")\nfor i, entity1 in enumerate(test_entities):\n    for j, entity2 in enumerate(test_entities):\n        if i < j:  # Avoid duplicates\n            similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n            if similarity >= 0.75:\n                print(f\"  ✓ {entity1:15s} ↔ {entity2:15s} = {similarity:.3f}\")\n\nprint(\"\\nPairs below threshold (stay separate):\")\nfor i, entity1 in enumerate(test_entities):\n    for j, entity2 in enumerate(test_entities):\n        if i < j:\n            similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n            if similarity < 0.75:\n                print(f\"  ✗ {entity1:15s} ↔ {entity2:15s} = {similarity:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:01.000492Z","iopub.status.idle":"2025-10-14T18:45:01.000737Z","shell.execute_reply.started":"2025-10-14T18:45:01.000639Z","shell.execute_reply":"2025-10-14T18:45:01.000649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# FINAL KGGEN WITH SEMANTIC CLUSTERING\n# ============================================================================\n\nclass KGGenSemantic:\n    def __init__(self, similarity_threshold=0.75):\n        self.entity_extractor = entity_extractor\n        self.relation_extractor = relation_extractor\n        self.entity_clusterer = SemanticEntityClustering(similarity_threshold=similarity_threshold)\n        self.graph = nx.DiGraph()\n        self.entity_clusters = {}\n        \n    def generate_from_json(self, json_data: List[Dict], max_docs: int = None) -> nx.DiGraph:\n        \"\"\"Generate KG from JSON dataset\"\"\"\n        all_entities = set()\n        all_relations = []\n        \n        if max_docs:\n            json_data = json_data[:max_docs]\n        \n        print(f\"Processing {len(json_data)} documents...\")\n        print(\"=\" * 80)\n        \n        for idx, item in enumerate(json_data):\n            text = item.get('Article Text', '')\n            concepts = item.get('Concept', [])\n            \n            if not text or len(text.strip()) < 5:\n                continue\n            \n            try:\n                # Extract entities\n                entities = self.entity_extractor(text)\n                all_entities.update(entities)\n                \n                # Add concepts\n                for concept in concepts:\n                    if concept and isinstance(concept, str):\n                        all_entities.add(concept.lower().strip())\n                \n                # Extract relations\n                relations = self.relation_extractor(text, list(all_entities))\n                all_relations.extend(relations)\n                \n                if (idx + 1) % 20 == 0:\n                    print(f\"  {idx + 1}/{len(json_data)} docs | {len(all_entities)} entities | {len(all_relations)} relations\")\n                    \n            except Exception as e:\n                continue\n        \n        print(f\"\\n✓ Extraction complete!\")\n        print(f\"  Total entities: {len(all_entities)}\")\n        print(f\"  Total relations: {len(all_relations)}\")\n        \n        # Build graph\n        for subj, pred, obj in all_relations:\n            self.graph.add_edge(subj, obj, relation=pred)\n        \n        print(f\"  Graph nodes: {len(self.graph.nodes())}\")\n        print(f\"  Graph edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def cluster_entities(self):\n        \"\"\"Semantic clustering with embeddings\"\"\"\n        nodes = list(self.graph.nodes())\n        \n        if len(nodes) == 0:\n            print(\"No nodes to cluster!\")\n            return self.graph\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"SEMANTIC CLUSTERING: {len(nodes)} ENTITIES\")\n        print(f\"{'='*80}\")\n        \n        self.entity_clusters = self.entity_clusterer(nodes)\n        \n        # Map entities\n        entity_mapping = {}\n        for cluster_label, cluster_entities in self.entity_clusters.items():\n            for entity in cluster_entities:\n                entity_mapping[entity] = cluster_label\n        \n        # Rebuild graph\n        new_graph = nx.DiGraph()\n        for u, v, data in self.graph.edges(data=True):\n            new_u = entity_mapping.get(u, u)\n            new_v = entity_mapping.get(v, v)\n            relation = data.get('relation', 'related_to')\n            \n            if new_u == new_v:\n                continue\n            \n            if not new_graph.has_edge(new_u, new_v):\n                new_graph.add_edge(new_u, new_v, relation=relation)\n        \n        self.graph = new_graph\n        \n        print(f\"\\n✓ Clustering complete!\")\n        print(f\"  Final nodes: {len(self.graph.nodes())}\")\n        print(f\"  Final edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def save_graph(self, filepath: str):\n        data = nx.node_link_data(self.graph)\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n        print(f\"✓ Saved to {filepath}\")\n    \n    def export_triples(self, filepath: str):\n        triples = []\n        for u, v, data in self.graph.edges(data=True):\n            triples.append({\n                'subject': u,\n                'predicate': data.get('relation', 'related_to'),\n                'object': v\n            })\n        import pandas as pd\n        df = pd.DataFrame(triples)\n        df.to_csv(filepath, index=False)\n        print(f\"✓ Exported to {filepath}\")\n\n# Initialize semantic KGGen (threshold 0.75 = balanced)\nkg_gen_semantic = KGGenSemantic(similarity_threshold=0.75)\nprint(\"\\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:01.001436Z","iopub.status.idle":"2025-10-14T18:45:01.001749Z","shell.execute_reply.started":"2025-10-14T18:45:01.001581Z","shell.execute_reply":"2025-10-14T18:45:01.001596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# GENERATE KNOWLEDGE GRAPH FROM CELLULAR PHONE DATASET\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"GENERATING KNOWLEDGE GRAPH\")\nprint(\"=\" * 80)\n\n# Use the semantic KGGen we just created\nkg_gen = kg_gen_semantic\n\n# Generate from your data (start with 50 documents for testing)\nprint(\"\\nStep 1: Extracting entities and relations...\")\ngraph = kg_gen.generate_from_json(data, max_docs=50)\n\nprint(\"\\nStep 2: Clustering similar entities...\")\nclustered_graph = kg_gen.cluster_entities()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"KNOWLEDGE GRAPH GENERATED!\")\nprint(\"=\" * 80)\nprint(f\"  Final nodes: {len(kg_gen.graph.nodes())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:01.003194Z","iopub.status.idle":"2025-10-14T18:45:01.003408Z","shell.execute_reply.started":"2025-10-14T18:45:01.003306Z","shell.execute_reply":"2025-10-14T18:45:01.003315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# VISUALIZE THE KNOWLEDGE GRAPH\n# ============================================================================\n\n!pip install pyvis --quiet\n\nfrom pyvis.network import Network\nimport networkx as nx\n\ndef visualize_kg_interactive(graph, output_file='/kaggle/working/knowledge_graph.html'):\n    \"\"\"\n    Create an interactive HTML visualization\n    \"\"\"\n    print(f\"Creating interactive visualization...\")\n    print(f\"  Nodes: {len(graph.nodes())}\")\n    print(f\"  Edges: {len(graph.edges())}\")\n    \n    # Create pyvis network\n    net = Network(\n        height='800px',\n        width='100%',\n        bgcolor='#ffffff',\n        font_color='black',\n        directed=True,\n        notebook=False  # Set to False for Kaggle\n    )\n    \n    # Set physics options for better layout\n    net.set_options(\"\"\"\n    {\n      \"physics\": {\n        \"enabled\": true,\n        \"barnesHut\": {\n          \"gravitationalConstant\": -8000,\n          \"centralGravity\": 0.3,\n          \"springLength\": 95,\n          \"springConstant\": 0.04\n        },\n        \"minVelocity\": 0.75\n      },\n      \"nodes\": {\n        \"font\": {\"size\": 16}\n      },\n      \"edges\": {\n        \"font\": {\"size\": 12}\n      }\n    }\n    \"\"\")\n    \n    # Add nodes with size based on degree\n    for node in graph.nodes():\n        degree = graph.degree(node)\n        size = 10 + degree * 3\n        net.add_node(node, label=node, size=size, title=f\"{node}\\nConnections: {degree}\")\n    \n    # Add edges with labels\n    for source, target, data in graph.edges(data=True):\n        relation = data.get('relation', 'related_to')\n        net.add_edge(source, target, label=relation, title=relation)\n    \n    # Save\n    net.save_graph(output_file)\n    print(f\"✓ Interactive visualization saved to: {output_file}\")\n    print(f\"  Download it to open in your browser!\")\n    \n    return net\n\n# Create visualization\nvisualize_kg_interactive(kg_gen.graph)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:01.004352Z","iopub.status.idle":"2025-10-14T18:45:01.004630Z","shell.execute_reply.started":"2025-10-14T18:45:01.004472Z","shell.execute_reply":"2025-10-14T18:45:01.004484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# EXPORT KNOWLEDGE GRAPH DATA\n# ============================================================================\n\n# Export as JSON\nkg_gen.save_graph('/kaggle/working/knowledge_graph.json')\n\n# Export as CSV triples\nkg_gen.export_triples('/kaggle/working/knowledge_graph_triples.csv')\n\n# Print sample triples\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAMPLE KNOWLEDGE GRAPH TRIPLES:\")\nprint(\"=\" * 80)\n\nfor i, (u, v, data) in enumerate(list(kg_gen.graph.edges(data=True))[:15]):\n    relation = data.get('relation', 'related_to')\n    print(f\"{i+1:2d}. ({u}) --[{relation}]--> ({v})\")\n\nprint(\"\\n\" + \"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:01.006139Z","iopub.status.idle":"2025-10-14T18:45:01.006418Z","shell.execute_reply.started":"2025-10-14T18:45:01.006277Z","shell.execute_reply":"2025-10-14T18:45:01.006288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# STATIC MATPLOTLIB VISUALIZATION (For Quick View)\n# ============================================================================\n\nimport matplotlib.pyplot as plt\n\ndef visualize_kg_static(graph, figsize=(16, 12)):\n    \"\"\"Quick static visualization\"\"\"\n    plt.figure(figsize=figsize)\n    \n    # Layout\n    pos = nx.spring_layout(graph, k=0.5, iterations=50, seed=42)\n    \n    # Node sizes based on degree\n    node_sizes = [300 + 50 * graph.degree(node) for node in graph.nodes()]\n    \n    # Draw\n    nx.draw_networkx_nodes(graph, pos, node_size=node_sizes, \n                          node_color='lightblue', alpha=0.7, \n                          edgecolors='darkblue', linewidths=2)\n    \n    nx.draw_networkx_edges(graph, pos, edge_color='gray', \n                          arrows=True, arrowsize=15, \n                          arrowstyle='->', width=1.5, alpha=0.6)\n    \n    nx.draw_networkx_labels(graph, pos, font_size=8, \n                           font_weight='bold', font_color='darkblue')\n    \n    # Edge labels (sample)\n    edge_labels = {}\n    for u, v, data in list(graph.edges(data=True))[:20]:  # Show first 20\n        edge_labels[(u, v)] = data.get('relation', '')[:15]  # Truncate\n    \n    nx.draw_networkx_edge_labels(graph, pos, edge_labels, \n                                 font_size=6, font_color='red')\n    \n    plt.title(\"Knowledge Graph Visualization\", fontsize=16, fontweight='bold')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.savefig('/kaggle/working/kg_static_view.png', dpi=150, bbox_inches='tight')\n    print(\"✓ Static visualization saved to: /kaggle/working/kg_static_view.png\")\n    plt.show()\n\n# Create static visualization\nvisualize_kg_static(kg_gen.graph)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:01.007501Z","iopub.status.idle":"2025-10-14T18:45:01.007744Z","shell.execute_reply.started":"2025-10-14T18:45:01.007646Z","shell.execute_reply":"2025-10-14T18:45:01.007655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# GRAPH STATISTICS\n# ============================================================================\n\nimport pandas as pd\n\nprint(\"KNOWLEDGE GRAPH STATISTICS\")\nprint(\"=\" * 80)\n\n# Basic stats\nprint(f\"\\nBasic Statistics:\")\nprint(f\"  Total nodes: {len(kg_gen.graph.nodes())}\")\nprint(f\"  Total edges: {len(kg_gen.graph.edges())}\")\nprint(f\"  Graph density: {nx.density(kg_gen.graph):.4f}\")\nprint(f\"  Average degree: {sum(dict(kg_gen.graph.degree()).values()) / len(kg_gen.graph.nodes()):.2f}\")\n\n# Top connected nodes\nprint(f\"\\nTop 10 Most Connected Nodes:\")\nprint(\"-\" * 80)\ndegree_dict = dict(kg_gen.graph.degree())\ntop_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n\nfor i, (node, degree) in enumerate(top_nodes, 1):\n    print(f\"  {i:2d}. {node:30s} ({degree} connections)\")\n\n# Relation types\nprint(f\"\\nRelation Types:\")\nprint(\"-\" * 80)\nrelations = [data.get('relation', 'related_to') for _, _, data in kg_gen.graph.edges(data=True)]\nrelation_counts = pd.Series(relations).value_counts()\n\nfor i, (relation, count) in enumerate(relation_counts.head(10).items(), 1):\n    print(f\"  {i:2d}. {relation:30s} ({count} times)\")\n\n# Clustering results\nprint(f\"\\nClustering Results:\")\nprint(\"-\" * 80)\nmulti_clusters = {k: v for k, v in kg_gen.entity_clusters.items() if len(v) > 1}\nprint(f\"  Total entity clusters: {len(kg_gen.entity_clusters)}\")\nprint(f\"  Multi-entity clusters: {len(multi_clusters)}\")\nprint(f\"  Singleton entities: {len(kg_gen.entity_clusters) - len(multi_clusters)}\")\n\nif multi_clusters:\n    print(f\"\\nMulti-entity clusters found:\")\n    for label, members in multi_clusters.items():\n        print(f\"  - {members}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ All outputs saved to /kaggle/working/\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:45:01.008543Z","iopub.status.idle":"2025-10-14T18:45:01.008825Z","shell.execute_reply.started":"2025-10-14T18:45:01.008679Z","shell.execute_reply":"2025-10-14T18:45:01.008690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}