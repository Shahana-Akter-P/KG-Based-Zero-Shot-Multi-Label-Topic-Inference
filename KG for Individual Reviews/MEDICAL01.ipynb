{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13468504,"sourceType":"datasetVersion","datasetId":8549797},{"sourceId":13524188,"sourceType":"datasetVersion","datasetId":8587369}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:51.672171Z","iopub.execute_input":"2025-11-11T13:25:51.672406Z","iopub.status.idle":"2025-11-11T13:25:51.700917Z","shell.execute_reply.started":"2025-11-11T13:25:51.672383Z","shell.execute_reply":"2025-11-11T13:25:51.700317Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/keywords/Keyword_Medical.json\n/kaggle/input/keywords/Keyword_Canon.json\n/kaggle/input/keywords/Keyword_Creative.json\n/kaggle/input/keywords/Keyword_Apex.json\n/kaggle/input/keywords/Keyword_Apex 1.json\n/kaggle/input/keywords/Keyword_Nokia.json\n/kaggle/input/keywords/Keyword_News.json\n/kaggle/input/keywords/Keyword_Nikon.json\n/kaggle/input/zstikg/NewsConcept Data-set.json\n/kaggle/input/zstikg/DVD playerData-set.json\n/kaggle/input/zstikg/MedicalConcept Data-set.json\n/kaggle/input/zstikg/Cellular phone Data-set.json\n/kaggle/input/zstikg/Digital camera2 Data-set.json\n/kaggle/input/zstikg/Mp3 playerData-set.json\n/kaggle/input/zstikg/Digital camera1 Data-set.json\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ============================================================================\n# PART 1: INSTALLATION AND SETUP\n# ============================================================================\n\n# Install required packages\n!pip install dspy-ai huggingface_hub networkx sentence-transformers pandas --quiet\n\nprint(\"✓ Packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:51.702199Z","iopub.execute_input":"2025-11-11T13:25:51.702444Z","iopub.status.idle":"2025-11-11T13:25:55.379991Z","shell.execute_reply.started":"2025-11-11T13:25:51.702422Z","shell.execute_reply":"2025-11-11T13:25:55.379194Z"}},"outputs":[{"name":"stdout","text":"✓ Packages installed successfully!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ============================================================================\n# PART 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom collections import defaultdict\nimport pandas as pd\nimport os\nfrom huggingface_hub import InferenceClient\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:55.380996Z","iopub.execute_input":"2025-11-11T13:25:55.381297Z","iopub.status.idle":"2025-11-11T13:25:55.387116Z","shell.execute_reply.started":"2025-11-11T13:25:55.381271Z","shell.execute_reply":"2025-11-11T13:25:55.386250Z"}},"outputs":[{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: CONFIGURE HUGGING FACE WITH DSPY.LM (PROPER WAY)\n# ============================================================================\n\nos.environ['HUGGINGFACE_API_KEY'] = 'KEY'\n\n# Use DSPy's built-in LM class with the correct prefix\nlm = dspy.LM(\n    model='huggingface/meta-llama/Llama-3.1-8B-Instruct',\n    api_key=os.environ['HUGGINGFACE_API_KEY'],\n    max_tokens=12000,\n    temperature=0.3\n)\n\ndspy.settings.configure(lm=lm)\nprint(\"✓ Llama-3.1-8B-Instruct configured successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:55.388972Z","iopub.execute_input":"2025-11-11T13:25:55.389211Z","iopub.status.idle":"2025-11-11T13:25:55.400955Z","shell.execute_reply.started":"2025-11-11T13:25:55.389195Z","shell.execute_reply":"2025-11-11T13:25:55.400325Z"}},"outputs":[{"name":"stdout","text":"✓ Llama-3.1-8B-Instruct configured successfully!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ============================================================================\n# PART 1: DATA LOADING FUNCTION\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"\n    Load JSON data from file\n    \n    Expected format:\n    [\n        {\n            \"Article Title\": [],\n            \"Article Text\": \"text here...\",\n            \"Concept\": [\"concept1\", \"concept2\"]\n        },\n        ...\n    ]\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\n# Test with sample data\nsample_data = [\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"excellent phone, excellent service.\",\n        \"Concept\": []\n    },\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"i am a business user who heavily depend on mobile service.\",\n        \"Concept\": [\"service\"]\n    }\n]\n\nprint(\"✓ Sample data ready for testing\")\nprint(f\"  Sample has {len(sample_data)} documents\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:55.401796Z","iopub.execute_input":"2025-11-11T13:25:55.401977Z","iopub.status.idle":"2025-11-11T13:25:55.414817Z","shell.execute_reply.started":"2025-11-11T13:25:55.401963Z","shell.execute_reply":"2025-11-11T13:25:55.414068Z"}},"outputs":[{"name":"stdout","text":"✓ Sample data ready for testing\n  Sample has 2 documents\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ============================================================================\n# LOAD ONE SPECIFIC DATASET\n# ============================================================================\n\n# Choose which dataset you want to work with:\n# Option 1: Cellular phone\nfilepath = '/kaggle/input/zstikg/MedicalConcept Data-set.json'\n\n# Option 2: News\n# filepath = '/kaggle/input/zsltikg/NewsConcept Data-set.json'\n\n# Option 3: Medical\n# filepath = '/kaggle/input/zsltikg/MedicalConcept Data-set.json'\n\n# Load the data\ndata = load_json_data(filepath)\n\nprint(f\"✓ Loaded {len(data)} documents\")\nprint(f\"\\nFirst document preview:\")\nprint(f\"  Keys: {list(data[0].keys())}\")\nprint(f\"  Text: {data[0].get('Article Text', '')}...\")\nprint(f\"  Concepts: {data[0].get('Concept', [])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:55.415677Z","iopub.execute_input":"2025-11-11T13:25:55.416404Z","iopub.status.idle":"2025-11-11T13:25:55.481685Z","shell.execute_reply.started":"2025-11-11T13:25:55.416376Z","shell.execute_reply":"2025-11-11T13:25:55.480877Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 2066 documents from /kaggle/input/zstikg/MedicalConcept Data-set.json\n✓ Loaded 2066 documents\n\nFirst document preview:\n  Keys: ['Article Title', 'Article Text', 'Concept']\n  Text: 1. Health care reform\n\nHow could the health care reform legislation that President Barack Obama signed into law on March 23, 2010, not be the #1 story of the year? Whether you are for or against it, the Patient Protection and Affordable Care Act is nothing if not ambitious, and if implemented, it will fundamentally alter how American health care is financed and perhaps delivered. The law is designed to patch holes in the health insurance system and extend coverage to 32 million Americans by 2019 while also reining in health care spending, which now accounts for more than 17% of the country’s gross domestic product. The biggest changes aren’t scheduled to occur until 2014, when most people will be required to have health insurance or pay a penalty (the so-called individual mandate) and when state-level health insurance exchanges should be in place. The Medicaid program is also scheduled to be expanded that year so that it covers more people, and subsidized insurance will be available through the exchanges for people in lower- and middle-income brackets. But plenty is happening before 2014. The 1,000-page law contains hundreds of provisions, and they’re being rolled out in phases. This year, for example, the law created high-risk pools for people with pre-existing conditions, required health plans to extend coverage to adult children up to age 26, and imposed a 10% tax on indoor tanning salons. Next year, about 20 different provisions are scheduled to take effect, including the elimination of copayments for many preventive services for Medicare enrollees, the imposition of limits on non-medical spending by health plans, and the creation of a voluntary insurance that will help pay for home health care and other long-term care services received outside a nursing home. Getting a handle on the complicated law is difficult. If you’re looking for a short course, the Kaiser Family Foundation has created an excellent timeline of the law’s implementation (we depended on it for this post) and a short (nine minutes) animated video that’s one of the best (and most amusing) overviews available. The big question now is whether the sweeping health care law can survive various legal and political challenges. In December, a federal judge in Virginia ruled that the individual mandate was unconstitutional. Meanwhile, congressional Republicans have vowed to thwart the legislation, and if the party were to win the White House and control of the Senate in the 2012 election, Republicans would be in a position to follow through on their threats to repeal it.\n\n2. Smartphones, medical apps, and remote monitoring\n\nSmartphones and tablet computers are making it easier to get health care information, advice, and reminders on an anywhere-and-anytime basis. Hundreds of health and medical apps for smartphones like the iPhone became available this year. Some are just for fun. Others provide useful information (calorie counters, first aid and CPR instructions) or perform calculations. Even the federal government is getting into the act: the app store it opened this summer has several free health-related apps, including one called My Dietary Supplements for keeping track of vitamins and supplements and another one from the Environmental Protection Agency that allows you to check the UV index and air quality wherever you are. Smartphones are also being used with at-home monitoring devices; for example, glucose meters have been developed that send blood sugar readings wirelessly to an app on a smartphone. The number of doctors using apps and mobile devices is increasing, a trend that is likely to accelerate as electronic health records become more common. Check out iMedicalapps if you want to see the apps your doctor might be using or talking about. It has become a popular Web site for commentary and critiques of medical apps for doctors and medical students. Meanwhile, the FDA is wrestling with the issue of how tightly it should regulate medical apps. Some adverse events resulting from programming errors have been reported to the agency. Medical apps are part of a larger “e-health” trend toward delivering health care reminders and advice remotely with the help of computers and phones of all types. These phone services are being used in combination with increasingly sophisticated at-home monitoring devices. Research results have been mixed. Simple, low-cost text messages have been shown to be effective in getting people wear sunscreen. But one study published this year found that regular telephone contact and at-home monitoring of heart failure patients had no effect on hospitalizations of death from any cause over a six-month period. Another study found that remote monitoring did lower hospital readmission rates among heart failure patients, although the difference between remote monitoring and regular care didn’t reach statistical significance.\n\n3. New CPR guidelines\n\nThe American Heart Association issued new guidelines for cardiopulmonary resuscitation (CPR) this year that continue the trend toward simplifying CPR and emphasizing chest compressions. For trained rescuers, the guidelines change the CPR sequence from airway, breathing, and chest compressions and the A-B-C mnemonic to putting chest compressions first, followed by checks of the airway and breathing, or C-A-B. People who haven’t had CPR training are encouraged to do only chest compressions because they are easier and “more readily guided by dispatchers over the telephone.” The compressions should be fast (about 100 times a minute) and hard (so that the chest goes down by two inches or more). The American Heart Association produced a very good video about the guidelines that’s well worth watching. Fewer than half of those who suffer cardiac arrest receive CPR, so the hope is that more people will give CPR—and do so quickly— if it’s simpler and doesn’t involve mouth-to-mouth breathing. The guidelines note that the results for chest compression–only CPR are similar to those for traditional CPR for adults in cardiac arrest outside of the hospital. But conventional CPR is still better for children because cardiac arrest in children is usually preceded by a lack of breathing, so the mouth-to-mouth breaths are needed to restore oxygen levels in the blood. Research results reported this year in The Lancet, Journal of the American Medical Association, and The New England Journal of Medicine all suggested that in most cardiac arrest cases, chest compression–only CPR is as effective, if not more so, than conventional CPR.\n\n4. Making stem cells\n\nHeart attacks, strokes, and many other conditions destroy cells, and for years, scientists believed that it was impossible to make replacements. Then, four years ago, a Japanese researcher, Shinya Yamanaka, discovered a technique for reprogramming cells back into stem cells, so that they would function like a biological blank slate and be capable of turning into any other type of cell. Dr. Yamanka called his creations induced pluripotent stem cells, and a whole new frontier of stem cell research opened up. Scientists have since figured out ways to turn one cell type directly into another type: blood vessel cells have been turned into bone and fat cells, and skin cells have been turned into blood cells. And this year, stem cell research took another leap forward when a Harvard researcher, Derrick Rossi, reported results demonstrating a technique that may make the creation of induced pluripotent stem cells a lot easier and safer. Rossi and his colleagues at the Harvard Stem Cell Institute reprogrammed adult skin cells with synthetic messenger RNA that leaves DNA intact, instead of inserting genes into DNA. Research with embryonic stem cells remains important, and in October 2010, Geron, a California biotech company, began enrolling people in a trial to test the safety of using cells derived from embryonic stem cells to treat spinal cord injuries. But researchers are also making some remarkable progress toward turning readily available cells, such as skin or blood cells, into other types of cells. These new cells would be genetically identical to other cells in the body and therefore shouldn’t be rejected by the immune system when they’re transplanted to replace cells lost to disease.\n\n5. Heightened awareness of concussions\n\nConcern about sports-related concussions, especially in football, has been growing as evidence has increased that repeated concussions can cause permanent brain damage over the long term, even if the short-term effects are fairly mild (most concussions do not result in a loss of consciousness, for example) and CT and MRI scans are normal. Some researchers are calling concussion-related brain damage chronic traumatic encephalopathy (encephalopathy is a catchall term for any degenerative disease of the brain). There were several efforts in 2010 to reduce the number and severity of concussions. The National Football League started to fine players for illegal hits this season. The American Academy of Neurology came out with a position paper that says any athlete who might have suffered a concussion shouldn’t be allowed to partcipate again until he or she has been evaluated by a doctor with training in the evalulation and management of sports concussions. The American Academy of Pediatrics released a report about sports-related concussions in children and adolescents that says younger people often need more time (7 to 10 days or even longer) to recover from a concussion than college or professional athletes. Several states have passed laws requiring high schools to have concussion management programs. The concussion risk is greatest for football players, but girl basketball and soccer players also have relatively high rates. Meanwhile, research into concussions continues. Boston University researchers who have been prominent in the field caused a stir with a finding that linked concussions and chronic traumatic encephalopathy to amyotrophic lateral sclerosis (Lou Gherig’s disease).\n\n6. An anti-aging possibility\n\nResearchers at the Harvard-affliliated Dana-Farber Cancer Institute reported results this year that kindled hopes for altering the fundamental biology of aging. Their experiment involved mice that had been genetically engineered so that an enzyme called telomerase that is known to be important in the aging of cells could be turned on and off. When the enzyme was turned off, the mice aged prematurely. When they reached the chronological equivalent of adolescence, they appeared to be biologically very old: their brains and other organs had shrunk and were starting to fail. Then the scientists turned on the enzyme. Promptly, the brain and other shrunken organs started to grow with new cells, and organ failure ceased. The animals recovered their sense of smell. You might say the mice became adolescents again. Of course, what works in mice doesn’t always work in humans. There are concerns that the activation of telomerase could cause cancer, although that didn’t occur in this particular experiment. And this is very much an experimental finding; at this point, all those products making anti-aging claims are way ahead of the game and not to be trusted. Still, along with other research, this study hints at a future when it might be possible to slow down biological aging and possibly prevent some of the diseases associated with it.\n\n7. CT scans for lung cancer screening\n\nMore Americans die from lung cancer than from any other type of cancer, yet there’s no accepted screening test for the disease. Study results reported this year may change that situation. The National Cancer Institute (NCI) stopped the National Lung Screening Trial comparing CT scans to chest x-rays earlier than expected because the CT scans appeared to be so effective at reducing lung cancer deaths. The trial included 53,000 current and former heavy (30 pack years or more) smokers. Results released in October showed that over a five-year period, 354 of those screened with CT scans died from lung cancer (or about 1.4%) compared with 442 of those screened with chest x-rays (about 1.7%). Catching any cancer at an earlier, more treatable stage is an appealing idea, and especially lung cancer, because of its high mortality rate. But screening tests have become more controversial lately because of concerns that they lead to overdiagnosis and overtreatment. Almost one in every four people who were screened with CT scans in the National Lung Cancer Screening Trial had a false positive, the finding of an abnormality that turns out not to be cancer. There are also concerns about radiation exposure from CT scans and whether scans for lung cancer will add to that problem.\n\n8. New vitamin D guidelines\n\nAfter years of debate, discussion, and research, the Institute of Medicine (IOM) issued new vitamin D guidelines this year. The Recommended Dietary Allowance (RDA) is now 600 international units (IU) a day for people ages 1 to 70 and 800 IU a day for those 71 and older. The previous guidelines, set in 1997, recommended a daily intake of 200 IU through age 50, 400 IU between the ages of 51 and 70, and 600 IU for those 71 and older. The IOM panel also established a new safe upper limit of 4,000 IU a day, double the old limit of 2,000 IU. The new guidelines were criticized as being too conservative by many experts, who would have preferred an RDA closer to 1,000 IU a day and a blood level target of 30 ng/ml (75 nmol/l) for the vitamin, not the 20 ng/ml (50 nmol/l) set by the IOM panel. The difference of opinion stems, in part, from the fact that the IOM panel gave results from randomized clincial trials (RCTs) far more weight than results from other types of studies. As a result, the panel found evidence that vitamin D benefits bone and little else. If other kinds of studies are taken into account, a case can be made that blood levels of 30 ng/ml or even higher would result in optimal bone health and that the vitamin has a wide range of health benefits beyond bone, including protective effects against some cancers (especially colon cancer) and some autoimmune disorders. The debate about vitamin D is bound to continue. Soon after the IOM panel released its report, a different set of experts, the U.S. Preventive Services Task Force, came out with fall-prevention recommendations that include an endorsement of vitamin D.\n\n9. Alternatives to warfarin\n\nThe FDA approved one alternative to warfarin this year, a drug called dabigatran (sold as Pradaxa). Another alternative, rivaroxaban, seems to be waiting in the wings after largely favorable results were reported this year (here and here) from trials testing the drug in patients with deep-vein thrombosis and atrial fibrillation. A third drug, apixaban, which is related to rivaroxaban, is also looking promising. Warfarin (the brand-name version is called Coumadin) has been the mainstay for preventing blood clots for decades, but it’s a tricky, high-maintenance drug that requires frequent blood tests to make sure the dose is producing the desired results: enough anti-coagulation to prevent blood clots but not so much as to cause bleeding . Warfarin also interacts with many foods and drugs. In contrast, these warfarin alternatives seem simple as pie: they can be given in fixed doses, don’t require blood monitoring, and don’t seem to pose interteaction problems. Cost, however, will be a barrier. Drug companies set high prices for new brand-name drugs. Warfarin, widely available as a generic, is relatively cheap. And there’s always the possibility of unforeseen side effects once the new drugs are more widely used. Still, millions of people stand to benefit if good alternatives to difficult-to-use warfarin pan out.\n\n10. Concerns about bisphosphonates\n\nBisphosphonates are prescribed to prevent and treat osteoporosis, a decrease in bone density that makes fractures more likely. Well-known brands include Fosamax (alendronate) and Actonel (risedronate). Millions of people, most of them postmenopausal women, take bisphosphonates; for the most part, they are safe and effective medications that have been shown to cut the risk of fractures by 50%. But concerns about ill effects from long-term use have been growing. In October, the FDA issued a new warning about bisphosphonates increasing the risk of a rare kind of thighbone (femur) fracture. Two years ago, the agency issued a different warning about the bone drugs causing bone, joint, and muscle pain. There have also been reports about a small percentage of bisphosphonate users developing osteonecrosis in their jawbones, although most of those cases have occurred in cancer patients who have received high intravenous doses (bisphosphonates can relieve pain and strengthen bone if cancer has spread to the bone). Some doctors are now recommending “drug holidays” for people who take bisphosphontes for osteoporosis for extended periods. Other bone-building drugs, such as denosumab (sold as Prolia), which was approved by the FDA this year, may get a closer look because of concerns about the side effects of bisphosphonates. And perhaps the nonpharmacological ways to strengthen bones will gain some adherents. Pill-free bone builders include a regimen of regular weight-bearing exercise and adequte intake of calicum and vitamin D (see item #8)....\n  Concepts: []\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ============================================================================\n# PROCESS YOUR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"PROCESSING ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\n# data is now correctly loaded as a list of dicts\nindividual_articles = []\n\nfor idx, item in enumerate(data):\n    article = {\n        'id': idx,\n        'Article Text': item['Article Text'],\n        #'Concept': item['Concept'] if item['Concept'] else []\n    }\n    individual_articles.append(article)\n\nprint(f\"\\n✓ Created list of {len(individual_articles)} individual articles\")\n\n# Show first 3\nprint(f\"\\nFirst 3 articles:\")\nprint(\"-\" * 80)\nfor i in range(min(3, len(individual_articles))):\n    article = individual_articles[i]\n    print(f\"\\nArticle {article['id']}:\")\n    print(f\"  Text: {article['Article Text'][:80]}...\")\n    #print(f\"  Concepts: {article['Concept']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:55.482408Z","iopub.execute_input":"2025-11-11T13:25:55.482575Z","iopub.status.idle":"2025-11-11T13:25:55.490346Z","shell.execute_reply.started":"2025-11-11T13:25:55.482562Z","shell.execute_reply":"2025-11-11T13:25:55.489496Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPROCESSING ALL THE ARTICLES\n================================================================================\n\n✓ Created list of 2066 individual articles\n\nFirst 3 articles:\n--------------------------------------------------------------------------------\n\nArticle 0:\n  Text: 1. Health care reform\n\nHow could the health care reform legislation that Preside...\n\nArticle 1:\n  Text: My colleagues at Harvard Health Publishing and I have a mission: to provide accu...\n\nArticle 2:\n  Text: Does echinacea, the popular natural cold remedy, really work?\n\nIt depends on wha...\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: DEFINE SIGNATURES\n# ============================================================================\n\nclass EntityExtractor(dspy.Signature):\n    \"\"\"Extract key entities from the given text. Extracted entities are nouns, \n    verbs, or adjectives, particularly regarding sentiment. This is for an \n    extraction task, please be thorough and accurate to the reference text.\n    \n    Return ONLY a valid JSON list format: [\"entity1\", \"entity2\", \"entity3\"]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract entities from\")\n    entities = dspy.OutputField(desc=\"List of extracted entities in JSON format\")\n\nclass RelationExtractor(dspy.Signature):\n    \"\"\"Extract subject-predicate-object triples from the assistant message. \n    A predicate (1-3 words) defines the relationship between the subject and \n    object. Relationship may be fact or sentiment based on assistant's message. \n    Subject and object are entities. Entities provided are from the assistant \n    message and prior conversation history, though you may not need all of them. \n    This is for an extraction task, please be thorough, accurate, and faithful \n    to the reference text.\n    \n    Return ONLY valid JSON format: [[\"subject1\", \"predicate1\", \"object1\"], [\"subject2\", \"predicate2\", \"object2\"]]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract relations from\")\n    entities = dspy.InputField(desc=\"List of available entities\")\n    triples = dspy.OutputField(desc=\"List of [subject, predicate, object] triples in JSON format\")\n\nprint(\"✓ Signatures defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:55.491286Z","iopub.execute_input":"2025-11-11T13:25:55.492020Z","iopub.status.idle":"2025-11-11T13:25:55.505584Z","shell.execute_reply.started":"2025-11-11T13:25:55.491996Z","shell.execute_reply":"2025-11-11T13:25:55.504886Z"}},"outputs":[{"name":"stdout","text":"✓ Signatures defined\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: CREATE ENTITY EXTRACTOR\n# ============================================================================\n\nclass ExtractEntities(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(EntityExtractor)\n    \n    def forward(self, text: str) -> List[str]:\n        if not text or len(text.strip()) < 3:\n            return []\n            \n        result = self.extract(text=text)\n        \n        try:\n            entities_text = result.entities.strip()\n            \n            if '[' in entities_text and ']' in entities_text:\n                start = entities_text.find('[')\n                end = entities_text.rfind(']') + 1\n                entities_text = entities_text[start:end]\n            \n            entities = json.loads(entities_text)\n            \n            if isinstance(entities, list):\n                return [str(e).lower().strip() for e in entities if e and len(str(e).strip()) > 1]\n            return []\n            \n        except:\n            try:\n                entities_text = result.entities.strip()\n                if entities_text.startswith('['):\n                    entities_text = entities_text[1:]\n                if entities_text.endswith(']'):\n                    entities_text = entities_text[:-1]\n                \n                entities = []\n                for item in entities_text.split(','):\n                    item = item.strip(' \"\\'\\n\\t')\n                    if item and len(item) > 1:\n                        entities.append(item.lower())\n                \n                return entities[:50]\n            except:\n                return []\n\nentity_extractor = ExtractEntities()\nprint(\"✓ Entity Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:55.506205Z","iopub.execute_input":"2025-11-11T13:25:55.506385Z","iopub.status.idle":"2025-11-11T13:25:55.521019Z","shell.execute_reply.started":"2025-11-11T13:25:55.506371Z","shell.execute_reply":"2025-11-11T13:25:55.520450Z"}},"outputs":[{"name":"stdout","text":"✓ Entity Extractor created\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: CREATE RELATION EXTRACTOR\n# ============================================================================\n\nclass ExtractRelations(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(RelationExtractor)\n    \n    def forward(self, text: str, entities: List[str]) -> List[Tuple[str, str, str]]:\n        if not entities or not text:\n            return []\n        \n        entities_subset = entities[:30]\n        entities_str = json.dumps(entities_subset)\n        \n        result = self.extract(text=text, entities=entities_str)\n        \n        try:\n            triples_text = result.triples.strip()\n            \n            if '[' in triples_text and ']' in triples_text:\n                start = triples_text.find('[')\n                end = triples_text.rfind(']') + 1\n                triples_text = triples_text[start:end]\n            \n            triples = json.loads(triples_text)\n            \n            normalized_triples = []\n            for triple in triples:\n                if isinstance(triple, (list, tuple)) and len(triple) == 3:\n                    s, p, o = triple\n                    s = str(s).lower().strip()\n                    p = str(p).lower().strip()\n                    o = str(o).lower().strip()\n                    \n                    if s and p and o and s != o:\n                        normalized_triples.append((s, p, o))\n            \n            return normalized_triples\n            \n        except Exception as e:\n            return []\n\nrelation_extractor = ExtractRelations()\nprint(\"✓ Relation Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:55.523233Z","iopub.execute_input":"2025-11-11T13:25:55.523661Z","iopub.status.idle":"2025-11-11T13:25:55.540130Z","shell.execute_reply.started":"2025-11-11T13:25:55.523645Z","shell.execute_reply":"2025-11-11T13:25:55.539544Z"}},"outputs":[{"name":"stdout","text":"✓ Relation Extractor created\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ============================================================================\n# CLUSTERING SIGNATURES (DEFINE FIRST!)\n# ============================================================================\n\nclass ClusterValidator(dspy.Signature):\n    \"\"\"Verify if these entities belong in the same cluster.\n    A cluster should contain entities that are the same in meaning, with different:\n    - tenses, plural forms, stem forms, upper/lower cases\n    Or entities with close semantic meanings.\n    \n    Return ONLY valid JSON format: [\"entity1\", \"entity2\", \"entity3\"]\n    Return only entities you are confident belong together.\n    If not confident, return empty list [].\n    \"\"\"\n    \n    entities = dspy.InputField(desc=\"Entities to validate\")\n    valid_cluster = dspy.OutputField(desc=\"Validated cluster in JSON format\")\n\nprint(\"✓ ClusterValidator Signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:55.540678Z","iopub.execute_input":"2025-11-11T13:25:55.540873Z","iopub.status.idle":"2025-11-11T13:25:55.553239Z","shell.execute_reply.started":"2025-11-11T13:25:55.540854Z","shell.execute_reply":"2025-11-11T13:25:55.552504Z"}},"outputs":[{"name":"stdout","text":"✓ ClusterValidator Signature defined\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ============================================================================\n# SEMANTIC SIMILARITY CLUSTERING (FROM PAPER)\n# ============================================================================\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nclass SemanticEntityClustering(dspy.Module):\n    def __init__(self, similarity_threshold=0.75):\n        super().__init__()\n        self.validator = dspy.ChainOfThought(ClusterValidator)\n        \n        # Load embedding model (same as paper)\n        print(\"Loading sentence transformer model...\")\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        print(\"✓ Model loaded\")\n        \n        self.similarity_threshold = similarity_threshold\n    \n    def _parse_cluster(self, text: str) -> List[str]:\n        \"\"\"Parse cluster from LLM response\"\"\"\n        try:\n            text = text.strip()\n            if '[' in text and ']' in text:\n                start = text.find('[')\n                end = text.rfind(']') + 1\n                text = text[start:end]\n            \n            cluster = json.loads(text)\n            if isinstance(cluster, list):\n                return [str(e).lower().strip() for e in cluster if e]\n            return []\n        except:\n            return []\n    \n    def _get_semantic_clusters(self, entities: List[str]) -> List[List[str]]:\n        \"\"\"Group entities by semantic similarity using embeddings\"\"\"\n        \n        if len(entities) == 0:\n            return []\n        \n        # Get embeddings for all entities\n        embeddings = self.model.encode(entities)\n        \n        # Compute pairwise cosine similarity\n        similarity_matrix = cosine_similarity(embeddings)\n        \n        # Find clusters using similarity threshold\n        clusters = []\n        remaining = set(range(len(entities)))\n        \n        for i in range(len(entities)):\n            if i not in remaining:\n                continue\n            \n            # Find all entities similar to this one\n            cluster_indices = [i]\n            remaining.discard(i)\n            \n            for j in range(i + 1, len(entities)):\n                if j not in remaining:\n                    continue\n                \n                # Check if similar enough\n                if similarity_matrix[i][j] >= self.similarity_threshold:\n                    cluster_indices.append(j)\n                    remaining.discard(j)\n            \n            # Convert indices to entity names\n            cluster = [entities[idx] for idx in cluster_indices]\n            \n            # Only keep clusters with 2-4 entities\n            if 2 <= len(cluster) <= 4:\n                clusters.append(cluster)\n            elif len(cluster) == 1:\n                # Keep singletons for later\n                pass\n        \n        return clusters\n    \n    def forward(self, entities: List[str]) -> Dict[str, List[str]]:\n        \"\"\"Semantic clustering with LLM validation\"\"\"\n        \n        print(f\"Starting semantic clustering with {len(entities)} entities...\")\n        print(f\"  Similarity threshold: {self.similarity_threshold}\")\n        \n        # Remove duplicates\n        unique_entities = list(set(entities))\n        \n        # Step 1: Find semantic clusters using embeddings\n        print(\"  Computing semantic similarities...\")\n        potential_clusters = self._get_semantic_clusters(unique_entities)\n        \n        print(f\"  Found {len(potential_clusters)} potential clusters\")\n        \n        # Step 2: Validate with LLM\n        validated_clusters = {}\n        cluster_id = 0\n        clustered_entities = set()\n        \n        for cluster in potential_clusters:\n            try:\n                # Ask LLM to validate\n                validation = self.validator(entities=json.dumps(cluster))\n                validated = self._parse_cluster(validation.valid_cluster)\n                \n                if validated and len(validated) >= 2:\n                    cluster_label = validated[0]\n                    validated_clusters[cluster_label] = validated\n                    \n                    for entity in validated:\n                        clustered_entities.add(entity)\n                    \n                    print(f\"  ✓ Cluster {cluster_id}: {validated}\")\n                    cluster_id += 1\n                else:\n                    # LLM rejected - add as singletons\n                    for entity in cluster:\n                        if entity not in clustered_entities:\n                            validated_clusters[entity] = [entity]\n                            clustered_entities.add(entity)\n            except:\n                # Error - add as singletons\n                for entity in cluster:\n                    if entity not in clustered_entities:\n                        validated_clusters[entity] = [entity]\n                        clustered_entities.add(entity)\n        \n        # Step 3: Add all remaining entities as singletons\n        for entity in unique_entities:\n            if entity not in clustered_entities:\n                validated_clusters[entity] = [entity]\n        \n        multi = sum(1 for v in validated_clusters.values() if len(v) > 1)\n        print(f\"✓ Semantic clustering complete: {len(validated_clusters)} total clusters\")\n        print(f\"  Multi-entity clusters: {multi}\")\n        print(f\"  Singleton entities: {len(validated_clusters) - multi}\")\n        \n        return validated_clusters\n\n# Create semantic clusterer with different thresholds\nentity_clusterer_semantic = SemanticEntityClustering(similarity_threshold=0.75)\nprint(\"\\n✓ Semantic Entity Clustering Module created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:55.553931Z","iopub.execute_input":"2025-11-11T13:25:55.554158Z","iopub.status.idle":"2025-11-11T13:25:56.490043Z","shell.execute_reply.started":"2025-11-11T13:25:55.554135Z","shell.execute_reply":"2025-11-11T13:25:56.489261Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n✓ Model loaded\n\n✓ Semantic Entity Clustering Module created\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ============================================================================\n# FINAL KGGEN WITH SEMANTIC CLUSTERING\n# ============================================================================\n\nclass KGGenSemantic:\n    def __init__(self, similarity_threshold=0.75):\n        self.entity_extractor = entity_extractor\n        self.relation_extractor = relation_extractor\n        self.entity_clusterer = SemanticEntityClustering(similarity_threshold=similarity_threshold)\n        self.graph = nx.DiGraph()\n        self.entity_clusters = {}\n        \n    def generate_from_json(self, json_data: List[Dict], max_docs: int = None) -> nx.DiGraph:\n        \"\"\"Generate KG from JSON dataset\"\"\"\n        all_entities = set()\n        all_relations = []\n        \n        if max_docs:\n            json_data = json_data[:max_docs]\n        \n        print(f\"Processing {len(json_data)} documents...\")\n        print(\"=\" * 80)\n        \n        for idx, item in enumerate(json_data):\n            text = item.get('Article Text', '')\n            concepts = item.get('Concept', [])\n            \n            if not text or len(text.strip()) < 5:\n                continue\n            \n            try:\n                # Extract entities\n                entities = self.entity_extractor(text)\n                all_entities.update(entities)\n                \n                # Add concepts\n                #for concept in concepts:\n                    #if concept and isinstance(concept, str):\n                        #all_entities.add(concept.lower().strip())\n                \n                # Extract relations\n                relations = self.relation_extractor(text, list(all_entities))\n                all_relations.extend(relations)\n                \n                if (idx + 1) % 20 == 0:\n                    print(f\"  {idx + 1}/{len(json_data)} docs | {len(all_entities)} entities | {len(all_relations)} relations\")\n                    \n            except Exception as e:\n                continue\n        \n        print(f\"\\n✓ Extraction complete!\")\n        print(f\"  Total entities: {len(all_entities)}\")\n        print(f\"  Total relations: {len(all_relations)}\")\n        \n        # Build graph\n        for subj, pred, obj in all_relations:\n            self.graph.add_edge(subj, obj, relation=pred)\n        \n        print(f\"  Graph nodes: {len(self.graph.nodes())}\")\n        print(f\"  Graph edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def cluster_entities(self):\n        \"\"\"Semantic clustering with embeddings\"\"\"\n        nodes = list(self.graph.nodes())\n        \n        if len(nodes) == 0:\n            print(\"No nodes to cluster!\")\n            return self.graph\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"SEMANTIC CLUSTERING: {len(nodes)} ENTITIES\")\n        print(f\"{'='*80}\")\n        \n        self.entity_clusters = self.entity_clusterer(nodes)\n        \n        # Map entities\n        entity_mapping = {}\n        for cluster_label, cluster_entities in self.entity_clusters.items():\n            for entity in cluster_entities:\n                entity_mapping[entity] = cluster_label\n        \n        # Rebuild graph\n        new_graph = nx.DiGraph()\n        for u, v, data in self.graph.edges(data=True):\n            new_u = entity_mapping.get(u, u)\n            new_v = entity_mapping.get(v, v)\n            relation = data.get('relation', 'related_to')\n            \n            if new_u == new_v:\n                continue\n            \n            if not new_graph.has_edge(new_u, new_v):\n                new_graph.add_edge(new_u, new_v, relation=relation)\n        \n        self.graph = new_graph\n        \n        print(f\"\\n✓ Clustering complete!\")\n        print(f\"  Final nodes: {len(self.graph.nodes())}\")\n        print(f\"  Final edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def save_graph(self, filepath: str):\n        data = nx.node_link_data(self.graph)\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n        print(f\"✓ Saved to {filepath}\")\n    \n    def export_triples(self, filepath: str):\n        triples = []\n        for u, v, data in self.graph.edges(data=True):\n            triples.append({\n                'subject': u,\n                'predicate': data.get('relation', 'related_to'),\n                'object': v\n            })\n        import pandas as pd\n        df = pd.DataFrame(triples)\n        df.to_csv(filepath, index=False)\n        print(f\"✓ Exported to {filepath}\")\n\n# Initialize semantic KGGen (threshold 0.75 = balanced)\nkg_gen_semantic = KGGenSemantic(similarity_threshold=0.75)\nprint(\"\\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:56.490894Z","iopub.execute_input":"2025-11-11T13:25:56.491134Z","iopub.status.idle":"2025-11-11T13:25:57.242514Z","shell.execute_reply.started":"2025-11-11T13:25:56.491117Z","shell.execute_reply":"2025-11-11T13:25:57.241742Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n✓ Model loaded\n\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# ============================================================================\n# GENERATE KG FOR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\nall_article_kgs = []\n\ntotal = len(individual_articles)\nprint(f\"\\nProcessing {total} articles...\\n\")\n\nfor idx, article in enumerate(individual_articles):\n    article_id = article['id']\n    text = article['Article Text']\n    #concepts = article['Concept']\n    \n    # Skip empty articles\n    if not text or len(text.strip()) < 5:\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n        continue\n    \n    try:\n        # Extract entities for THIS article\n        entities = entity_extractor(text)\n        \n        # Add original concepts as entities\n        #for concept in concepts:\n            #if concept and isinstance(concept, str):\n                #entities.append(concept.lower().strip())\n        \n        entities = list(set(entities))  # Remove duplicates\n        \n        # Extract relations for THIS article\n        if entities:\n            relations = relation_extractor(text, entities)\n        else:\n            relations = []\n        \n        # Build graph for THIS article\n        graph = nx.DiGraph()\n        for subj, pred, obj in relations:\n            graph.add_edge(subj, obj, relation=pred)\n        \n        # Store everything for this article\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': graph,\n            'entities': entities,\n            'relations': relations,\n            'num_nodes': len(graph.nodes()),\n            'num_edges': len(graph.edges())\n        })\n        \n        # Progress update\n        if (idx + 1) % 50 == 0:\n            print(f\"✓ Processed {idx + 1}/{total} articles...\")\n        \n    except Exception as e:\n        print(f\"✗ Article {article_id}: Error - {str(e)}\")\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n\nprint(f\"\\n\" + \"=\" * 80)\nprint(\"✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\")\nprint(\"=\" * 80)\nprint(f\"Total articles processed: {len(all_article_kgs)}\")\nprint(f\"Articles with graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] > 0)}\")\nprint(f\"Articles without graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] == 0)}\")\n\n# Statistics\ntotal_entities = sum(len(kg['entities']) for kg in all_article_kgs)\ntotal_relations = sum(len(kg['relations']) for kg in all_article_kgs)\n\nprint(f\"\\nTotal entities extracted: {total_entities}\")\nprint(f\"Total relations extracted: {total_relations}\")\n\nwith_graphs = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\nif with_graphs:\n    avg_nodes = sum(kg['num_nodes'] for kg in with_graphs) / len(with_graphs)\n    avg_edges = sum(kg['num_edges'] for kg in with_graphs) / len(with_graphs)\n    #print(f\"\\nAverage nodes per KG: {avg_nodes:.2f}\")\n    #print(f\"Average edges per KG: {avg_edges:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T13:25:57.243350Z","iopub.execute_input":"2025-11-11T13:25:57.243624Z","iopub.status.idle":"2025-11-11T15:24:00.915282Z","shell.execute_reply.started":"2025-11-11T13:25:57.243601Z","shell.execute_reply":"2025-11-11T15:24:00.914469Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nGENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\n================================================================================\n\nProcessing 2066 articles...\n\n✓ Processed 50/2066 articles...\n✓ Processed 100/2066 articles...\n✓ Processed 150/2066 articles...\n✓ Processed 200/2066 articles...\n✓ Processed 250/2066 articles...\n✓ Processed 300/2066 articles...\n✓ Processed 350/2066 articles...\n✓ Processed 400/2066 articles...\n✓ Processed 450/2066 articles...\n✓ Processed 500/2066 articles...\n✓ Processed 550/2066 articles...\n✓ Processed 600/2066 articles...\n✓ Processed 650/2066 articles...\n✓ Processed 700/2066 articles...\n✓ Processed 750/2066 articles...\n✓ Processed 800/2066 articles...\n✓ Processed 850/2066 articles...\n✓ Processed 900/2066 articles...\n✓ Processed 950/2066 articles...\n✓ Processed 1000/2066 articles...\n✓ Processed 1050/2066 articles...\n✓ Processed 1100/2066 articles...\n✓ Processed 1150/2066 articles...\n✓ Processed 1200/2066 articles...\n✓ Processed 1250/2066 articles...\n✓ Processed 1300/2066 articles...\n✓ Processed 1350/2066 articles...\n✓ Processed 1400/2066 articles...\n✓ Processed 1450/2066 articles...\n✓ Processed 1500/2066 articles...\n✓ Processed 1550/2066 articles...\n✓ Processed 1600/2066 articles...\n✓ Processed 1650/2066 articles...\n✓ Processed 1700/2066 articles...\n✓ Processed 1750/2066 articles...\n✓ Processed 1800/2066 articles...\n✓ Processed 1850/2066 articles...\n✓ Processed 1900/2066 articles...\n✓ Processed 1950/2066 articles...\n✓ Processed 2000/2066 articles...\n✗ Article 2022: Error - litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"Requested token count exceeds the model's maximum context length of 16384 tokens. You requested a total of 21391 tokens: 9391 tokens from the input messages and 12000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\",\"type\":\"BadRequestError\"}\n\n✗ Article 2023: Error - litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"Requested token count exceeds the model's maximum context length of 16384 tokens. You requested a total of 21391 tokens: 9391 tokens from the input messages and 12000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\",\"type\":\"BadRequestError\"}\n\n✓ Processed 2050/2066 articles...\n✗ Article 2050: Error - litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"Requested token count exceeds the model's maximum context length of 16384 tokens. You requested a total of 21391 tokens: 9391 tokens from the input messages and 12000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\",\"type\":\"BadRequestError\"}\n\n✗ Article 2051: Error - litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"Requested token count exceeds the model's maximum context length of 16384 tokens. You requested a total of 21391 tokens: 9391 tokens from the input messages and 12000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\",\"type\":\"BadRequestError\"}\n\n\n================================================================================\n✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\n================================================================================\nTotal articles processed: 2066\nArticles with graphs: 2050\nArticles without graphs: 16\n\nTotal entities extracted: 65763\nTotal relations extracted: 29673\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom dspy.signatures import Signature\nfrom dspy import OutputField, InputField\nimport pandas as pd\nimport os\nfrom collections import Counter\nimport numpy as np\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:24:03.454363Z","iopub.status.idle":"2025-11-11T15:24:03.454579Z","shell.execute_reply.started":"2025-11-11T15:24:03.454477Z","shell.execute_reply":"2025-11-11T15:24:03.454487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SAVE KNOWLEDGE GRAPHS TO JSON FILE\n# ============================================================================\n\nimport json\nimport networkx as nx\n\ndef save_knowledge_graphs(all_article_kgs, filepath='/kaggle/working/article_knowledge_graphs.json'):\n    \"\"\"\n    Save knowledge graphs to JSON file.\n    \n    Args:\n        all_article_kgs: List of dictionaries containing KG information\n        filepath: Path to save the JSON file\n    \"\"\"\n    # Convert NetworkX graphs to serializable format\n    serializable_kgs = []\n    \n    for kg_data in all_article_kgs:\n        # Convert NetworkX graph to node-link format\n        graph = kg_data['graph']\n        graph_dict = nx.node_link_data(graph) if graph.number_of_nodes() > 0 else None\n        \n        serializable_kg = {\n            'id': kg_data['id'],\n            'text': kg_data['text'],\n            'entities': kg_data['entities'],\n            'relations': kg_data['relations'],\n            'num_nodes': kg_data['num_nodes'],\n            'num_edges': kg_data['num_edges'],\n            'graph_data': graph_dict  # Serialized graph\n        }\n        \n        serializable_kgs.append(serializable_kg)\n    \n    # Save to JSON\n    with open(filepath, 'w', encoding='utf-8') as f:\n        json.dump(serializable_kgs, f, indent=2, ensure_ascii=False)\n    \n    print(f\"\\n{'='*80}\")\n    print(\"KNOWLEDGE GRAPHS SAVED\")\n    print(f\"{'='*80}\")\n    print(f\"Saved {len(serializable_kgs)} knowledge graphs to:\")\n    print(f\"  {filepath}\")\n    print(f\"\\nFile size: {len(json.dumps(serializable_kgs)) / (1024*1024):.2f} MB\")\n    print(f\"Articles with graphs: {sum(1 for kg in serializable_kgs if kg['num_edges'] > 0)}\")\n    print(f\"Articles without graphs: {sum(1 for kg in serializable_kgs if kg['num_edges'] == 0)}\")\n\n# Save the knowledge graphs\nsave_knowledge_graphs(all_article_kgs, '/kaggle/working/article_knowledge_graphs.json')\n\nprint(\"\\n✓ Knowledge graphs saved successfully!\")\nprint(\"You can now proceed to the topic classification step.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:24:00.916063Z","iopub.execute_input":"2025-11-11T15:24:00.916274Z","iopub.status.idle":"2025-11-11T15:24:01.769209Z","shell.execute_reply.started":"2025-11-11T15:24:00.916258Z","shell.execute_reply":"2025-11-11T15:24:01.768339Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nKNOWLEDGE GRAPHS SAVED\n================================================================================\nSaved 2066 knowledge graphs to:\n  /kaggle/working/article_knowledge_graphs.json\n\nFile size: 15.81 MB\nArticles with graphs: 2050\nArticles without graphs: 16\n\n✓ Knowledge graphs saved successfully!\nYou can now proceed to the topic classification step.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: EXTRACT ALL UNIQUE CONCEPTS FROM DATASET (FIXED)\n# ============================================================================\n\ndef extract_unique_concepts(articles: List[Dict]) -> List[str]:\n    \"\"\"\n    Extract all unique concepts from the dataset's 'Concept' field.\n    \n    Args:\n        articles: List of article dictionaries\n    \n    Returns:\n        Sorted list of unique concepts\n    \"\"\"\n    all_concepts = set()\n    \n    for article in articles:\n        # Get concepts from the Concept field\n        if 'Concept' in article and article['Concept']:\n            if isinstance(article['Concept'], list):\n                all_concepts.update([c.lower().strip() for c in article['Concept'] if c])\n            elif isinstance(article['Concept'], str):\n                all_concepts.add(article['Concept'].lower().strip())\n    \n    # Convert to sorted list\n    unique_concepts = sorted(list(all_concepts))\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"UNIQUE CONCEPTS EXTRACTED FROM DATASET\")\n    print(f\"{'='*80}\")\n    print(f\"Total unique concepts: {len(unique_concepts)}\")\n    print(f\"\\nFirst 20 concepts: {unique_concepts[:20]}\")\n    print(f\"\\nLast 20 concepts: {unique_concepts[-20:]}\")\n    \n    return unique_concepts\n\n\n# FIX: Use 'data' instead of 'articles' (or rename 'data' to 'articles')\n# Option 1: Extract concepts from 'data'\nall_concepts = extract_unique_concepts(data)\n\n# Option 2: If you want to use 'articles', just rename 'data':\n# articles = data\n# all_concepts = extract_unique_concepts(articles)\n\n# Save concept list for reference\nwith open('/kaggle/working/all_concepts.json', 'w') as f:\n    json.dump(all_concepts, f, indent=2)\n\nprint(f\"\\n✓ Saved concept list to /kaggle/working/all_concepts.json\")\nprint(f\"✓ Ready for topic classification with {len(all_concepts)} concepts\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:24:01.770155Z","iopub.execute_input":"2025-11-11T15:24:01.770895Z","iopub.status.idle":"2025-11-11T15:24:01.780514Z","shell.execute_reply.started":"2025-11-11T15:24:01.770866Z","shell.execute_reply":"2025-11-11T15:24:01.779463Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nUNIQUE CONCEPTS EXTRACTED FROM DATASET\n================================================================================\nTotal unique concepts: 18\n\nFirst 20 concepts: ['addiction', 'alcohol', 'arthritis', 'brain and cognitive health', 'breast cancer', 'cancer', \"children's health\", 'exercise and fitness', 'headache', 'healthy eating', 'heart health', 'mental health', 'osteoporosis', 'pain management', 'prostate knowledge', 'sleep', 'smoking cessation', \"women's health\"]\n\nLast 20 concepts: ['addiction', 'alcohol', 'arthritis', 'brain and cognitive health', 'breast cancer', 'cancer', \"children's health\", 'exercise and fitness', 'headache', 'healthy eating', 'heart health', 'mental health', 'osteoporosis', 'pain management', 'prostate knowledge', 'sleep', 'smoking cessation', \"women's health\"]\n\n✓ Saved concept list to /kaggle/working/all_concepts.json\n✓ Ready for topic classification with 18 concepts\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: LOAD DATA AND EXISTING KNOWLEDGE GRAPHS\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"Load JSON data from file\"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\ndef load_knowledge_graphs(filepath: str) -> List[Dict]:\n    \"\"\"Load previously created knowledge graphs\"\"\"\n    if not os.path.exists(filepath):\n        print(f\"\\n⚠️  WARNING: Knowledge graph file not found at {filepath}\")\n        return None\n    \n    with open(filepath, 'r', encoding='utf-8') as f:\n        kgs = json.load(f)\n    print(f\"✓ Loaded {len(kgs)} knowledge graphs from {filepath}\")\n    return kgs\n\n# Load your data\narticles = load_json_data('/kaggle/input/zstikg/MedicalConcept Data-set.json')\nknowledge_graphs = load_knowledge_graphs('/kaggle/working/article_knowledge_graphs.json')\n\n# Only classify articles that have KGs\narticles_with_kgs = articles[:len(knowledge_graphs)] \n\nif knowledge_graphs is None:\n    print(\"\\n❌ Cannot proceed without knowledge graphs. Please save them first.\")\nelse:\n    print(f\"\\n✓ Successfully loaded {len(articles)} articles and {len(knowledge_graphs)} KGs\")\n    print(\"✓ Ready to proceed with topic classification!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:24:01.781320Z","iopub.execute_input":"2025-11-11T15:24:01.782106Z","iopub.status.idle":"2025-11-11T15:24:03.379384Z","shell.execute_reply.started":"2025-11-11T15:24:01.782085Z","shell.execute_reply":"2025-11-11T15:24:03.378659Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 2066 documents from /kaggle/input/zstikg/MedicalConcept Data-set.json\n✓ Loaded 2066 knowledge graphs from /kaggle/working/article_knowledge_graphs.json\n\n✓ Successfully loaded 2066 articles and 2066 KGs\n✓ Ready to proceed with topic classification!\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: KG FORMATTING FUNCTION\n# ============================================================================\n\ndef format_kg_for_llm(kg_data: Dict) -> str:\n    \"\"\"Format knowledge graph into readable text for the LLM.\"\"\"\n    if not kg_data or kg_data.get('num_edges', 0) == 0:\n        return \"No knowledge graph available.\"\n    \n    # Format entities\n    entities = kg_data.get('entities', [])\n    entities_str = \", \".join(entities[:30]) if entities else \"None\"\n    if len(entities) > 30:\n        entities_str += f\"... ({len(entities) - 30} more)\"\n    \n    # Format relationships\n    relations = kg_data.get('relations', [])\n    if relations:\n        relationships = []\n        for relation in relations[:20]:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                relationships.append(f\"{source} --[{rel_type}]--> {target}\")\n        relationships_str = \"\\n\".join(relationships)\n        if len(relations) > 20:\n            relationships_str += f\"\\n... ({len(relations) - 20} more relationships)\"\n    else:\n        relationships_str = \"None\"\n    \n    kg_summary = f\"\"\"Knowledge Graph Summary:\nEntities: {entities_str}\n\nKey Relationships:\n{relationships_str}\"\"\"\n    \n    return kg_summary\n\nprint(\"✓ KG formatting function ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:24:03.380253Z","iopub.execute_input":"2025-11-11T15:24:03.380462Z","iopub.status.idle":"2025-11-11T15:24:03.387631Z","shell.execute_reply.started":"2025-11-11T15:24:03.380444Z","shell.execute_reply":"2025-11-11T15:24:03.386751Z"}},"outputs":[{"name":"stdout","text":"✓ KG formatting function ready\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: DEFINE DSPY SIGNATURE FOR TOPIC CLASSIFICATION\n# ============================================================================\n\nfrom dspy import Signature, InputField, OutputField  # ✅ Add this import\n\nclass TopicClassification(Signature):\n    \"\"\"Given an article text, its knowledge graph, and a list of possible topics,\n    determine which topics (can be multiple) are most relevant to this article.\n    Return only topic names that are actually present in the available_topics list.\"\"\"\n    \n    article_text: str = InputField(\n        desc=\"The text content of the article\"\n    )\n    knowledge_graph: str = InputField(\n        desc=\"Knowledge graph extracted from the article showing entities and relationships\"\n    )\n    available_topics: str = InputField(\n        desc=\"Comma-separated list of all possible topic/concept names\"\n    )\n    \n    predicted_topics: str = OutputField(\n        desc=\"Comma-separated list of relevant topics from the available_topics list. \"\n             \"Only return topics that actually appear in the available_topics list. \"\n             \"If multiple topics apply, list them all (maximum 5). If no topics match well, return 'none'.\"\n    )\n\nprint(\"✓ Topic classification signature defined\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:36:17.885530Z","iopub.execute_input":"2025-11-11T15:36:17.886090Z","iopub.status.idle":"2025-11-11T15:36:17.892249Z","shell.execute_reply.started":"2025-11-11T15:36:17.886068Z","shell.execute_reply":"2025-11-11T15:36:17.891454Z"}},"outputs":[{"name":"stdout","text":"✓ Topic classification signature defined\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# ============================================================================\n# STEP 7: CREATE TOPIC CLASSIFIER CLASS\n# ============================================================================\n\nclass ArticleTopicClassifier:\n    \"\"\"Multi-label topic classifier using article text, KG, and available topics.\"\"\"\n    \n    def __init__(self, available_topics: List[str]):\n        self.available_topics = available_topics\n        self.available_topics_str = \", \".join(available_topics)\n        self.classifier = dspy.ChainOfThought(TopicClassification)\n        print(f\"✓ Classifier initialized with {len(available_topics)} possible topics\")\n    \n    def classify_article(self, article_text: str, kg_data: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        kg_summary = format_kg_for_llm(kg_data)\n        \n        # Truncate article if too long\n        #max_text_length = 500\n        #if len(article_text) > max_text_length:\n            #article_text = article_text[:max_text_length] + \"...\"\n        \n        try:\n            result = self.classifier(\n                article_text=article_text,\n                knowledge_graph=kg_summary,\n                available_topics=self.available_topics_str\n            )\n            \n            predicted_topics_raw = result.predicted_topics\n            \n            if predicted_topics_raw.lower() == 'none':\n                predicted_topics = []\n            else:\n                predicted_topics = [\n                    t.strip().lower() \n                    for t in predicted_topics_raw.split(',')\n                    if t.strip()\n                ]\n                predicted_topics = [\n                    t for t in predicted_topics \n                    if t in [at.lower() for at in self.available_topics]\n                ]\n            \n            return {\n                'predicted_topics': predicted_topics,\n                'num_topics': len(predicted_topics),\n                #'confidence': result.confidence,\n                #'reasoning': result.reasoning\n            }\n            \n        except Exception as e:\n            print(f\"  Error: {str(e)}\")\n            return {\n                'predicted_topics': [],\n                'num_topics': 0,\n                #'confidence': 'error',\n                #'reasoning': f'Classification failed: {str(e)}'\n            }\n    \n    def classify_dataset(self, articles: List[Dict], knowledge_graphs: List[Dict],\n                        max_articles: int = None) -> List[Dict]:\n        \"\"\"Classify multiple articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        if max_articles:\n            articles = articles[:max_articles]\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(articles)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx, article in enumerate(articles):\n            print(f\"Processing article {idx}...\", end=\" \")\n            \n            article_text = article.get('Article Text', '')\n            kg_data = kg_dict.get(idx, {'num_edges': 0})\n            classification = self.classify_article(article_text, kg_data)\n            \n            result = {\n                'article_id': idx,\n                'article_text': article_text + \"...\",\n                'true_concepts': article.get('Concept', []),\n                'predicted_topics': classification['predicted_topics'],\n                'num_predicted': classification['num_topics'],\n                #'confidence': classification['confidence'],\n                #'reasoning': classification['reasoning']\n            }\n            \n            results.append(result)\n            print(f\"✓ Found {len(classification['predicted_topics'])} topics\")\n        \n        return results\n\nprint(\"✓ Classifier class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:36:21.309598Z","iopub.execute_input":"2025-11-11T15:36:21.310268Z","iopub.status.idle":"2025-11-11T15:36:21.320514Z","shell.execute_reply.started":"2025-11-11T15:36:21.310242Z","shell.execute_reply":"2025-11-11T15:36:21.319676Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier class defined\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# ============================================================================\n# STEP 8: ANALYSIS FUNCTIONS\n# ============================================================================\n\ndef analyze_classification_results(results: List[Dict], available_topics: List[str]) -> Dict:\n    \"\"\"Analyze classification results and generate statistics.\"\"\"\n    from collections import Counter\n    \n    topic_counts = Counter()\n    #confidence_dist = Counter()\n    \n    multi_label_count = 0\n    no_label_count = 0\n    \n    for result in results:\n        predicted = result['predicted_topics']\n        \n        if len(predicted) == 0:\n            no_label_count += 1\n        elif len(predicted) > 1:\n            multi_label_count += 1\n        \n        for topic in predicted:\n            topic_counts[topic] += 1\n        \n        #confidence_dist[result['confidence']] += 1\n    \n    stats = {\n        'total_articles': len(results),\n        'articles_with_topics': len(results) - no_label_count,\n        'articles_without_topics': no_label_count,\n        'multi_label_articles': multi_label_count,\n        #'avg_topics_per_article': sum(len(r['predicted_topics']) for r in results) / len(results),\n        #'most_common_topics': topic_counts.most_common(10)\n    }\n    \n    print(f\"\\n{'='*80}\")\n    print(\"CLASSIFICATION RESULTS SUMMARY\")\n    print(f\"{'='*80}\")\n    print(f\"\\nTotal articles: {stats['total_articles']}\")\n    print(f\"Articles with topics: {stats['articles_with_topics']} ({stats['articles_with_topics']/stats['total_articles']*100:.1f}%)\")\n    print(f\"Multi-label articles: {stats['multi_label_articles']} ({stats['multi_label_articles']/stats['total_articles']*100:.1f}%)\")\n    #print(f\"Average topics per article: {stats['avg_topics_per_article']:.2f}\")\n    \n    print(f\"\\n{'='*80}\")\n    print(\"TOP 10 MOST FREQUENT TOPICS\")\n    print(f\"{'='*80}\")\n    #for topic, count in stats['most_common_topics']:\n        #print(f\"  {topic}: {count} articles ({count/stats['total_articles']*100:.1f}%)\")\n    \n    return stats\n\ndef save_results(results: List[Dict], output_path: str):\n    \"\"\"Save classification results to JSON file.\"\"\"\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(results, f, indent=2, ensure_ascii=False)\n    print(f\"\\n✓ Results saved to {output_path}\")\n\ndef export_to_csv(results: List[Dict], output_path: str):\n    \"\"\"Export results to CSV for easy analysis.\"\"\"\n    rows = []\n    for r in results:\n        rows.append({\n            'article_id': r['article_id'],\n            #'article_preview': r['article_text'],\n            'true_concepts': '|'.join(r['true_concepts']) if r['true_concepts'] else '',\n            'predicted_topics': '|'.join(r['predicted_topics']),\n            'num_predicted': len(r['predicted_topics']),\n            #'confidence': r['confidence'],\n            #'reasoning': r['reasoning']\n        })\n    \n    df = pd.DataFrame(rows)\n    df.to_csv(output_path, index=False, encoding='utf-8')\n    print(f\"✓ CSV exported to {output_path}\")\n\nprint(\"✓ Analysis functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:36:25.691488Z","iopub.execute_input":"2025-11-11T15:36:25.692071Z","iopub.status.idle":"2025-11-11T15:36:25.702052Z","shell.execute_reply.started":"2025-11-11T15:36:25.692045Z","shell.execute_reply":"2025-11-11T15:36:25.701080Z"}},"outputs":[{"name":"stdout","text":"✓ Analysis functions defined\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# ============================================================================\n# RUN CLASSIFICATION ON ALL THE ARTICLES\n# ============================================================================\n\n# Initialize classifier\nclassifier = ArticleTopicClassifier(all_concepts)\n\n# Classify all the articles with KGs\nresults = classifier.classify_dataset(\n    articles=articles,\n    knowledge_graphs=knowledge_graphs,\n    max_articles=10000\n)\n\n# Analyze results\nstats = analyze_classification_results(results, all_concepts)\n\n# Save results\nsave_results(results, '/kaggle/working/topic_classification_results.json')\nexport_to_csv(results, '/kaggle/working/topic_classification_results.csv')\n\n# Show sample results\nprint(f\"\\n{'='*80}\")\nprint(\"SAMPLE RESULTS (First 3)\")\nprint(f\"{'='*80}\\n\")\n\nfor result in results[:3]:\n    print(f\"Article {result['article_id']}:\")\n    print(f\"  Preview: {result['article_text']}\")\n    print(f\"  True: {result['true_concepts']}\")\n    print(f\"  Predicted: {result['predicted_topics']}\")\n    #print(f\"  Confidence: {result['confidence']}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:36:28.765860Z","iopub.execute_input":"2025-11-11T15:36:28.766141Z","iopub.status.idle":"2025-11-11T17:40:06.690452Z","shell.execute_reply.started":"2025-11-11T15:36:28.766119Z","shell.execute_reply":"2025-11-11T17:40:06.689627Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier initialized with 18 possible topics\n\n================================================================================\nCLASSIFYING 2066 ARTICLES\n================================================================================\n\nProcessing article 0...   Error: litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"Requested token count exceeds the model's maximum context length of 16384 tokens. You requested a total of 16631 tokens: 4631 tokens from the input messages and 12000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\",\"type\":\"BadRequestError\"}\n\n✓ Found 0 topics\nProcessing article 1... ✓ Found 1 topics\nProcessing article 2... ✓ Found 1 topics\nProcessing article 3... ✓ Found 4 topics\nProcessing article 4... ✓ Found 2 topics\nProcessing article 5... ✓ Found 5 topics\nProcessing article 6... ✓ Found 1 topics\nProcessing article 7... ✓ Found 5 topics\nProcessing article 8... ✓ Found 5 topics\nProcessing article 9... ✓ Found 5 topics\nProcessing article 10...   Error: litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"Requested token count exceeds the model's maximum context length of 16384 tokens. You requested a total of 16631 tokens: 4631 tokens from the input messages and 12000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\",\"type\":\"BadRequestError\"}\n\n✓ Found 0 topics\nProcessing article 11... ✓ Found 1 topics\nProcessing article 12... ✓ Found 1 topics\nProcessing article 13... ✓ Found 4 topics\nProcessing article 14... ✓ Found 2 topics\nProcessing article 15... ✓ Found 5 topics\nProcessing article 16... ✓ Found 1 topics\nProcessing article 17... ✓ Found 5 topics\nProcessing article 18... ✓ Found 5 topics\nProcessing article 19... ✓ Found 5 topics\nProcessing article 20... ✓ Found 5 topics\nProcessing article 21... ✓ Found 3 topics\nProcessing article 22... ✓ Found 2 topics\nProcessing article 23... ✓ Found 5 topics\nProcessing article 24... ✓ Found 2 topics\nProcessing article 25... ✓ Found 2 topics\nProcessing article 26... ✓ Found 4 topics\nProcessing article 27... ✓ Found 3 topics\nProcessing article 28... ✓ Found 2 topics\nProcessing article 29... ✓ Found 2 topics\nProcessing article 30... ✓ Found 1 topics\nProcessing article 31... ✓ Found 5 topics\nProcessing article 32... ✓ Found 1 topics\nProcessing article 33... ✓ Found 5 topics\nProcessing article 34... ✓ Found 1 topics\nProcessing article 35... ✓ Found 2 topics\nProcessing article 36... ✓ Found 3 topics\nProcessing article 37... ✓ Found 1 topics\nProcessing article 38... ✓ Found 2 topics\nProcessing article 39... ✓ Found 2 topics\nProcessing article 40... ✓ Found 0 topics\nProcessing article 41... ✓ Found 2 topics\nProcessing article 42... ✓ Found 3 topics\nProcessing article 43... ✓ Found 1 topics\nProcessing article 44... ✓ Found 3 topics\nProcessing article 45... ✓ Found 2 topics\nProcessing article 46... ✓ Found 1 topics\nProcessing article 47... ✓ Found 2 topics\nProcessing article 48... ✓ Found 3 topics\nProcessing article 49... ✓ Found 3 topics\nProcessing article 50... ✓ Found 4 topics\nProcessing article 51... ✓ Found 5 topics\nProcessing article 52... ✓ Found 3 topics\nProcessing article 53... ✓ Found 3 topics\nProcessing article 54... ✓ Found 2 topics\nProcessing article 55... ✓ Found 5 topics\nProcessing article 56... ✓ Found 2 topics\nProcessing article 57... ✓ Found 2 topics\nProcessing article 58... ✓ Found 2 topics\nProcessing article 59... ✓ Found 2 topics\nProcessing article 60... ✓ Found 3 topics\nProcessing article 61... ✓ Found 5 topics\nProcessing article 62... ✓ Found 3 topics\nProcessing article 63... ✓ Found 3 topics\nProcessing article 64... ✓ Found 3 topics\nProcessing article 65... ✓ Found 2 topics\nProcessing article 66... ✓ Found 1 topics\nProcessing article 67... ✓ Found 1 topics\nProcessing article 68... ✓ Found 2 topics\nProcessing article 69... ✓ Found 2 topics\nProcessing article 70... ✓ Found 1 topics\nProcessing article 71... ✓ Found 3 topics\nProcessing article 72... ✓ Found 3 topics\nProcessing article 73... ✓ Found 0 topics\nProcessing article 74... ✓ Found 3 topics\nProcessing article 75... ✓ Found 0 topics\nProcessing article 76... ✓ Found 4 topics\nProcessing article 77... ✓ Found 4 topics\nProcessing article 78... ✓ Found 2 topics\nProcessing article 79... ✓ Found 2 topics\nProcessing article 80... ✓ Found 1 topics\nProcessing article 81... ✓ Found 3 topics\nProcessing article 82... ✓ Found 3 topics\nProcessing article 83... ✓ Found 0 topics\nProcessing article 84... ✓ Found 3 topics\nProcessing article 85... ✓ Found 0 topics\nProcessing article 86... ✓ Found 4 topics\nProcessing article 87... ✓ Found 4 topics\nProcessing article 88... ✓ Found 2 topics\nProcessing article 89... ✓ Found 5 topics\nProcessing article 90... ✓ Found 1 topics\nProcessing article 91... ✓ Found 2 topics\nProcessing article 92... ✓ Found 1 topics\nProcessing article 93... ✓ Found 4 topics\nProcessing article 94... ✓ Found 0 topics\nProcessing article 95... ✓ Found 0 topics\nProcessing article 96... ✓ Found 1 topics\nProcessing article 97... ✓ Found 1 topics\nProcessing article 98... ✓ Found 4 topics\nProcessing article 99... ✓ Found 5 topics\nProcessing article 100... ✓ Found 5 topics\nProcessing article 101... ✓ Found 3 topics\nProcessing article 102... ✓ Found 3 topics\nProcessing article 103... ✓ Found 3 topics\nProcessing article 104... ✓ Found 4 topics\nProcessing article 105... ✓ Found 5 topics\nProcessing article 106... ✓ Found 0 topics\nProcessing article 107... ✓ Found 3 topics\nProcessing article 108... ✓ Found 5 topics\nProcessing article 109... ✓ Found 2 topics\nProcessing article 110... ✓ Found 3 topics\nProcessing article 111... ✓ Found 5 topics\nProcessing article 112... ✓ Found 3 topics\nProcessing article 113... ✓ Found 3 topics\nProcessing article 114... ✓ Found 2 topics\nProcessing article 115... ✓ Found 1 topics\nProcessing article 116... ✓ Found 3 topics\nProcessing article 117... ✓ Found 2 topics\nProcessing article 118... ✓ Found 5 topics\nProcessing article 119... ✓ Found 5 topics\nProcessing article 120... ✓ Found 1 topics\nProcessing article 121... ✓ Found 2 topics\nProcessing article 122... ✓ Found 2 topics\nProcessing article 123... ✓ Found 3 topics\nProcessing article 124... ✓ Found 4 topics\nProcessing article 125... ✓ Found 2 topics\nProcessing article 126... ✓ Found 2 topics\nProcessing article 127... ✓ Found 1 topics\nProcessing article 128... ✓ Found 4 topics\nProcessing article 129... ✓ Found 2 topics\nProcessing article 130... ✓ Found 1 topics\nProcessing article 131... ✓ Found 5 topics\nProcessing article 132... ✓ Found 0 topics\nProcessing article 133... ✓ Found 5 topics\nProcessing article 134... ✓ Found 1 topics\nProcessing article 135... ✓ Found 5 topics\nProcessing article 136... ✓ Found 5 topics\nProcessing article 137... ✓ Found 1 topics\nProcessing article 138... ✓ Found 5 topics\nProcessing article 139... ✓ Found 0 topics\nProcessing article 140... ✓ Found 0 topics\nProcessing article 141... ✓ Found 1 topics\nProcessing article 142... ✓ Found 2 topics\nProcessing article 143... ✓ Found 4 topics\nProcessing article 144... ✓ Found 1 topics\nProcessing article 145... ✓ Found 1 topics\nProcessing article 146... ✓ Found 2 topics\nProcessing article 147... ✓ Found 3 topics\nProcessing article 148... ✓ Found 3 topics\nProcessing article 149... ✓ Found 2 topics\nProcessing article 150... ✓ Found 4 topics\nProcessing article 151... ✓ Found 5 topics\nProcessing article 152... ✓ Found 4 topics\nProcessing article 153... ✓ Found 1 topics\nProcessing article 154... ✓ Found 1 topics\nProcessing article 155... ✓ Found 5 topics\nProcessing article 156... ✓ Found 1 topics\nProcessing article 157... ✓ Found 2 topics\nProcessing article 158... ✓ Found 2 topics\nProcessing article 159... ✓ Found 4 topics\nProcessing article 160... ✓ Found 5 topics\nProcessing article 161... ✓ Found 0 topics\nProcessing article 162... ✓ Found 2 topics\nProcessing article 163... ✓ Found 5 topics\nProcessing article 164... ✓ Found 2 topics\nProcessing article 165... ✓ Found 4 topics\nProcessing article 166... ✓ Found 3 topics\nProcessing article 167... ✓ Found 1 topics\nProcessing article 168... ✓ Found 5 topics\nProcessing article 169... ✓ Found 4 topics\nProcessing article 170... ✓ Found 0 topics\nProcessing article 171... ✓ Found 1 topics\nProcessing article 172... ✓ Found 3 topics\nProcessing article 173... ✓ Found 3 topics\nProcessing article 174... ✓ Found 2 topics\nProcessing article 175... ✓ Found 2 topics\nProcessing article 176... ✓ Found 2 topics\nProcessing article 177... ✓ Found 5 topics\nProcessing article 178... ✓ Found 4 topics\nProcessing article 179... ✓ Found 3 topics\nProcessing article 180... ✓ Found 3 topics\nProcessing article 181... ✓ Found 1 topics\nProcessing article 182... ✓ Found 3 topics\nProcessing article 183... ✓ Found 0 topics\nProcessing article 184... ✓ Found 2 topics\nProcessing article 185... ✓ Found 2 topics\nProcessing article 186... ✓ Found 1 topics\nProcessing article 187... ✓ Found 0 topics\nProcessing article 188... ✓ Found 3 topics\nProcessing article 189... ✓ Found 3 topics\nProcessing article 190... ✓ Found 4 topics\nProcessing article 191... ✓ Found 5 topics\nProcessing article 192... ✓ Found 5 topics\nProcessing article 193... ✓ Found 4 topics\nProcessing article 194... ✓ Found 3 topics\nProcessing article 195... ✓ Found 2 topics\nProcessing article 196... ✓ Found 1 topics\nProcessing article 197... ✓ Found 5 topics\nProcessing article 198... ✓ Found 3 topics\nProcessing article 199... ✓ Found 2 topics\nProcessing article 200... ✓ Found 5 topics\nProcessing article 201... ✓ Found 3 topics\nProcessing article 202... ✓ Found 5 topics\nProcessing article 203... ✓ Found 2 topics\nProcessing article 204... ✓ Found 5 topics\nProcessing article 205... ✓ Found 5 topics\nProcessing article 206... ✓ Found 1 topics\nProcessing article 207... ✓ Found 0 topics\nProcessing article 208... ✓ Found 0 topics\nProcessing article 209... ✓ Found 3 topics\nProcessing article 210... ✓ Found 0 topics\nProcessing article 211... ✓ Found 5 topics\nProcessing article 212... ✓ Found 2 topics\nProcessing article 213... ✓ Found 2 topics\nProcessing article 214... ✓ Found 3 topics\nProcessing article 215... ✓ Found 5 topics\nProcessing article 216... ✓ Found 0 topics\nProcessing article 217... ✓ Found 2 topics\nProcessing article 218... ✓ Found 2 topics\nProcessing article 219... ✓ Found 0 topics\nProcessing article 220... ✓ Found 2 topics\nProcessing article 221... ✓ Found 1 topics\nProcessing article 222... ✓ Found 5 topics\nProcessing article 223... ✓ Found 0 topics\nProcessing article 224... ✓ Found 1 topics\nProcessing article 225... ✓ Found 3 topics\nProcessing article 226... ✓ Found 5 topics\nProcessing article 227... ✓ Found 3 topics\nProcessing article 228... ✓ Found 1 topics\nProcessing article 229... ✓ Found 3 topics\nProcessing article 230... ✓ Found 4 topics\nProcessing article 231... ✓ Found 2 topics\nProcessing article 232... ✓ Found 0 topics\nProcessing article 233... ✓ Found 2 topics\nProcessing article 234... ✓ Found 5 topics\nProcessing article 235... ✓ Found 2 topics\nProcessing article 236... ✓ Found 5 topics\nProcessing article 237... ✓ Found 2 topics\nProcessing article 238... ✓ Found 3 topics\nProcessing article 239... ✓ Found 1 topics\nProcessing article 240... ✓ Found 4 topics\nProcessing article 241... ✓ Found 3 topics\nProcessing article 242... ✓ Found 2 topics\nProcessing article 243... ✓ Found 1 topics\nProcessing article 244... ✓ Found 5 topics\nProcessing article 245... ✓ Found 3 topics\nProcessing article 246... ✓ Found 0 topics\nProcessing article 247... ✓ Found 3 topics\nProcessing article 248... ✓ Found 1 topics\nProcessing article 249... ✓ Found 1 topics\nProcessing article 250... ✓ Found 1 topics\nProcessing article 251... ✓ Found 3 topics\nProcessing article 252... ✓ Found 5 topics\nProcessing article 253... ✓ Found 5 topics\nProcessing article 254... ✓ Found 2 topics\nProcessing article 255... ✓ Found 5 topics\nProcessing article 256... ✓ Found 5 topics\nProcessing article 257... ✓ Found 2 topics\nProcessing article 258... ✓ Found 2 topics\nProcessing article 259... ✓ Found 2 topics\nProcessing article 260... ✓ Found 5 topics\nProcessing article 261... ✓ Found 3 topics\nProcessing article 262... ✓ Found 5 topics\nProcessing article 263... ✓ Found 5 topics\nProcessing article 264... ✓ Found 2 topics\nProcessing article 265... ✓ Found 5 topics\nProcessing article 266... ✓ Found 5 topics\nProcessing article 267... ✓ Found 2 topics\nProcessing article 268... ✓ Found 2 topics\nProcessing article 269... ✓ Found 2 topics\nProcessing article 270... ✓ Found 5 topics\nProcessing article 271... ✓ Found 3 topics\nProcessing article 272... ✓ Found 5 topics\nProcessing article 273... ✓ Found 3 topics\nProcessing article 274... ✓ Found 2 topics\nProcessing article 275... ✓ Found 3 topics\nProcessing article 276... ✓ Found 3 topics\nProcessing article 277... ✓ Found 1 topics\nProcessing article 278... ✓ Found 2 topics\nProcessing article 279... ✓ Found 5 topics\nProcessing article 280... ✓ Found 2 topics\nProcessing article 281... ✓ Found 3 topics\nProcessing article 282... ✓ Found 5 topics\nProcessing article 283... ✓ Found 0 topics\nProcessing article 284... ✓ Found 3 topics\nProcessing article 285... ✓ Found 4 topics\nProcessing article 286... ✓ Found 3 topics\nProcessing article 287... ✓ Found 5 topics\nProcessing article 288... ✓ Found 2 topics\nProcessing article 289... ✓ Found 2 topics\nProcessing article 290... ✓ Found 5 topics\nProcessing article 291... ✓ Found 2 topics\nProcessing article 292... ✓ Found 4 topics\nProcessing article 293... ✓ Found 2 topics\nProcessing article 294... ✓ Found 5 topics\nProcessing article 295... ✓ Found 2 topics\nProcessing article 296... ✓ Found 4 topics\nProcessing article 297... ✓ Found 0 topics\nProcessing article 298... ✓ Found 0 topics\nProcessing article 299... ✓ Found 4 topics\nProcessing article 300... ✓ Found 5 topics\nProcessing article 301... ✓ Found 3 topics\nProcessing article 302... ✓ Found 3 topics\nProcessing article 303... ✓ Found 2 topics\nProcessing article 304... ✓ Found 3 topics\nProcessing article 305... ✓ Found 5 topics\nProcessing article 306... ✓ Found 3 topics\nProcessing article 307... ✓ Found 1 topics\nProcessing article 308... ✓ Found 4 topics\nProcessing article 309... ✓ Found 2 topics\nProcessing article 310... ✓ Found 2 topics\nProcessing article 311... ✓ Found 3 topics\nProcessing article 312... ✓ Found 5 topics\nProcessing article 313... ✓ Found 1 topics\nProcessing article 314... ✓ Found 5 topics\nProcessing article 315... ✓ Found 4 topics\nProcessing article 316... ✓ Found 2 topics\nProcessing article 317... ✓ Found 4 topics\nProcessing article 318... ✓ Found 2 topics\nProcessing article 319... ✓ Found 4 topics\nProcessing article 320... ✓ Found 5 topics\nProcessing article 321... ✓ Found 2 topics\nProcessing article 322... ✓ Found 5 topics\nProcessing article 323... ✓ Found 4 topics\nProcessing article 324... ✓ Found 1 topics\nProcessing article 325... ✓ Found 3 topics\nProcessing article 326... ✓ Found 0 topics\nProcessing article 327... ✓ Found 3 topics\nProcessing article 328... ✓ Found 3 topics\nProcessing article 329... ✓ Found 3 topics\nProcessing article 330... ✓ Found 1 topics\nProcessing article 331... ✓ Found 1 topics\nProcessing article 332... ✓ Found 3 topics\nProcessing article 333... ✓ Found 2 topics\nProcessing article 334... ✓ Found 2 topics\nProcessing article 335... ✓ Found 2 topics\nProcessing article 336... ✓ Found 2 topics\nProcessing article 337... ✓ Found 5 topics\nProcessing article 338... ✓ Found 5 topics\nProcessing article 339... ✓ Found 4 topics\nProcessing article 340... ✓ Found 3 topics\nProcessing article 341... ✓ Found 3 topics\nProcessing article 342... ✓ Found 2 topics\nProcessing article 343... ✓ Found 3 topics\nProcessing article 344... ✓ Found 0 topics\nProcessing article 345... ✓ Found 3 topics\nProcessing article 346... ✓ Found 2 topics\nProcessing article 347... ✓ Found 4 topics\nProcessing article 348... ✓ Found 2 topics\nProcessing article 349... ✓ Found 5 topics\nProcessing article 350... ✓ Found 7 topics\nProcessing article 351... ✓ Found 2 topics\nProcessing article 352... ✓ Found 5 topics\nProcessing article 353... ✓ Found 2 topics\nProcessing article 354... ✓ Found 4 topics\nProcessing article 355... ✓ Found 4 topics\nProcessing article 356... ✓ Found 3 topics\nProcessing article 357... ✓ Found 1 topics\nProcessing article 358... ✓ Found 3 topics\nProcessing article 359... ✓ Found 4 topics\nProcessing article 360... ✓ Found 2 topics\nProcessing article 361... ✓ Found 3 topics\nProcessing article 362... ✓ Found 1 topics\nProcessing article 363... ✓ Found 4 topics\nProcessing article 364... ✓ Found 2 topics\nProcessing article 365... ✓ Found 1 topics\nProcessing article 366... ✓ Found 0 topics\nProcessing article 367... ✓ Found 3 topics\nProcessing article 368... ✓ Found 2 topics\nProcessing article 369... ✓ Found 1 topics\nProcessing article 370... ✓ Found 5 topics\nProcessing article 371... ✓ Found 5 topics\nProcessing article 372... ✓ Found 1 topics\nProcessing article 373... ✓ Found 2 topics\nProcessing article 374... ✓ Found 0 topics\nProcessing article 375... ✓ Found 4 topics\nProcessing article 376... ✓ Found 3 topics\nProcessing article 377... ✓ Found 2 topics\nProcessing article 378... ✓ Found 2 topics\nProcessing article 379... ✓ Found 2 topics\nProcessing article 380... ✓ Found 3 topics\nProcessing article 381... ✓ Found 5 topics\nProcessing article 382... ✓ Found 0 topics\nProcessing article 383... ✓ Found 5 topics\nProcessing article 384... ✓ Found 1 topics\nProcessing article 385... ✓ Found 0 topics\nProcessing article 386... ✓ Found 5 topics\nProcessing article 387... ✓ Found 5 topics\nProcessing article 388... ✓ Found 4 topics\nProcessing article 389... ✓ Found 4 topics\nProcessing article 390... ✓ Found 5 topics\nProcessing article 391... ✓ Found 2 topics\nProcessing article 392... ✓ Found 6 topics\nProcessing article 393... ✓ Found 1 topics\nProcessing article 394... ✓ Found 3 topics\nProcessing article 395... ✓ Found 2 topics\nProcessing article 396... ✓ Found 3 topics\nProcessing article 397... ✓ Found 5 topics\nProcessing article 398... ✓ Found 2 topics\nProcessing article 399... ✓ Found 0 topics\nProcessing article 400... ✓ Found 4 topics\nProcessing article 401... ✓ Found 5 topics\nProcessing article 402... ✓ Found 2 topics\nProcessing article 403... ✓ Found 2 topics\nProcessing article 404... ✓ Found 5 topics\nProcessing article 405... ✓ Found 2 topics\nProcessing article 406... ✓ Found 2 topics\nProcessing article 407... ✓ Found 2 topics\nProcessing article 408... ✓ Found 3 topics\nProcessing article 409... ✓ Found 3 topics\nProcessing article 410... ✓ Found 3 topics\nProcessing article 411... ✓ Found 4 topics\nProcessing article 412... ✓ Found 4 topics\nProcessing article 413... ✓ Found 0 topics\nProcessing article 414... ✓ Found 3 topics\nProcessing article 415... ✓ Found 2 topics\nProcessing article 416... ✓ Found 2 topics\nProcessing article 417... ✓ Found 1 topics\nProcessing article 418... ✓ Found 4 topics\nProcessing article 419... ✓ Found 5 topics\nProcessing article 420... ✓ Found 1 topics\nProcessing article 421... ✓ Found 3 topics\nProcessing article 422... ✓ Found 1 topics\nProcessing article 423... ✓ Found 0 topics\nProcessing article 424... ✓ Found 0 topics\nProcessing article 425... ✓ Found 5 topics\nProcessing article 426... ✓ Found 2 topics\nProcessing article 427... ✓ Found 2 topics\nProcessing article 428... ✓ Found 2 topics\nProcessing article 429... ✓ Found 3 topics\nProcessing article 430... ✓ Found 0 topics\nProcessing article 431... ✓ Found 5 topics\nProcessing article 432... ✓ Found 5 topics\nProcessing article 433... ✓ Found 2 topics\nProcessing article 434... ✓ Found 2 topics\nProcessing article 435... ✓ Found 3 topics\nProcessing article 436... ✓ Found 3 topics\nProcessing article 437... ✓ Found 5 topics\nProcessing article 438... ✓ Found 3 topics\nProcessing article 439... ✓ Found 1 topics\nProcessing article 440... ✓ Found 1 topics\nProcessing article 441... ✓ Found 1 topics\nProcessing article 442... ✓ Found 2 topics\nProcessing article 443... ✓ Found 5 topics\nProcessing article 444... ✓ Found 2 topics\nProcessing article 445... ✓ Found 3 topics\nProcessing article 446... ✓ Found 3 topics\nProcessing article 447... ✓ Found 5 topics\nProcessing article 448... ✓ Found 3 topics\nProcessing article 449... ✓ Found 1 topics\nProcessing article 450... ✓ Found 1 topics\nProcessing article 451... ✓ Found 1 topics\nProcessing article 452... ✓ Found 2 topics\nProcessing article 453... ✓ Found 5 topics\nProcessing article 454... ✓ Found 2 topics\nProcessing article 455... ✓ Found 3 topics\nProcessing article 456... ✓ Found 5 topics\nProcessing article 457... ✓ Found 1 topics\nProcessing article 458... ✓ Found 5 topics\nProcessing article 459... ✓ Found 0 topics\nProcessing article 460... ✓ Found 5 topics\nProcessing article 461... ✓ Found 3 topics\nProcessing article 462... ✓ Found 4 topics\nProcessing article 463... ✓ Found 3 topics\nProcessing article 464... ✓ Found 4 topics\nProcessing article 465... ✓ Found 2 topics\nProcessing article 466... ✓ Found 3 topics\nProcessing article 467... ✓ Found 4 topics\nProcessing article 468... ✓ Found 4 topics\nProcessing article 469... ✓ Found 2 topics\nProcessing article 470... ✓ Found 3 topics\nProcessing article 471... ✓ Found 3 topics\nProcessing article 472... ✓ Found 5 topics\nProcessing article 473... ✓ Found 5 topics\nProcessing article 474... ✓ Found 4 topics\nProcessing article 475... ✓ Found 1 topics\nProcessing article 476... ✓ Found 3 topics\nProcessing article 477... ✓ Found 0 topics\nProcessing article 478... ✓ Found 2 topics\nProcessing article 479... ✓ Found 3 topics\nProcessing article 480... ✓ Found 5 topics\nProcessing article 481... ✓ Found 3 topics\nProcessing article 482... ✓ Found 5 topics\nProcessing article 483... ✓ Found 4 topics\nProcessing article 484... ✓ Found 0 topics\nProcessing article 485... ✓ Found 0 topics\nProcessing article 486... ✓ Found 1 topics\nProcessing article 487... ✓ Found 2 topics\nProcessing article 488... ✓ Found 2 topics\nProcessing article 489... ✓ Found 2 topics\nProcessing article 490... ✓ Found 1 topics\nProcessing article 491... ✓ Found 5 topics\nProcessing article 492... ✓ Found 2 topics\nProcessing article 493... ✓ Found 2 topics\nProcessing article 494... ✓ Found 4 topics\nProcessing article 495... ✓ Found 3 topics\nProcessing article 496... ✓ Found 2 topics\nProcessing article 497... ✓ Found 3 topics\nProcessing article 498... ✓ Found 1 topics\nProcessing article 499... ✓ Found 4 topics\nProcessing article 500... ✓ Found 5 topics\nProcessing article 501... ✓ Found 1 topics\nProcessing article 502... ✓ Found 3 topics\nProcessing article 503... ✓ Found 5 topics\nProcessing article 504... ✓ Found 5 topics\nProcessing article 505... ✓ Found 0 topics\nProcessing article 506... ✓ Found 3 topics\nProcessing article 507... ✓ Found 1 topics\nProcessing article 508... ✓ Found 4 topics\nProcessing article 509... ✓ Found 4 topics\nProcessing article 510... ✓ Found 0 topics\nProcessing article 511... ✓ Found 5 topics\nProcessing article 512... ✓ Found 3 topics\nProcessing article 513... ✓ Found 5 topics\nProcessing article 514... ✓ Found 4 topics\nProcessing article 515... ✓ Found 0 topics\nProcessing article 516... ✓ Found 3 topics\nProcessing article 517... ✓ Found 1 topics\nProcessing article 518... ✓ Found 2 topics\nProcessing article 519... ✓ Found 2 topics\nProcessing article 520... ✓ Found 4 topics\nProcessing article 521... ✓ Found 5 topics\nProcessing article 522... ✓ Found 2 topics\nProcessing article 523... ✓ Found 5 topics\nProcessing article 524... ✓ Found 3 topics\nProcessing article 525... ✓ Found 0 topics\nProcessing article 526... ✓ Found 0 topics\nProcessing article 527... ✓ Found 2 topics\nProcessing article 528... ✓ Found 3 topics\nProcessing article 529... ✓ Found 3 topics\nProcessing article 530... ✓ Found 1 topics\nProcessing article 531... ✓ Found 3 topics\nProcessing article 532... ✓ Found 0 topics\nProcessing article 533... ✓ Found 2 topics\nProcessing article 534... ✓ Found 2 topics\nProcessing article 535... ✓ Found 5 topics\nProcessing article 536... ✓ Found 3 topics\nProcessing article 537... ✓ Found 2 topics\nProcessing article 538... ✓ Found 1 topics\nProcessing article 539... ✓ Found 0 topics\nProcessing article 540... ✓ Found 1 topics\nProcessing article 541... ✓ Found 3 topics\nProcessing article 542... ✓ Found 3 topics\nProcessing article 543... ✓ Found 5 topics\nProcessing article 544... ✓ Found 3 topics\nProcessing article 545... ✓ Found 4 topics\nProcessing article 546... ✓ Found 4 topics\nProcessing article 547... ✓ Found 1 topics\nProcessing article 548... ✓ Found 2 topics\nProcessing article 549... ✓ Found 2 topics\nProcessing article 550... ✓ Found 5 topics\nProcessing article 551... ✓ Found 2 topics\nProcessing article 552... ✓ Found 2 topics\nProcessing article 553... ✓ Found 3 topics\nProcessing article 554... ✓ Found 1 topics\nProcessing article 555... ✓ Found 4 topics\nProcessing article 556... ✓ Found 1 topics\nProcessing article 557... ✓ Found 7 topics\nProcessing article 558... ✓ Found 3 topics\nProcessing article 559... ✓ Found 3 topics\nProcessing article 560... ✓ Found 0 topics\nProcessing article 561... ✓ Found 3 topics\nProcessing article 562... ✓ Found 3 topics\nProcessing article 563... ✓ Found 2 topics\nProcessing article 564... ✓ Found 5 topics\nProcessing article 565... ✓ Found 3 topics\nProcessing article 566... ✓ Found 2 topics\nProcessing article 567... ✓ Found 3 topics\nProcessing article 568... ✓ Found 2 topics\nProcessing article 569... ✓ Found 2 topics\nProcessing article 570... ✓ Found 5 topics\nProcessing article 571... ✓ Found 3 topics\nProcessing article 572... ✓ Found 1 topics\nProcessing article 573... ✓ Found 2 topics\nProcessing article 574... ✓ Found 0 topics\nProcessing article 575... ✓ Found 1 topics\nProcessing article 576... ✓ Found 0 topics\nProcessing article 577... ✓ Found 3 topics\nProcessing article 578... ✓ Found 3 topics\nProcessing article 579... ✓ Found 3 topics\nProcessing article 580... ✓ Found 3 topics\nProcessing article 581... ✓ Found 2 topics\nProcessing article 582... ✓ Found 2 topics\nProcessing article 583... ✓ Found 1 topics\nProcessing article 584... ✓ Found 1 topics\nProcessing article 585... ✓ Found 2 topics\nProcessing article 586... ✓ Found 3 topics\nProcessing article 587... ✓ Found 5 topics\nProcessing article 588... ✓ Found 1 topics\nProcessing article 589... ✓ Found 0 topics\nProcessing article 590... ✓ Found 0 topics\nProcessing article 591... ✓ Found 2 topics\nProcessing article 592... ✓ Found 3 topics\nProcessing article 593... ✓ Found 0 topics\nProcessing article 594... ✓ Found 3 topics\nProcessing article 595... ✓ Found 1 topics\nProcessing article 596... ✓ Found 3 topics\nProcessing article 597... ✓ Found 5 topics\nProcessing article 598... ✓ Found 3 topics\nProcessing article 599... ✓ Found 5 topics\nProcessing article 600... ✓ Found 5 topics\nProcessing article 601... ✓ Found 2 topics\nProcessing article 602... ✓ Found 3 topics\nProcessing article 603... ✓ Found 0 topics\nProcessing article 604... ✓ Found 3 topics\nProcessing article 605... ✓ Found 1 topics\nProcessing article 606... ✓ Found 3 topics\nProcessing article 607... ✓ Found 5 topics\nProcessing article 608... ✓ Found 3 topics\nProcessing article 609... ✓ Found 5 topics\nProcessing article 610... ✓ Found 5 topics\nProcessing article 611... ✓ Found 4 topics\nProcessing article 612... ✓ Found 3 topics\nProcessing article 613... ✓ Found 5 topics\nProcessing article 614... ✓ Found 2 topics\nProcessing article 615... ✓ Found 1 topics\nProcessing article 616... ✓ Found 5 topics\nProcessing article 617... ✓ Found 3 topics\nProcessing article 618... ✓ Found 2 topics\nProcessing article 619... ✓ Found 2 topics\nProcessing article 620... ✓ Found 5 topics\nProcessing article 621... ✓ Found 2 topics\nProcessing article 622... ✓ Found 2 topics\nProcessing article 623... ✓ Found 2 topics\nProcessing article 624... ✓ Found 1 topics\nProcessing article 625... ✓ Found 4 topics\nProcessing article 626... ✓ Found 5 topics\nProcessing article 627... ✓ Found 3 topics\nProcessing article 628... ✓ Found 3 topics\nProcessing article 629... ✓ Found 2 topics\nProcessing article 630... ✓ Found 4 topics\nProcessing article 631... ✓ Found 3 topics\nProcessing article 632... ✓ Found 1 topics\nProcessing article 633... ✓ Found 0 topics\nProcessing article 634... ✓ Found 5 topics\nProcessing article 635... ✓ Found 2 topics\nProcessing article 636... ✓ Found 5 topics\nProcessing article 637... ✓ Found 1 topics\nProcessing article 638... ✓ Found 3 topics\nProcessing article 639... ✓ Found 5 topics\nProcessing article 640... ✓ Found 0 topics\nProcessing article 641... ✓ Found 5 topics\nProcessing article 642... ✓ Found 0 topics\nProcessing article 643... ✓ Found 5 topics\nProcessing article 644... ✓ Found 4 topics\nProcessing article 645... ✓ Found 5 topics\nProcessing article 646... ✓ Found 4 topics\nProcessing article 647... ✓ Found 2 topics\nProcessing article 648... ✓ Found 3 topics\nProcessing article 649... ✓ Found 0 topics\nProcessing article 650... ✓ Found 2 topics\nProcessing article 651... ✓ Found 3 topics\nProcessing article 652... ✓ Found 4 topics\nProcessing article 653... ✓ Found 2 topics\nProcessing article 654... ✓ Found 0 topics\nProcessing article 655... ✓ Found 1 topics\nProcessing article 656... ✓ Found 3 topics\nProcessing article 657... ✓ Found 2 topics\nProcessing article 658... ✓ Found 5 topics\nProcessing article 659... ✓ Found 1 topics\nProcessing article 660... ✓ Found 1 topics\nProcessing article 661... ✓ Found 2 topics\nProcessing article 662... ✓ Found 2 topics\nProcessing article 663... ✓ Found 0 topics\nProcessing article 664... ✓ Found 5 topics\nProcessing article 665... ✓ Found 2 topics\nProcessing article 666... ✓ Found 6 topics\nProcessing article 667... ✓ Found 5 topics\nProcessing article 668... ✓ Found 0 topics\nProcessing article 669... ✓ Found 2 topics\nProcessing article 670... ✓ Found 4 topics\nProcessing article 671... ✓ Found 2 topics\nProcessing article 672... ✓ Found 3 topics\nProcessing article 673... ✓ Found 0 topics\nProcessing article 674... ✓ Found 5 topics\nProcessing article 675... ✓ Found 5 topics\nProcessing article 676... ✓ Found 4 topics\nProcessing article 677... ✓ Found 2 topics\nProcessing article 678... ✓ Found 1 topics\nProcessing article 679... ✓ Found 5 topics\nProcessing article 680... ✓ Found 1 topics\nProcessing article 681... ✓ Found 3 topics\nProcessing article 682... ✓ Found 3 topics\nProcessing article 683... ✓ Found 5 topics\nProcessing article 684... ✓ Found 5 topics\nProcessing article 685... ✓ Found 4 topics\nProcessing article 686... ✓ Found 1 topics\nProcessing article 687... ✓ Found 2 topics\nProcessing article 688... ✓ Found 2 topics\nProcessing article 689... ✓ Found 5 topics\nProcessing article 690... ✓ Found 5 topics\nProcessing article 691... ✓ Found 4 topics\nProcessing article 692... ✓ Found 2 topics\nProcessing article 693... ✓ Found 4 topics\nProcessing article 694... ✓ Found 3 topics\nProcessing article 695... ✓ Found 4 topics\nProcessing article 696... ✓ Found 2 topics\nProcessing article 697... ✓ Found 5 topics\nProcessing article 698... ✓ Found 2 topics\nProcessing article 699... ✓ Found 1 topics\nProcessing article 700... ✓ Found 2 topics\nProcessing article 701... ✓ Found 5 topics\nProcessing article 702... ✓ Found 3 topics\nProcessing article 703... ✓ Found 2 topics\nProcessing article 704... ✓ Found 1 topics\nProcessing article 705... ✓ Found 3 topics\nProcessing article 706... ✓ Found 5 topics\nProcessing article 707... ✓ Found 3 topics\nProcessing article 708... ✓ Found 1 topics\nProcessing article 709... ✓ Found 0 topics\nProcessing article 710... ✓ Found 2 topics\nProcessing article 711... ✓ Found 5 topics\nProcessing article 712... ✓ Found 3 topics\nProcessing article 713... ✓ Found 2 topics\nProcessing article 714... ✓ Found 1 topics\nProcessing article 715... ✓ Found 3 topics\nProcessing article 716... ✓ Found 5 topics\nProcessing article 717... ✓ Found 3 topics\nProcessing article 718... ✓ Found 1 topics\nProcessing article 719... ✓ Found 0 topics\nProcessing article 720... ✓ Found 2 topics\nProcessing article 721... ✓ Found 5 topics\nProcessing article 722... ✓ Found 0 topics\nProcessing article 723... ✓ Found 1 topics\nProcessing article 724... ✓ Found 1 topics\nProcessing article 725... ✓ Found 5 topics\nProcessing article 726... ✓ Found 5 topics\nProcessing article 727... ✓ Found 0 topics\nProcessing article 728... ✓ Found 5 topics\nProcessing article 729... ✓ Found 5 topics\nProcessing article 730... ✓ Found 2 topics\nProcessing article 731... ✓ Found 1 topics\nProcessing article 732... ✓ Found 3 topics\nProcessing article 733... ✓ Found 0 topics\nProcessing article 734... ✓ Found 2 topics\nProcessing article 735... ✓ Found 3 topics\nProcessing article 736... ✓ Found 2 topics\nProcessing article 737... ✓ Found 2 topics\nProcessing article 738... ✓ Found 2 topics\nProcessing article 739... ✓ Found 3 topics\nProcessing article 740... ✓ Found 3 topics\nProcessing article 741... ✓ Found 5 topics\nProcessing article 742... ✓ Found 3 topics\nProcessing article 743... ✓ Found 1 topics\nProcessing article 744... ✓ Found 2 topics\nProcessing article 745... ✓ Found 1 topics\nProcessing article 746... ✓ Found 3 topics\nProcessing article 747... ✓ Found 2 topics\nProcessing article 748... ✓ Found 4 topics\nProcessing article 749... ✓ Found 1 topics\nProcessing article 750... ✓ Found 5 topics\nProcessing article 751... ✓ Found 3 topics\nProcessing article 752... ✓ Found 4 topics\nProcessing article 753... ✓ Found 1 topics\nProcessing article 754... ✓ Found 2 topics\nProcessing article 755... ✓ Found 3 topics\nProcessing article 756... ✓ Found 4 topics\nProcessing article 757... ✓ Found 1 topics\nProcessing article 758... ✓ Found 0 topics\nProcessing article 759... ✓ Found 3 topics\nProcessing article 760... ✓ Found 5 topics\nProcessing article 761... ✓ Found 5 topics\nProcessing article 762... ✓ Found 1 topics\nProcessing article 763... ✓ Found 4 topics\nProcessing article 764... ✓ Found 2 topics\nProcessing article 765... ✓ Found 0 topics\nProcessing article 766... ✓ Found 5 topics\nProcessing article 767... ✓ Found 5 topics\nProcessing article 768... ✓ Found 3 topics\nProcessing article 769... ✓ Found 2 topics\nProcessing article 770... ✓ Found 5 topics\nProcessing article 771... ✓ Found 3 topics\nProcessing article 772... ✓ Found 5 topics\nProcessing article 773... ✓ Found 4 topics\nProcessing article 774... ✓ Found 1 topics\nProcessing article 775... ✓ Found 3 topics\nProcessing article 776... ✓ Found 1 topics\nProcessing article 777... ✓ Found 1 topics\nProcessing article 778... ✓ Found 3 topics\nProcessing article 779... ✓ Found 3 topics\nProcessing article 780... ✓ Found 2 topics\nProcessing article 781... ✓ Found 3 topics\nProcessing article 782... ✓ Found 0 topics\nProcessing article 783... ✓ Found 1 topics\nProcessing article 784... ✓ Found 3 topics\nProcessing article 785... ✓ Found 3 topics\nProcessing article 786... ✓ Found 1 topics\nProcessing article 787... ✓ Found 5 topics\nProcessing article 788... ✓ Found 5 topics\nProcessing article 789... ✓ Found 2 topics\nProcessing article 790... ✓ Found 5 topics\nProcessing article 791... ✓ Found 3 topics\nProcessing article 792... ✓ Found 1 topics\nProcessing article 793... ✓ Found 1 topics\nProcessing article 794... ✓ Found 3 topics\nProcessing article 795... ✓ Found 1 topics\nProcessing article 796... ✓ Found 0 topics\nProcessing article 797... ✓ Found 1 topics\nProcessing article 798... ✓ Found 2 topics\nProcessing article 799... ✓ Found 5 topics\nProcessing article 800... ✓ Found 1 topics\nProcessing article 801... ✓ Found 1 topics\nProcessing article 802... ✓ Found 1 topics\nProcessing article 803... ✓ Found 5 topics\nProcessing article 804... ✓ Found 2 topics\nProcessing article 805... ✓ Found 3 topics\nProcessing article 806... ✓ Found 0 topics\nProcessing article 807... ✓ Found 0 topics\nProcessing article 808... ✓ Found 5 topics\nProcessing article 809... ✓ Found 1 topics\nProcessing article 810... ✓ Found 2 topics\nProcessing article 811... ✓ Found 3 topics\nProcessing article 812... ✓ Found 1 topics\nProcessing article 813... ✓ Found 0 topics\nProcessing article 814... ✓ Found 4 topics\nProcessing article 815... ✓ Found 2 topics\nProcessing article 816... ✓ Found 5 topics\nProcessing article 817... ✓ Found 5 topics\nProcessing article 818... ✓ Found 4 topics\nProcessing article 819... ✓ Found 5 topics\nProcessing article 820... ✓ Found 2 topics\nProcessing article 821... ✓ Found 0 topics\nProcessing article 822... ✓ Found 5 topics\nProcessing article 823... ✓ Found 1 topics\nProcessing article 824... ✓ Found 4 topics\nProcessing article 825... ✓ Found 1 topics\nProcessing article 826... ✓ Found 0 topics\nProcessing article 827... ✓ Found 0 topics\nProcessing article 828... ✓ Found 4 topics\nProcessing article 829... ✓ Found 3 topics\nProcessing article 830... ✓ Found 3 topics\nProcessing article 831... ✓ Found 0 topics\nProcessing article 832... ✓ Found 3 topics\nProcessing article 833... ✓ Found 0 topics\nProcessing article 834... ✓ Found 3 topics\nProcessing article 835... ✓ Found 4 topics\nProcessing article 836... ✓ Found 5 topics\nProcessing article 837... ✓ Found 1 topics\nProcessing article 838... ✓ Found 3 topics\nProcessing article 839... ✓ Found 1 topics\nProcessing article 840... ✓ Found 1 topics\nProcessing article 841... ✓ Found 2 topics\nProcessing article 842... ✓ Found 2 topics\nProcessing article 843... ✓ Found 3 topics\nProcessing article 844... ✓ Found 1 topics\nProcessing article 845... ✓ Found 2 topics\nProcessing article 846... ✓ Found 2 topics\nProcessing article 847... ✓ Found 5 topics\nProcessing article 848... ✓ Found 1 topics\nProcessing article 849... ✓ Found 2 topics\nProcessing article 850... ✓ Found 0 topics\nProcessing article 851... ✓ Found 2 topics\nProcessing article 852... ✓ Found 1 topics\nProcessing article 853... ✓ Found 3 topics\nProcessing article 854... ✓ Found 3 topics\nProcessing article 855... ✓ Found 2 topics\nProcessing article 856... ✓ Found 3 topics\nProcessing article 857... ✓ Found 3 topics\nProcessing article 858... ✓ Found 1 topics\nProcessing article 859... ✓ Found 1 topics\nProcessing article 860... ✓ Found 4 topics\nProcessing article 861... ✓ Found 1 topics\nProcessing article 862... ✓ Found 2 topics\nProcessing article 863... ✓ Found 0 topics\nProcessing article 864... ✓ Found 5 topics\nProcessing article 865... ✓ Found 2 topics\nProcessing article 866... ✓ Found 2 topics\nProcessing article 867... ✓ Found 5 topics\nProcessing article 868... ✓ Found 3 topics\nProcessing article 869... ✓ Found 2 topics\nProcessing article 870... ✓ Found 2 topics\nProcessing article 871... ✓ Found 2 topics\nProcessing article 872... ✓ Found 3 topics\nProcessing article 873... ✓ Found 2 topics\nProcessing article 874... ✓ Found 3 topics\nProcessing article 875... ✓ Found 2 topics\nProcessing article 876... ✓ Found 5 topics\nProcessing article 877... ✓ Found 2 topics\nProcessing article 878... ✓ Found 3 topics\nProcessing article 879... ✓ Found 4 topics\nProcessing article 880... ✓ Found 4 topics\nProcessing article 881... ✓ Found 5 topics\nProcessing article 882... ✓ Found 3 topics\nProcessing article 883... ✓ Found 2 topics\nProcessing article 884... ✓ Found 3 topics\nProcessing article 885... ✓ Found 3 topics\nProcessing article 886... ✓ Found 3 topics\nProcessing article 887... ✓ Found 1 topics\nProcessing article 888... ✓ Found 3 topics\nProcessing article 889... ✓ Found 5 topics\nProcessing article 890... ✓ Found 3 topics\nProcessing article 891... ✓ Found 2 topics\nProcessing article 892... ✓ Found 2 topics\nProcessing article 893... ✓ Found 2 topics\nProcessing article 894... ✓ Found 3 topics\nProcessing article 895... ✓ Found 3 topics\nProcessing article 896... ✓ Found 3 topics\nProcessing article 897... ✓ Found 1 topics\nProcessing article 898... ✓ Found 3 topics\nProcessing article 899... ✓ Found 5 topics\nProcessing article 900... ✓ Found 3 topics\nProcessing article 901... ✓ Found 2 topics\nProcessing article 902... ✓ Found 2 topics\nProcessing article 903... ✓ Found 2 topics\nProcessing article 904... ✓ Found 3 topics\nProcessing article 905... ✓ Found 0 topics\nProcessing article 906... ✓ Found 2 topics\nProcessing article 907... ✓ Found 2 topics\nProcessing article 908... ✓ Found 5 topics\nProcessing article 909... ✓ Found 2 topics\nProcessing article 910... ✓ Found 1 topics\nProcessing article 911... ✓ Found 4 topics\nProcessing article 912... ✓ Found 2 topics\nProcessing article 913... ✓ Found 2 topics\nProcessing article 914... ✓ Found 5 topics\nProcessing article 915... ✓ Found 4 topics\nProcessing article 916... ✓ Found 1 topics\nProcessing article 917... ✓ Found 5 topics\nProcessing article 918... ✓ Found 10 topics\nProcessing article 919... ✓ Found 1 topics\nProcessing article 920... ✓ Found 3 topics\nProcessing article 921... ✓ Found 2 topics\nProcessing article 922... ✓ Found 3 topics\nProcessing article 923... ✓ Found 2 topics\nProcessing article 924... ✓ Found 0 topics\nProcessing article 925... ✓ Found 2 topics\nProcessing article 926... ✓ Found 2 topics\nProcessing article 927... ✓ Found 2 topics\nProcessing article 928... ✓ Found 5 topics\nProcessing article 929... ✓ Found 5 topics\nProcessing article 930... ✓ Found 4 topics\nProcessing article 931... ✓ Found 5 topics\nProcessing article 932... ✓ Found 4 topics\nProcessing article 933... ✓ Found 2 topics\nProcessing article 934... ✓ Found 2 topics\nProcessing article 935... ✓ Found 5 topics\nProcessing article 936... ✓ Found 1 topics\nProcessing article 937... ✓ Found 1 topics\nProcessing article 938... ✓ Found 2 topics\nProcessing article 939... ✓ Found 5 topics\nProcessing article 940... ✓ Found 2 topics\nProcessing article 941... ✓ Found 1 topics\nProcessing article 942... ✓ Found 5 topics\nProcessing article 943... ✓ Found 1 topics\nProcessing article 944... ✓ Found 2 topics\nProcessing article 945... ✓ Found 2 topics\nProcessing article 946... ✓ Found 0 topics\nProcessing article 947... ✓ Found 0 topics\nProcessing article 948... ✓ Found 4 topics\nProcessing article 949... ✓ Found 1 topics\nProcessing article 950... ✓ Found 1 topics\nProcessing article 951... ✓ Found 5 topics\nProcessing article 952... ✓ Found 3 topics\nProcessing article 953... ✓ Found 1 topics\nProcessing article 954... ✓ Found 5 topics\nProcessing article 955... ✓ Found 2 topics\nProcessing article 956... ✓ Found 1 topics\nProcessing article 957... ✓ Found 3 topics\nProcessing article 958... ✓ Found 3 topics\nProcessing article 959... ✓ Found 2 topics\nProcessing article 960... ✓ Found 4 topics\nProcessing article 961... ✓ Found 5 topics\nProcessing article 962... ✓ Found 3 topics\nProcessing article 963... ✓ Found 2 topics\nProcessing article 964... ✓ Found 2 topics\nProcessing article 965... ✓ Found 0 topics\nProcessing article 966... ✓ Found 2 topics\nProcessing article 967... ✓ Found 5 topics\nProcessing article 968... ✓ Found 0 topics\nProcessing article 969... ✓ Found 1 topics\nProcessing article 970... ✓ Found 0 topics\nProcessing article 971... ✓ Found 0 topics\nProcessing article 972... ✓ Found 5 topics\nProcessing article 973... ✓ Found 0 topics\nProcessing article 974... ✓ Found 5 topics\nProcessing article 975... ✓ Found 1 topics\nProcessing article 976... ✓ Found 2 topics\nProcessing article 977... ✓ Found 1 topics\nProcessing article 978... ✓ Found 2 topics\nProcessing article 979... ✓ Found 3 topics\nProcessing article 980... ✓ Found 2 topics\nProcessing article 981... ✓ Found 2 topics\nProcessing article 982... ✓ Found 0 topics\nProcessing article 983... ✓ Found 5 topics\nProcessing article 984... ✓ Found 3 topics\nProcessing article 985... ✓ Found 0 topics\nProcessing article 986... ✓ Found 5 topics\nProcessing article 987... ✓ Found 2 topics\nProcessing article 988... ✓ Found 4 topics\nProcessing article 989... ✓ Found 2 topics\nProcessing article 990... ✓ Found 3 topics\nProcessing article 991... ✓ Found 5 topics\nProcessing article 992... ✓ Found 2 topics\nProcessing article 993... ✓ Found 3 topics\nProcessing article 994... ✓ Found 4 topics\nProcessing article 995... ✓ Found 4 topics\nProcessing article 996... ✓ Found 2 topics\nProcessing article 997... ✓ Found 2 topics\nProcessing article 998... ✓ Found 3 topics\nProcessing article 999... ✓ Found 3 topics\nProcessing article 1000... ✓ Found 2 topics\nProcessing article 1001... ✓ Found 2 topics\nProcessing article 1002... ✓ Found 1 topics\nProcessing article 1003... ✓ Found 2 topics\nProcessing article 1004... ✓ Found 4 topics\nProcessing article 1005... ✓ Found 4 topics\nProcessing article 1006... ✓ Found 0 topics\nProcessing article 1007... ✓ Found 1 topics\nProcessing article 1008... ✓ Found 0 topics\nProcessing article 1009... ✓ Found 0 topics\nProcessing article 1010... ✓ Found 1 topics\nProcessing article 1011... ✓ Found 3 topics\nProcessing article 1012... ✓ Found 5 topics\nProcessing article 1013... ✓ Found 0 topics\nProcessing article 1014... ✓ Found 5 topics\nProcessing article 1015... ✓ Found 3 topics\nProcessing article 1016... ✓ Found 0 topics\nProcessing article 1017... ✓ Found 5 topics\nProcessing article 1018... ✓ Found 4 topics\nProcessing article 1019... ✓ Found 1 topics\nProcessing article 1020... ✓ Found 3 topics\nProcessing article 1021... ✓ Found 1 topics\nProcessing article 1022... ✓ Found 0 topics\nProcessing article 1023... ✓ Found 4 topics\nProcessing article 1024... ✓ Found 1 topics\nProcessing article 1025... ✓ Found 1 topics\nProcessing article 1026... ✓ Found 4 topics\nProcessing article 1027... ✓ Found 4 topics\nProcessing article 1028... ✓ Found 3 topics\nProcessing article 1029... ✓ Found 2 topics\nProcessing article 1030... ✓ Found 2 topics\nProcessing article 1031... ✓ Found 3 topics\nProcessing article 1032... ✓ Found 5 topics\nProcessing article 1033... ✓ Found 1 topics\nProcessing article 1034... ✓ Found 5 topics\nProcessing article 1035... ✓ Found 3 topics\nProcessing article 1036... ✓ Found 3 topics\nProcessing article 1037... ✓ Found 3 topics\nProcessing article 1038... ✓ Found 2 topics\nProcessing article 1039... ✓ Found 2 topics\nProcessing article 1040... ✓ Found 2 topics\nProcessing article 1041... ✓ Found 1 topics\nProcessing article 1042... ✓ Found 2 topics\nProcessing article 1043... ✓ Found 4 topics\nProcessing article 1044... ✓ Found 1 topics\nProcessing article 1045... ✓ Found 2 topics\nProcessing article 1046... ✓ Found 1 topics\nProcessing article 1047... ✓ Found 5 topics\nProcessing article 1048... ✓ Found 2 topics\nProcessing article 1049... ✓ Found 5 topics\nProcessing article 1050... ✓ Found 4 topics\nProcessing article 1051... ✓ Found 2 topics\nProcessing article 1052... ✓ Found 4 topics\nProcessing article 1053... ✓ Found 3 topics\nProcessing article 1054... ✓ Found 3 topics\nProcessing article 1055... ✓ Found 4 topics\nProcessing article 1056... ✓ Found 2 topics\nProcessing article 1057... ✓ Found 2 topics\nProcessing article 1058... ✓ Found 2 topics\nProcessing article 1059... ✓ Found 0 topics\nProcessing article 1060... ✓ Found 2 topics\nProcessing article 1061... ✓ Found 3 topics\nProcessing article 1062... ✓ Found 3 topics\nProcessing article 1063... ✓ Found 3 topics\nProcessing article 1064... ✓ Found 0 topics\nProcessing article 1065... ✓ Found 2 topics\nProcessing article 1066... ✓ Found 2 topics\nProcessing article 1067... ✓ Found 3 topics\nProcessing article 1068... ✓ Found 4 topics\nProcessing article 1069... ✓ Found 2 topics\nProcessing article 1070... ✓ Found 3 topics\nProcessing article 1071... ✓ Found 5 topics\nProcessing article 1072... ✓ Found 5 topics\nProcessing article 1073... ✓ Found 5 topics\nProcessing article 1074... ✓ Found 5 topics\nProcessing article 1075... ✓ Found 3 topics\nProcessing article 1076... ✓ Found 5 topics\nProcessing article 1077... ✓ Found 1 topics\nProcessing article 1078... ✓ Found 5 topics\nProcessing article 1079... ✓ Found 3 topics\nProcessing article 1080... ✓ Found 4 topics\nProcessing article 1081... ✓ Found 4 topics\nProcessing article 1082... ✓ Found 2 topics\nProcessing article 1083... ✓ Found 2 topics\nProcessing article 1084... ✓ Found 5 topics\nProcessing article 1085... ✓ Found 2 topics\nProcessing article 1086... ✓ Found 5 topics\nProcessing article 1087... ✓ Found 2 topics\nProcessing article 1088... ✓ Found 2 topics\nProcessing article 1089... ✓ Found 2 topics\nProcessing article 1090... ✓ Found 2 topics\nProcessing article 1091... ✓ Found 3 topics\nProcessing article 1092... ✓ Found 0 topics\nProcessing article 1093... ✓ Found 1 topics\nProcessing article 1094... ✓ Found 4 topics\nProcessing article 1095... ✓ Found 2 topics\nProcessing article 1096... ✓ Found 1 topics\nProcessing article 1097... ✓ Found 4 topics\nProcessing article 1098... ✓ Found 3 topics\nProcessing article 1099... ✓ Found 2 topics\nProcessing article 1100... ✓ Found 1 topics\nProcessing article 1101... ✓ Found 5 topics\nProcessing article 1102... ✓ Found 2 topics\nProcessing article 1103... ✓ Found 2 topics\nProcessing article 1104... ✓ Found 3 topics\nProcessing article 1105... ✓ Found 2 topics\nProcessing article 1106... ✓ Found 2 topics\nProcessing article 1107... ✓ Found 2 topics\nProcessing article 1108... ✓ Found 0 topics\nProcessing article 1109... ✓ Found 5 topics\nProcessing article 1110... ✓ Found 0 topics\nProcessing article 1111... ✓ Found 4 topics\nProcessing article 1112... ✓ Found 5 topics\nProcessing article 1113... ✓ Found 1 topics\nProcessing article 1114... ✓ Found 6 topics\nProcessing article 1115... ✓ Found 3 topics\nProcessing article 1116... ✓ Found 0 topics\nProcessing article 1117... ✓ Found 2 topics\nProcessing article 1118... ✓ Found 3 topics\nProcessing article 1119... ✓ Found 2 topics\nProcessing article 1120... ✓ Found 0 topics\nProcessing article 1121... ✓ Found 2 topics\nProcessing article 1122... ✓ Found 3 topics\nProcessing article 1123... ✓ Found 1 topics\nProcessing article 1124... ✓ Found 0 topics\nProcessing article 1125... ✓ Found 0 topics\nProcessing article 1126... ✓ Found 1 topics\nProcessing article 1127... ✓ Found 0 topics\nProcessing article 1128... ✓ Found 3 topics\nProcessing article 1129... ✓ Found 2 topics\nProcessing article 1130... ✓ Found 3 topics\nProcessing article 1131... ✓ Found 3 topics\nProcessing article 1132... ✓ Found 5 topics\nProcessing article 1133... ✓ Found 5 topics\nProcessing article 1134... ✓ Found 2 topics\nProcessing article 1135... ✓ Found 3 topics\nProcessing article 1136... ✓ Found 0 topics\nProcessing article 1137... ✓ Found 2 topics\nProcessing article 1138... ✓ Found 5 topics\nProcessing article 1139... ✓ Found 4 topics\nProcessing article 1140... ✓ Found 1 topics\nProcessing article 1141... ✓ Found 2 topics\nProcessing article 1142... ✓ Found 2 topics\nProcessing article 1143... ✓ Found 4 topics\nProcessing article 1144... ✓ Found 1 topics\nProcessing article 1145... ✓ Found 4 topics\nProcessing article 1146... ✓ Found 3 topics\nProcessing article 1147... ✓ Found 1 topics\nProcessing article 1148... ✓ Found 3 topics\nProcessing article 1149... ✓ Found 2 topics\nProcessing article 1150... ✓ Found 1 topics\nProcessing article 1151... ✓ Found 3 topics\nProcessing article 1152... ✓ Found 2 topics\nProcessing article 1153... ✓ Found 5 topics\nProcessing article 1154... ✓ Found 5 topics\nProcessing article 1155... ✓ Found 5 topics\nProcessing article 1156... ✓ Found 5 topics\nProcessing article 1157... ✓ Found 1 topics\nProcessing article 1158... ✓ Found 2 topics\nProcessing article 1159... ✓ Found 2 topics\nProcessing article 1160... ✓ Found 1 topics\nProcessing article 1161... ✓ Found 3 topics\nProcessing article 1162... ✓ Found 2 topics\nProcessing article 1163... ✓ Found 5 topics\nProcessing article 1164... ✓ Found 5 topics\nProcessing article 1165... ✓ Found 5 topics\nProcessing article 1166... ✓ Found 5 topics\nProcessing article 1167... ✓ Found 1 topics\nProcessing article 1168... ✓ Found 2 topics\nProcessing article 1169... ✓ Found 4 topics\nProcessing article 1170... ✓ Found 0 topics\nProcessing article 1171... ✓ Found 4 topics\nProcessing article 1172... ✓ Found 2 topics\nProcessing article 1173... ✓ Found 2 topics\nProcessing article 1174... ✓ Found 2 topics\nProcessing article 1175... ✓ Found 4 topics\nProcessing article 1176... ✓ Found 0 topics\nProcessing article 1177... ✓ Found 3 topics\nProcessing article 1178... ✓ Found 2 topics\nProcessing article 1179... ✓ Found 1 topics\nProcessing article 1180... ✓ Found 2 topics\nProcessing article 1181... ✓ Found 3 topics\nProcessing article 1182... ✓ Found 5 topics\nProcessing article 1183... ✓ Found 5 topics\nProcessing article 1184... ✓ Found 2 topics\nProcessing article 1185... ✓ Found 3 topics\nProcessing article 1186... ✓ Found 0 topics\nProcessing article 1187... ✓ Found 4 topics\nProcessing article 1188... ✓ Found 4 topics\nProcessing article 1189... ✓ Found 1 topics\nProcessing article 1190... ✓ Found 0 topics\nProcessing article 1191... ✓ Found 4 topics\nProcessing article 1192... ✓ Found 5 topics\nProcessing article 1193... ✓ Found 3 topics\nProcessing article 1194... ✓ Found 3 topics\nProcessing article 1195... ✓ Found 3 topics\nProcessing article 1196... ✓ Found 3 topics\nProcessing article 1197... ✓ Found 0 topics\nProcessing article 1198... ✓ Found 2 topics\nProcessing article 1199... ✓ Found 2 topics\nProcessing article 1200... ✓ Found 3 topics\nProcessing article 1201... ✓ Found 5 topics\nProcessing article 1202... ✓ Found 3 topics\nProcessing article 1203... ✓ Found 2 topics\nProcessing article 1204... ✓ Found 2 topics\nProcessing article 1205... ✓ Found 4 topics\nProcessing article 1206... ✓ Found 5 topics\nProcessing article 1207... ✓ Found 1 topics\nProcessing article 1208... ✓ Found 3 topics\nProcessing article 1209... ✓ Found 3 topics\nProcessing article 1210... ✓ Found 2 topics\nProcessing article 1211... ✓ Found 0 topics\nProcessing article 1212... ✓ Found 2 topics\nProcessing article 1213... ✓ Found 3 topics\nProcessing article 1214... ✓ Found 2 topics\nProcessing article 1215... ✓ Found 2 topics\nProcessing article 1216... ✓ Found 5 topics\nProcessing article 1217... ✓ Found 2 topics\nProcessing article 1218... ✓ Found 1 topics\nProcessing article 1219... ✓ Found 4 topics\nProcessing article 1220... ✓ Found 2 topics\nProcessing article 1221... ✓ Found 2 topics\nProcessing article 1222... ✓ Found 2 topics\nProcessing article 1223... ✓ Found 4 topics\nProcessing article 1224... ✓ Found 3 topics\nProcessing article 1225... ✓ Found 5 topics\nProcessing article 1226... ✓ Found 1 topics\nProcessing article 1227... ✓ Found 2 topics\nProcessing article 1228... ✓ Found 5 topics\nProcessing article 1229... ✓ Found 2 topics\nProcessing article 1230... ✓ Found 2 topics\nProcessing article 1231... ✓ Found 5 topics\nProcessing article 1232... ✓ Found 5 topics\nProcessing article 1233... ✓ Found 4 topics\nProcessing article 1234... ✓ Found 1 topics\nProcessing article 1235... ✓ Found 2 topics\nProcessing article 1236... ✓ Found 1 topics\nProcessing article 1237... ✓ Found 3 topics\nProcessing article 1238... ✓ Found 5 topics\nProcessing article 1239... ✓ Found 2 topics\nProcessing article 1240... ✓ Found 2 topics\nProcessing article 1241... ✓ Found 4 topics\nProcessing article 1242... ✓ Found 3 topics\nProcessing article 1243... ✓ Found 5 topics\nProcessing article 1244... ✓ Found 5 topics\nProcessing article 1245... ✓ Found 3 topics\nProcessing article 1246... ✓ Found 5 topics\nProcessing article 1247... ✓ Found 4 topics\nProcessing article 1248... ✓ Found 2 topics\nProcessing article 1249... ✓ Found 5 topics\nProcessing article 1250... ✓ Found 2 topics\nProcessing article 1251... ✓ Found 2 topics\nProcessing article 1252... ✓ Found 4 topics\nProcessing article 1253... ✓ Found 2 topics\nProcessing article 1254... ✓ Found 3 topics\nProcessing article 1255... ✓ Found 2 topics\nProcessing article 1256... ✓ Found 5 topics\nProcessing article 1257... ✓ Found 2 topics\nProcessing article 1258... ✓ Found 1 topics\nProcessing article 1259... ✓ Found 2 topics\nProcessing article 1260... ✓ Found 5 topics\nProcessing article 1261... ✓ Found 4 topics\nProcessing article 1262... ✓ Found 2 topics\nProcessing article 1263... ✓ Found 2 topics\nProcessing article 1264... ✓ Found 4 topics\nProcessing article 1265... ✓ Found 2 topics\nProcessing article 1266... ✓ Found 1 topics\nProcessing article 1267... ✓ Found 3 topics\nProcessing article 1268... ✓ Found 2 topics\nProcessing article 1269... ✓ Found 3 topics\nProcessing article 1270... ✓ Found 5 topics\nProcessing article 1271... ✓ Found 2 topics\nProcessing article 1272... ✓ Found 3 topics\nProcessing article 1273... ✓ Found 3 topics\nProcessing article 1274... ✓ Found 4 topics\nProcessing article 1275... ✓ Found 3 topics\nProcessing article 1276... ✓ Found 1 topics\nProcessing article 1277... ✓ Found 5 topics\nProcessing article 1278... ✓ Found 4 topics\nProcessing article 1279... ✓ Found 2 topics\nProcessing article 1280... ✓ Found 1 topics\nProcessing article 1281... ✓ Found 3 topics\nProcessing article 1282... ✓ Found 1 topics\nProcessing article 1283... ✓ Found 2 topics\nProcessing article 1284... ✓ Found 1 topics\nProcessing article 1285... ✓ Found 3 topics\nProcessing article 1286... ✓ Found 3 topics\nProcessing article 1287... ✓ Found 2 topics\nProcessing article 1288... ✓ Found 1 topics\nProcessing article 1289... ✓ Found 2 topics\nProcessing article 1290... ✓ Found 3 topics\nProcessing article 1291... ✓ Found 4 topics\nProcessing article 1292... ✓ Found 1 topics\nProcessing article 1293... ✓ Found 5 topics\nProcessing article 1294... ✓ Found 3 topics\nProcessing article 1295... ✓ Found 5 topics\nProcessing article 1296... ✓ Found 1 topics\nProcessing article 1297... ✓ Found 5 topics\nProcessing article 1298... ✓ Found 2 topics\nProcessing article 1299... ✓ Found 2 topics\nProcessing article 1300... ✓ Found 2 topics\nProcessing article 1301... ✓ Found 2 topics\nProcessing article 1302... ✓ Found 3 topics\nProcessing article 1303... ✓ Found 2 topics\nProcessing article 1304... ✓ Found 5 topics\nProcessing article 1305... ✓ Found 2 topics\nProcessing article 1306... ✓ Found 2 topics\nProcessing article 1307... ✓ Found 3 topics\nProcessing article 1308... ✓ Found 4 topics\nProcessing article 1309... ✓ Found 2 topics\nProcessing article 1310... ✓ Found 2 topics\nProcessing article 1311... ✓ Found 1 topics\nProcessing article 1312... ✓ Found 0 topics\nProcessing article 1313... ✓ Found 3 topics\nProcessing article 1314... ✓ Found 2 topics\nProcessing article 1315... ✓ Found 3 topics\nProcessing article 1316... ✓ Found 2 topics\nProcessing article 1317... ✓ Found 2 topics\nProcessing article 1318... ✓ Found 5 topics\nProcessing article 1319... ✓ Found 3 topics\nProcessing article 1320... ✓ Found 4 topics\nProcessing article 1321... ✓ Found 2 topics\nProcessing article 1322... ✓ Found 2 topics\nProcessing article 1323... ✓ Found 4 topics\nProcessing article 1324... ✓ Found 5 topics\nProcessing article 1325... ✓ Found 2 topics\nProcessing article 1326... ✓ Found 4 topics\nProcessing article 1327... ✓ Found 2 topics\nProcessing article 1328... ✓ Found 5 topics\nProcessing article 1329... ✓ Found 4 topics\nProcessing article 1330... ✓ Found 0 topics\nProcessing article 1331... ✓ Found 2 topics\nProcessing article 1332... ✓ Found 3 topics\nProcessing article 1333... ✓ Found 0 topics\nProcessing article 1334... ✓ Found 2 topics\nProcessing article 1335... ✓ Found 5 topics\nProcessing article 1336... ✓ Found 5 topics\nProcessing article 1337... ✓ Found 1 topics\nProcessing article 1338... ✓ Found 1 topics\nProcessing article 1339... ✓ Found 1 topics\nProcessing article 1340... ✓ Found 5 topics\nProcessing article 1341... ✓ Found 2 topics\nProcessing article 1342... ✓ Found 2 topics\nProcessing article 1343... ✓ Found 5 topics\nProcessing article 1344... ✓ Found 5 topics\nProcessing article 1345... ✓ Found 1 topics\nProcessing article 1346... ✓ Found 4 topics\nProcessing article 1347... ✓ Found 3 topics\nProcessing article 1348... ✓ Found 4 topics\nProcessing article 1349... ✓ Found 5 topics\nProcessing article 1350... ✓ Found 2 topics\nProcessing article 1351... ✓ Found 2 topics\nProcessing article 1352... ✓ Found 0 topics\nProcessing article 1353... ✓ Found 0 topics\nProcessing article 1354... ✓ Found 5 topics\nProcessing article 1355... ✓ Found 1 topics\nProcessing article 1356... ✓ Found 5 topics\nProcessing article 1357... ✓ Found 2 topics\nProcessing article 1358... ✓ Found 1 topics\nProcessing article 1359... ✓ Found 4 topics\nProcessing article 1360... ✓ Found 1 topics\nProcessing article 1361... ✓ Found 3 topics\nProcessing article 1362... ✓ Found 2 topics\nProcessing article 1363... ✓ Found 5 topics\nProcessing article 1364... ✓ Found 5 topics\nProcessing article 1365... ✓ Found 3 topics\nProcessing article 1366... ✓ Found 4 topics\nProcessing article 1367... ✓ Found 2 topics\nProcessing article 1368... ✓ Found 3 topics\nProcessing article 1369... ✓ Found 5 topics\nProcessing article 1370... ✓ Found 2 topics\nProcessing article 1371... ✓ Found 0 topics\nProcessing article 1372... ✓ Found 5 topics\nProcessing article 1373... ✓ Found 4 topics\nProcessing article 1374... ✓ Found 1 topics\nProcessing article 1375... ✓ Found 5 topics\nProcessing article 1376... ✓ Found 0 topics\nProcessing article 1377... ✓ Found 3 topics\nProcessing article 1378... ✓ Found 2 topics\nProcessing article 1379... ✓ Found 3 topics\nProcessing article 1380... ✓ Found 4 topics\nProcessing article 1381... ✓ Found 0 topics\nProcessing article 1382... ✓ Found 1 topics\nProcessing article 1383... ✓ Found 2 topics\nProcessing article 1384... ✓ Found 3 topics\nProcessing article 1385... ✓ Found 1 topics\nProcessing article 1386... ✓ Found 2 topics\nProcessing article 1387... ✓ Found 2 topics\nProcessing article 1388... ✓ Found 2 topics\nProcessing article 1389... ✓ Found 2 topics\nProcessing article 1390... ✓ Found 2 topics\nProcessing article 1391... ✓ Found 3 topics\nProcessing article 1392... ✓ Found 0 topics\nProcessing article 1393... ✓ Found 2 topics\nProcessing article 1394... ✓ Found 5 topics\nProcessing article 1395... ✓ Found 2 topics\nProcessing article 1396... ✓ Found 1 topics\nProcessing article 1397... ✓ Found 4 topics\nProcessing article 1398... ✓ Found 5 topics\nProcessing article 1399... ✓ Found 1 topics\nProcessing article 1400... ✓ Found 2 topics\nProcessing article 1401... ✓ Found 3 topics\nProcessing article 1402... ✓ Found 0 topics\nProcessing article 1403... ✓ Found 2 topics\nProcessing article 1404... ✓ Found 1 topics\nProcessing article 1405... ✓ Found 0 topics\nProcessing article 1406... ✓ Found 1 topics\nProcessing article 1407... ✓ Found 5 topics\nProcessing article 1408... ✓ Found 5 topics\nProcessing article 1409... ✓ Found 2 topics\nProcessing article 1410... ✓ Found 2 topics\nProcessing article 1411... ✓ Found 2 topics\nProcessing article 1412... ✓ Found 0 topics\nProcessing article 1413... ✓ Found 4 topics\nProcessing article 1414... ✓ Found 0 topics\nProcessing article 1415... ✓ Found 1 topics\nProcessing article 1416... ✓ Found 3 topics\nProcessing article 1417... ✓ Found 5 topics\nProcessing article 1418... ✓ Found 0 topics\nProcessing article 1419... ✓ Found 5 topics\nProcessing article 1420... ✓ Found 5 topics\nProcessing article 1421... ✓ Found 1 topics\nProcessing article 1422... ✓ Found 1 topics\nProcessing article 1423... ✓ Found 5 topics\nProcessing article 1424... ✓ Found 4 topics\nProcessing article 1425... ✓ Found 3 topics\nProcessing article 1426... ✓ Found 2 topics\nProcessing article 1427... ✓ Found 3 topics\nProcessing article 1428... ✓ Found 2 topics\nProcessing article 1429... ✓ Found 3 topics\nProcessing article 1430... ✓ Found 2 topics\nProcessing article 1431... ✓ Found 3 topics\nProcessing article 1432... ✓ Found 1 topics\nProcessing article 1433... ✓ Found 5 topics\nProcessing article 1434... ✓ Found 4 topics\nProcessing article 1435... ✓ Found 3 topics\nProcessing article 1436... ✓ Found 2 topics\nProcessing article 1437... ✓ Found 3 topics\nProcessing article 1438... ✓ Found 2 topics\nProcessing article 1439... ✓ Found 3 topics\nProcessing article 1440... ✓ Found 2 topics\nProcessing article 1441... ✓ Found 3 topics\nProcessing article 1442... ✓ Found 3 topics\nProcessing article 1443... ✓ Found 5 topics\nProcessing article 1444... ✓ Found 2 topics\nProcessing article 1445... ✓ Found 2 topics\nProcessing article 1446... ✓ Found 3 topics\nProcessing article 1447... ✓ Found 2 topics\nProcessing article 1448... ✓ Found 2 topics\nProcessing article 1449... ✓ Found 4 topics\nProcessing article 1450... ✓ Found 0 topics\nProcessing article 1451... ✓ Found 2 topics\nProcessing article 1452... ✓ Found 2 topics\nProcessing article 1453... ✓ Found 2 topics\nProcessing article 1454... ✓ Found 2 topics\nProcessing article 1455... ✓ Found 5 topics\nProcessing article 1456... ✓ Found 1 topics\nProcessing article 1457... ✓ Found 5 topics\nProcessing article 1458... ✓ Found 2 topics\nProcessing article 1459... ✓ Found 1 topics\nProcessing article 1460... ✓ Found 4 topics\nProcessing article 1461... ✓ Found 3 topics\nProcessing article 1462... ✓ Found 1 topics\nProcessing article 1463... ✓ Found 3 topics\nProcessing article 1464... ✓ Found 3 topics\nProcessing article 1465... ✓ Found 4 topics\nProcessing article 1466... ✓ Found 2 topics\nProcessing article 1467... ✓ Found 0 topics\nProcessing article 1468... ✓ Found 0 topics\nProcessing article 1469... ✓ Found 1 topics\nProcessing article 1470... ✓ Found 2 topics\nProcessing article 1471... ✓ Found 5 topics\nProcessing article 1472... ✓ Found 3 topics\nProcessing article 1473... ✓ Found 1 topics\nProcessing article 1474... ✓ Found 2 topics\nProcessing article 1475... ✓ Found 2 topics\nProcessing article 1476... ✓ Found 5 topics\nProcessing article 1477... ✓ Found 1 topics\nProcessing article 1478... ✓ Found 5 topics\nProcessing article 1479... ✓ Found 2 topics\nProcessing article 1480... ✓ Found 1 topics\nProcessing article 1481... ✓ Found 2 topics\nProcessing article 1482... ✓ Found 2 topics\nProcessing article 1483... ✓ Found 1 topics\nProcessing article 1484... ✓ Found 5 topics\nProcessing article 1485... ✓ Found 4 topics\nProcessing article 1486... ✓ Found 1 topics\nProcessing article 1487... ✓ Found 5 topics\nProcessing article 1488... ✓ Found 2 topics\nProcessing article 1489... ✓ Found 1 topics\nProcessing article 1490... ✓ Found 3 topics\nProcessing article 1491... ✓ Found 2 topics\nProcessing article 1492... ✓ Found 3 topics\nProcessing article 1493... ✓ Found 1 topics\nProcessing article 1494... ✓ Found 3 topics\nProcessing article 1495... ✓ Found 3 topics\nProcessing article 1496... ✓ Found 2 topics\nProcessing article 1497... ✓ Found 3 topics\nProcessing article 1498... ✓ Found 0 topics\nProcessing article 1499... ✓ Found 0 topics\nProcessing article 1500... ✓ Found 5 topics\nProcessing article 1501... ✓ Found 5 topics\nProcessing article 1502... ✓ Found 0 topics\nProcessing article 1503... ✓ Found 3 topics\nProcessing article 1504... ✓ Found 1 topics\nProcessing article 1505... ✓ Found 5 topics\nProcessing article 1506... ✓ Found 2 topics\nProcessing article 1507... ✓ Found 4 topics\nProcessing article 1508... ✓ Found 1 topics\nProcessing article 1509... ✓ Found 5 topics\nProcessing article 1510... ✓ Found 2 topics\nProcessing article 1511... ✓ Found 2 topics\nProcessing article 1512... ✓ Found 2 topics\nProcessing article 1513... ✓ Found 5 topics\nProcessing article 1514... ✓ Found 3 topics\nProcessing article 1515... ✓ Found 5 topics\nProcessing article 1516... ✓ Found 2 topics\nProcessing article 1517... ✓ Found 5 topics\nProcessing article 1518... ✓ Found 3 topics\nProcessing article 1519... ✓ Found 5 topics\nProcessing article 1520... ✓ Found 3 topics\nProcessing article 1521... ✓ Found 2 topics\nProcessing article 1522... ✓ Found 5 topics\nProcessing article 1523... ✓ Found 2 topics\nProcessing article 1524... ✓ Found 3 topics\nProcessing article 1525... ✓ Found 3 topics\nProcessing article 1526... ✓ Found 2 topics\nProcessing article 1527... ✓ Found 1 topics\nProcessing article 1528... ✓ Found 0 topics\nProcessing article 1529... ✓ Found 1 topics\nProcessing article 1530... ✓ Found 0 topics\nProcessing article 1531... ✓ Found 3 topics\nProcessing article 1532... ✓ Found 4 topics\nProcessing article 1533... ✓ Found 3 topics\nProcessing article 1534... ✓ Found 4 topics\nProcessing article 1535... ✓ Found 3 topics\nProcessing article 1536... ✓ Found 2 topics\nProcessing article 1537... ✓ Found 3 topics\nProcessing article 1538... ✓ Found 5 topics\nProcessing article 1539... ✓ Found 2 topics\nProcessing article 1540... ✓ Found 1 topics\nProcessing article 1541... ✓ Found 4 topics\nProcessing article 1542... ✓ Found 5 topics\nProcessing article 1543... ✓ Found 4 topics\nProcessing article 1544... ✓ Found 0 topics\nProcessing article 1545... ✓ Found 1 topics\nProcessing article 1546... ✓ Found 5 topics\nProcessing article 1547... ✓ Found 4 topics\nProcessing article 1548... ✓ Found 5 topics\nProcessing article 1549... ✓ Found 1 topics\nProcessing article 1550... ✓ Found 3 topics\nProcessing article 1551... ✓ Found 2 topics\nProcessing article 1552... ✓ Found 1 topics\nProcessing article 1553... ✓ Found 5 topics\nProcessing article 1554... ✓ Found 2 topics\nProcessing article 1555... ✓ Found 3 topics\nProcessing article 1556... ✓ Found 5 topics\nProcessing article 1557... ✓ Found 1 topics\nProcessing article 1558... ✓ Found 4 topics\nProcessing article 1559... ✓ Found 1 topics\nProcessing article 1560... ✓ Found 4 topics\nProcessing article 1561... ✓ Found 2 topics\nProcessing article 1562... ✓ Found 3 topics\nProcessing article 1563... ✓ Found 2 topics\nProcessing article 1564... ✓ Found 3 topics\nProcessing article 1565... ✓ Found 1 topics\nProcessing article 1566... ✓ Found 1 topics\nProcessing article 1567... ✓ Found 1 topics\nProcessing article 1568... ✓ Found 1 topics\nProcessing article 1569... ✓ Found 5 topics\nProcessing article 1570... ✓ Found 0 topics\nProcessing article 1571... ✓ Found 5 topics\nProcessing article 1572... ✓ Found 2 topics\nProcessing article 1573... ✓ Found 4 topics\nProcessing article 1574... ✓ Found 1 topics\nProcessing article 1575... ✓ Found 3 topics\nProcessing article 1576... ✓ Found 2 topics\nProcessing article 1577... ✓ Found 4 topics\nProcessing article 1578... ✓ Found 1 topics\nProcessing article 1579... ✓ Found 3 topics\nProcessing article 1580... ✓ Found 4 topics\nProcessing article 1581... ✓ Found 3 topics\nProcessing article 1582... ✓ Found 3 topics\nProcessing article 1583... ✓ Found 2 topics\nProcessing article 1584... ✓ Found 5 topics\nProcessing article 1585... ✓ Found 2 topics\nProcessing article 1586... ✓ Found 0 topics\nProcessing article 1587... ✓ Found 5 topics\nProcessing article 1588... ✓ Found 5 topics\nProcessing article 1589... ✓ Found 1 topics\nProcessing article 1590... ✓ Found 3 topics\nProcessing article 1591... ✓ Found 1 topics\nProcessing article 1592... ✓ Found 0 topics\nProcessing article 1593... ✓ Found 1 topics\nProcessing article 1594... ✓ Found 2 topics\nProcessing article 1595... ✓ Found 5 topics\nProcessing article 1596... ✓ Found 4 topics\nProcessing article 1597... ✓ Found 0 topics\nProcessing article 1598... ✓ Found 2 topics\nProcessing article 1599... ✓ Found 5 topics\nProcessing article 1600... ✓ Found 3 topics\nProcessing article 1601... ✓ Found 3 topics\nProcessing article 1602... ✓ Found 0 topics\nProcessing article 1603... ✓ Found 2 topics\nProcessing article 1604... ✓ Found 2 topics\nProcessing article 1605... ✓ Found 3 topics\nProcessing article 1606... ✓ Found 3 topics\nProcessing article 1607... ✓ Found 3 topics\nProcessing article 1608... ✓ Found 3 topics\nProcessing article 1609... ✓ Found 1 topics\nProcessing article 1610... ✓ Found 2 topics\nProcessing article 1611... ✓ Found 2 topics\nProcessing article 1612... ✓ Found 1 topics\nProcessing article 1613... ✓ Found 0 topics\nProcessing article 1614... ✓ Found 4 topics\nProcessing article 1615... ✓ Found 2 topics\nProcessing article 1616... ✓ Found 2 topics\nProcessing article 1617... ✓ Found 5 topics\nProcessing article 1618... ✓ Found 3 topics\nProcessing article 1619... ✓ Found 3 topics\nProcessing article 1620... ✓ Found 2 topics\nProcessing article 1621... ✓ Found 2 topics\nProcessing article 1622... ✓ Found 5 topics\nProcessing article 1623... ✓ Found 4 topics\nProcessing article 1624... ✓ Found 2 topics\nProcessing article 1625... ✓ Found 1 topics\nProcessing article 1626... ✓ Found 4 topics\nProcessing article 1627... ✓ Found 4 topics\nProcessing article 1628... ✓ Found 5 topics\nProcessing article 1629... ✓ Found 3 topics\nProcessing article 1630... ✓ Found 5 topics\nProcessing article 1631... ✓ Found 2 topics\nProcessing article 1632... ✓ Found 4 topics\nProcessing article 1633... ✓ Found 2 topics\nProcessing article 1634... ✓ Found 2 topics\nProcessing article 1635... ✓ Found 1 topics\nProcessing article 1636... ✓ Found 3 topics\nProcessing article 1637... ✓ Found 2 topics\nProcessing article 1638... ✓ Found 0 topics\nProcessing article 1639... ✓ Found 1 topics\nProcessing article 1640... ✓ Found 1 topics\nProcessing article 1641... ✓ Found 1 topics\nProcessing article 1642... ✓ Found 5 topics\nProcessing article 1643... ✓ Found 2 topics\nProcessing article 1644... ✓ Found 2 topics\nProcessing article 1645... ✓ Found 3 topics\nProcessing article 1646... ✓ Found 2 topics\nProcessing article 1647... ✓ Found 4 topics\nProcessing article 1648... ✓ Found 1 topics\nProcessing article 1649... ✓ Found 5 topics\nProcessing article 1650... ✓ Found 2 topics\nProcessing article 1651... ✓ Found 5 topics\nProcessing article 1652... ✓ Found 2 topics\nProcessing article 1653... ✓ Found 4 topics\nProcessing article 1654... ✓ Found 2 topics\nProcessing article 1655... ✓ Found 1 topics\nProcessing article 1656... ✓ Found 7 topics\nProcessing article 1657... ✓ Found 0 topics\nProcessing article 1658... ✓ Found 3 topics\nProcessing article 1659... ✓ Found 5 topics\nProcessing article 1660... ✓ Found 5 topics\nProcessing article 1661... ✓ Found 4 topics\nProcessing article 1662... ✓ Found 1 topics\nProcessing article 1663... ✓ Found 2 topics\nProcessing article 1664... ✓ Found 2 topics\nProcessing article 1665... ✓ Found 5 topics\nProcessing article 1666... ✓ Found 4 topics\nProcessing article 1667... ✓ Found 4 topics\nProcessing article 1668... ✓ Found 2 topics\nProcessing article 1669... ✓ Found 0 topics\nProcessing article 1670... ✓ Found 2 topics\nProcessing article 1671... ✓ Found 3 topics\nProcessing article 1672... ✓ Found 0 topics\nProcessing article 1673... ✓ Found 1 topics\nProcessing article 1674... ✓ Found 3 topics\nProcessing article 1675... ✓ Found 1 topics\nProcessing article 1676... ✓ Found 4 topics\nProcessing article 1677... ✓ Found 0 topics\nProcessing article 1678... ✓ Found 3 topics\nProcessing article 1679... ✓ Found 5 topics\nProcessing article 1680... ✓ Found 2 topics\nProcessing article 1681... ✓ Found 0 topics\nProcessing article 1682... ✓ Found 5 topics\nProcessing article 1683... ✓ Found 0 topics\nProcessing article 1684... ✓ Found 1 topics\nProcessing article 1685... ✓ Found 3 topics\nProcessing article 1686... ✓ Found 4 topics\nProcessing article 1687... ✓ Found 0 topics\nProcessing article 1688... ✓ Found 5 topics\nProcessing article 1689... ✓ Found 1 topics\nProcessing article 1690... ✓ Found 5 topics\nProcessing article 1691... ✓ Found 5 topics\nProcessing article 1692... ✓ Found 3 topics\nProcessing article 1693... ✓ Found 1 topics\nProcessing article 1694... ✓ Found 0 topics\nProcessing article 1695... ✓ Found 5 topics\nProcessing article 1696... ✓ Found 5 topics\nProcessing article 1697... ✓ Found 3 topics\nProcessing article 1698... ✓ Found 1 topics\nProcessing article 1699... ✓ Found 4 topics\nProcessing article 1700... ✓ Found 3 topics\nProcessing article 1701... ✓ Found 5 topics\nProcessing article 1702... ✓ Found 2 topics\nProcessing article 1703... ✓ Found 3 topics\nProcessing article 1704... ✓ Found 3 topics\nProcessing article 1705... ✓ Found 4 topics\nProcessing article 1706... ✓ Found 3 topics\nProcessing article 1707... ✓ Found 2 topics\nProcessing article 1708... ✓ Found 5 topics\nProcessing article 1709... ✓ Found 1 topics\nProcessing article 1710... ✓ Found 0 topics\nProcessing article 1711... ✓ Found 2 topics\nProcessing article 1712... ✓ Found 5 topics\nProcessing article 1713... ✓ Found 4 topics\nProcessing article 1714... ✓ Found 1 topics\nProcessing article 1715... ✓ Found 3 topics\nProcessing article 1716... ✓ Found 2 topics\nProcessing article 1717... ✓ Found 2 topics\nProcessing article 1718... ✓ Found 5 topics\nProcessing article 1719... ✓ Found 1 topics\nProcessing article 1720... ✓ Found 0 topics\nProcessing article 1721... ✓ Found 2 topics\nProcessing article 1722... ✓ Found 5 topics\nProcessing article 1723... ✓ Found 4 topics\nProcessing article 1724... ✓ Found 1 topics\nProcessing article 1725... ✓ Found 3 topics\nProcessing article 1726... ✓ Found 2 topics\nProcessing article 1727... ✓ Found 1 topics\nProcessing article 1728... ✓ Found 1 topics\nProcessing article 1729... ✓ Found 5 topics\nProcessing article 1730... ✓ Found 2 topics\nProcessing article 1731... ✓ Found 3 topics\nProcessing article 1732... ✓ Found 2 topics\nProcessing article 1733... ✓ Found 4 topics\nProcessing article 1734... ✓ Found 1 topics\nProcessing article 1735... ✓ Found 5 topics\nProcessing article 1736... ✓ Found 1 topics\nProcessing article 1737... ✓ Found 2 topics\nProcessing article 1738... ✓ Found 2 topics\nProcessing article 1739... ✓ Found 3 topics\nProcessing article 1740... ✓ Found 1 topics\nProcessing article 1741... ✓ Found 5 topics\nProcessing article 1742... ✓ Found 2 topics\nProcessing article 1743... ✓ Found 0 topics\nProcessing article 1744... ✓ Found 4 topics\nProcessing article 1745... ✓ Found 3 topics\nProcessing article 1746... ✓ Found 5 topics\nProcessing article 1747... ✓ Found 0 topics\nProcessing article 1748... ✓ Found 6 topics\nProcessing article 1749... ✓ Found 2 topics\nProcessing article 1750... ✓ Found 5 topics\nProcessing article 1751... ✓ Found 2 topics\nProcessing article 1752... ✓ Found 2 topics\nProcessing article 1753... ✓ Found 1 topics\nProcessing article 1754... ✓ Found 5 topics\nProcessing article 1755... ✓ Found 3 topics\nProcessing article 1756... ✓ Found 2 topics\nProcessing article 1757... ✓ Found 2 topics\nProcessing article 1758... ✓ Found 4 topics\nProcessing article 1759... ✓ Found 2 topics\nProcessing article 1760... ✓ Found 5 topics\nProcessing article 1761... ✓ Found 4 topics\nProcessing article 1762... ✓ Found 5 topics\nProcessing article 1763... ✓ Found 2 topics\nProcessing article 1764... ✓ Found 3 topics\nProcessing article 1765... ✓ Found 0 topics\nProcessing article 1766... ✓ Found 2 topics\nProcessing article 1767... ✓ Found 1 topics\nProcessing article 1768... ✓ Found 2 topics\nProcessing article 1769... ✓ Found 2 topics\nProcessing article 1770... ✓ Found 3 topics\nProcessing article 1771... ✓ Found 3 topics\nProcessing article 1772... ✓ Found 5 topics\nProcessing article 1773... ✓ Found 1 topics\nProcessing article 1774... ✓ Found 2 topics\nProcessing article 1775... ✓ Found 5 topics\nProcessing article 1776... ✓ Found 0 topics\nProcessing article 1777... ✓ Found 5 topics\nProcessing article 1778... ✓ Found 5 topics\nProcessing article 1779... ✓ Found 2 topics\nProcessing article 1780... ✓ Found 1 topics\nProcessing article 1781... ✓ Found 3 topics\nProcessing article 1782... ✓ Found 2 topics\nProcessing article 1783... ✓ Found 0 topics\nProcessing article 1784... ✓ Found 1 topics\nProcessing article 1785... ✓ Found 1 topics\nProcessing article 1786... ✓ Found 2 topics\nProcessing article 1787... ✓ Found 2 topics\nProcessing article 1788... ✓ Found 3 topics\nProcessing article 1789... ✓ Found 2 topics\nProcessing article 1790... ✓ Found 2 topics\nProcessing article 1791... ✓ Found 2 topics\nProcessing article 1792... ✓ Found 1 topics\nProcessing article 1793... ✓ Found 4 topics\nProcessing article 1794... ✓ Found 1 topics\nProcessing article 1795... ✓ Found 3 topics\nProcessing article 1796... ✓ Found 1 topics\nProcessing article 1797... ✓ Found 2 topics\nProcessing article 1798... ✓ Found 2 topics\nProcessing article 1799... ✓ Found 2 topics\nProcessing article 1800... ✓ Found 3 topics\nProcessing article 1801... ✓ Found 2 topics\nProcessing article 1802... ✓ Found 5 topics\nProcessing article 1803... ✓ Found 0 topics\nProcessing article 1804... ✓ Found 5 topics\nProcessing article 1805... ✓ Found 0 topics\nProcessing article 1806... ✓ Found 8 topics\nProcessing article 1807... ✓ Found 1 topics\nProcessing article 1808... ✓ Found 5 topics\nProcessing article 1809... ✓ Found 0 topics\nProcessing article 1810... ✓ Found 2 topics\nProcessing article 1811... ✓ Found 5 topics\nProcessing article 1812... ✓ Found 1 topics\nProcessing article 1813... ✓ Found 1 topics\nProcessing article 1814... ✓ Found 7 topics\nProcessing article 1815... ✓ Found 4 topics\nProcessing article 1816... ✓ Found 3 topics\nProcessing article 1817... ✓ Found 2 topics\nProcessing article 1818... ✓ Found 1 topics\nProcessing article 1819... ✓ Found 5 topics\nProcessing article 1820... ✓ Found 6 topics\nProcessing article 1821... ✓ Found 1 topics\nProcessing article 1822... ✓ Found 3 topics\nProcessing article 1823... ✓ Found 2 topics\nProcessing article 1824... ✓ Found 4 topics\nProcessing article 1825... ✓ Found 1 topics\nProcessing article 1826... ✓ Found 0 topics\nProcessing article 1827... ✓ Found 3 topics\nProcessing article 1828... ✓ Found 4 topics\nProcessing article 1829... ✓ Found 3 topics\nProcessing article 1830... ✓ Found 1 topics\nProcessing article 1831... ✓ Found 2 topics\nProcessing article 1832... ✓ Found 2 topics\nProcessing article 1833... ✓ Found 0 topics\nProcessing article 1834... ✓ Found 3 topics\nProcessing article 1835... ✓ Found 3 topics\nProcessing article 1836... ✓ Found 0 topics\nProcessing article 1837... ✓ Found 2 topics\nProcessing article 1838... ✓ Found 3 topics\nProcessing article 1839... ✓ Found 2 topics\nProcessing article 1840... ✓ Found 3 topics\nProcessing article 1841... ✓ Found 3 topics\nProcessing article 1842... ✓ Found 6 topics\nProcessing article 1843... ✓ Found 2 topics\nProcessing article 1844... ✓ Found 1 topics\nProcessing article 1845... ✓ Found 4 topics\nProcessing article 1846... ✓ Found 3 topics\nProcessing article 1847... ✓ Found 0 topics\nProcessing article 1848... ✓ Found 2 topics\nProcessing article 1849... ✓ Found 3 topics\nProcessing article 1850... ✓ Found 2 topics\nProcessing article 1851... ✓ Found 4 topics\nProcessing article 1852... ✓ Found 3 topics\nProcessing article 1853... ✓ Found 3 topics\nProcessing article 1854... ✓ Found 2 topics\nProcessing article 1855... ✓ Found 1 topics\nProcessing article 1856... ✓ Found 5 topics\nProcessing article 1857... ✓ Found 0 topics\nProcessing article 1858... ✓ Found 5 topics\nProcessing article 1859... ✓ Found 0 topics\nProcessing article 1860... ✓ Found 3 topics\nProcessing article 1861... ✓ Found 2 topics\nProcessing article 1862... ✓ Found 3 topics\nProcessing article 1863... ✓ Found 2 topics\nProcessing article 1864... ✓ Found 5 topics\nProcessing article 1865... ✓ Found 2 topics\nProcessing article 1866... ✓ Found 4 topics\nProcessing article 1867... ✓ Found 3 topics\nProcessing article 1868... ✓ Found 3 topics\nProcessing article 1869... ✓ Found 5 topics\nProcessing article 1870... ✓ Found 5 topics\nProcessing article 1871... ✓ Found 2 topics\nProcessing article 1872... ✓ Found 5 topics\nProcessing article 1873... ✓ Found 5 topics\nProcessing article 1874... ✓ Found 5 topics\nProcessing article 1875... ✓ Found 3 topics\nProcessing article 1876... ✓ Found 2 topics\nProcessing article 1877... ✓ Found 4 topics\nProcessing article 1878... ✓ Found 5 topics\nProcessing article 1879... ✓ Found 0 topics\nProcessing article 1880... ✓ Found 5 topics\nProcessing article 1881... ✓ Found 2 topics\nProcessing article 1882... ✓ Found 2 topics\nProcessing article 1883... ✓ Found 4 topics\nProcessing article 1884... ✓ Found 2 topics\nProcessing article 1885... ✓ Found 3 topics\nProcessing article 1886... ✓ Found 0 topics\nProcessing article 1887... ✓ Found 0 topics\nProcessing article 1888... ✓ Found 0 topics\nProcessing article 1889... ✓ Found 4 topics\nProcessing article 1890... ✓ Found 5 topics\nProcessing article 1891... ✓ Found 2 topics\nProcessing article 1892... ✓ Found 3 topics\nProcessing article 1893... ✓ Found 5 topics\nProcessing article 1894... ✓ Found 5 topics\nProcessing article 1895... ✓ Found 1 topics\nProcessing article 1896... ✓ Found 5 topics\nProcessing article 1897... ✓ Found 2 topics\nProcessing article 1898... ✓ Found 5 topics\nProcessing article 1899... ✓ Found 5 topics\nProcessing article 1900... ✓ Found 3 topics\nProcessing article 1901... ✓ Found 2 topics\nProcessing article 1902... ✓ Found 1 topics\nProcessing article 1903... ✓ Found 3 topics\nProcessing article 1904... ✓ Found 3 topics\nProcessing article 1905... ✓ Found 2 topics\nProcessing article 1906... ✓ Found 1 topics\nProcessing article 1907... ✓ Found 5 topics\nProcessing article 1908... ✓ Found 2 topics\nProcessing article 1909... ✓ Found 1 topics\nProcessing article 1910... ✓ Found 1 topics\nProcessing article 1911... ✓ Found 4 topics\nProcessing article 1912... ✓ Found 1 topics\nProcessing article 1913... ✓ Found 4 topics\nProcessing article 1914... ✓ Found 3 topics\nProcessing article 1915... ✓ Found 4 topics\nProcessing article 1916... ✓ Found 3 topics\nProcessing article 1917... ✓ Found 1 topics\nProcessing article 1918... ✓ Found 1 topics\nProcessing article 1919... ✓ Found 3 topics\nProcessing article 1920... ✓ Found 2 topics\nProcessing article 1921... ✓ Found 0 topics\nProcessing article 1922... ✓ Found 1 topics\nProcessing article 1923... ✓ Found 1 topics\nProcessing article 1924... ✓ Found 5 topics\nProcessing article 1925... ✓ Found 0 topics\nProcessing article 1926... ✓ Found 4 topics\nProcessing article 1927... ✓ Found 1 topics\nProcessing article 1928... ✓ Found 2 topics\nProcessing article 1929... ✓ Found 5 topics\nProcessing article 1930... ✓ Found 5 topics\nProcessing article 1931... ✓ Found 0 topics\nProcessing article 1932... ✓ Found 2 topics\nProcessing article 1933... ✓ Found 5 topics\nProcessing article 1934... ✓ Found 5 topics\nProcessing article 1935... ✓ Found 2 topics\nProcessing article 1936... ✓ Found 5 topics\nProcessing article 1937... ✓ Found 5 topics\nProcessing article 1938... ✓ Found 5 topics\nProcessing article 1939... ✓ Found 4 topics\nProcessing article 1940... ✓ Found 3 topics\nProcessing article 1941... ✓ Found 5 topics\nProcessing article 1942... ✓ Found 0 topics\nProcessing article 1943... ✓ Found 3 topics\nProcessing article 1944... ✓ Found 2 topics\nProcessing article 1945... ✓ Found 5 topics\nProcessing article 1946... ✓ Found 3 topics\nProcessing article 1947... ✓ Found 2 topics\nProcessing article 1948... ✓ Found 3 topics\nProcessing article 1949... ✓ Found 2 topics\nProcessing article 1950... ✓ Found 2 topics\nProcessing article 1951... ✓ Found 0 topics\nProcessing article 1952... ✓ Found 5 topics\nProcessing article 1953... ✓ Found 2 topics\nProcessing article 1954... ✓ Found 5 topics\nProcessing article 1955... ✓ Found 5 topics\nProcessing article 1956... ✓ Found 4 topics\nProcessing article 1957... ✓ Found 1 topics\nProcessing article 1958... ✓ Found 1 topics\nProcessing article 1959... ✓ Found 2 topics\nProcessing article 1960... ✓ Found 2 topics\nProcessing article 1961... ✓ Found 3 topics\nProcessing article 1962... ✓ Found 0 topics\nProcessing article 1963... ✓ Found 3 topics\nProcessing article 1964... ✓ Found 3 topics\nProcessing article 1965... ✓ Found 3 topics\nProcessing article 1966... ✓ Found 2 topics\nProcessing article 1967... ✓ Found 3 topics\nProcessing article 1968... ✓ Found 5 topics\nProcessing article 1969... ✓ Found 1 topics\nProcessing article 1970... ✓ Found 5 topics\nProcessing article 1971... ✓ Found 2 topics\nProcessing article 1972... ✓ Found 5 topics\nProcessing article 1973... ✓ Found 3 topics\nProcessing article 1974... ✓ Found 3 topics\nProcessing article 1975... ✓ Found 2 topics\nProcessing article 1976... ✓ Found 5 topics\nProcessing article 1977... ✓ Found 1 topics\nProcessing article 1978... ✓ Found 1 topics\nProcessing article 1979... ✓ Found 2 topics\nProcessing article 1980... ✓ Found 2 topics\nProcessing article 1981... ✓ Found 0 topics\nProcessing article 1982... ✓ Found 2 topics\nProcessing article 1983... ✓ Found 2 topics\nProcessing article 1984... ✓ Found 2 topics\nProcessing article 1985... ✓ Found 2 topics\nProcessing article 1986... ✓ Found 3 topics\nProcessing article 1987... ✓ Found 3 topics\nProcessing article 1988... ✓ Found 5 topics\nProcessing article 1989... ✓ Found 3 topics\nProcessing article 1990... ✓ Found 2 topics\nProcessing article 1991... ✓ Found 0 topics\nProcessing article 1992... ✓ Found 4 topics\nProcessing article 1993... ✓ Found 3 topics\nProcessing article 1994... ✓ Found 2 topics\nProcessing article 1995... ✓ Found 2 topics\nProcessing article 1996... ✓ Found 2 topics\nProcessing article 1997... ✓ Found 2 topics\nProcessing article 1998... ✓ Found 5 topics\nProcessing article 1999... ✓ Found 0 topics\nProcessing article 2000... ✓ Found 3 topics\nProcessing article 2001... ✓ Found 1 topics\nProcessing article 2002... ✓ Found 2 topics\nProcessing article 2003... ✓ Found 5 topics\nProcessing article 2004... ✓ Found 2 topics\nProcessing article 2005... ✓ Found 2 topics\nProcessing article 2006... ✓ Found 2 topics\nProcessing article 2007... ✓ Found 2 topics\nProcessing article 2008... ✓ Found 5 topics\nProcessing article 2009... ✓ Found 0 topics\nProcessing article 2010... ✓ Found 3 topics\nProcessing article 2011... ✓ Found 1 topics\nProcessing article 2012... ✓ Found 2 topics\nProcessing article 2013... ✓ Found 5 topics\nProcessing article 2014... ✓ Found 2 topics\nProcessing article 2015... ✓ Found 2 topics\nProcessing article 2016... ✓ Found 4 topics\nProcessing article 2017... ✓ Found 1 topics\nProcessing article 2018... ✓ Found 4 topics\nProcessing article 2019... ✓ Found 4 topics\nProcessing article 2020... ✓ Found 2 topics\nProcessing article 2021... ✓ Found 3 topics\nProcessing article 2022...   Error: litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"Requested token count exceeds the model's maximum context length of 16384 tokens. You requested a total of 21553 tokens: 9553 tokens from the input messages and 12000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\",\"type\":\"BadRequestError\"}\n\n✓ Found 0 topics\nProcessing article 2023...   Error: litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"Requested token count exceeds the model's maximum context length of 16384 tokens. You requested a total of 21553 tokens: 9553 tokens from the input messages and 12000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\",\"type\":\"BadRequestError\"}\n\n✓ Found 0 topics\nProcessing article 2024... ✓ Found 2 topics\nProcessing article 2025... ✓ Found 2 topics\nProcessing article 2026... ✓ Found 0 topics\nProcessing article 2027... ✓ Found 4 topics\nProcessing article 2028... ✓ Found 0 topics\nProcessing article 2029... ✓ Found 5 topics\nProcessing article 2030... ✓ Found 5 topics\nProcessing article 2031... ✓ Found 3 topics\nProcessing article 2032... ✓ Found 4 topics\nProcessing article 2033... ✓ Found 2 topics\nProcessing article 2034... ✓ Found 5 topics\nProcessing article 2035... ✓ Found 1 topics\nProcessing article 2036... ✓ Found 1 topics\nProcessing article 2037... ✓ Found 2 topics\nProcessing article 2038... ✓ Found 0 topics\nProcessing article 2039... ✓ Found 3 topics\nProcessing article 2040... ✓ Found 11 topics\nProcessing article 2041... ✓ Found 4 topics\nProcessing article 2042... ✓ Found 0 topics\nProcessing article 2043... ✓ Found 1 topics\nProcessing article 2044... ✓ Found 2 topics\nProcessing article 2045... ✓ Found 0 topics\nProcessing article 2046... ✓ Found 1 topics\nProcessing article 2047... ✓ Found 2 topics\nProcessing article 2048... ✓ Found 4 topics\nProcessing article 2049... ✓ Found 2 topics\nProcessing article 2050...   Error: litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"Requested token count exceeds the model's maximum context length of 16384 tokens. You requested a total of 21553 tokens: 9553 tokens from the input messages and 12000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\",\"type\":\"BadRequestError\"}\n\n✓ Found 0 topics\nProcessing article 2051...   Error: litellm.BadRequestError: HuggingfaceException - {\"code\":400,\"message\":\"Requested token count exceeds the model's maximum context length of 16384 tokens. You requested a total of 21553 tokens: 9553 tokens from the input messages and 12000 tokens for the completion. Please reduce the number of tokens in the input messages or the completion to fit within the limit.\",\"type\":\"BadRequestError\"}\n\n✓ Found 0 topics\nProcessing article 2052... ✓ Found 2 topics\nProcessing article 2053... ✓ Found 5 topics\nProcessing article 2054... ✓ Found 3 topics\nProcessing article 2055... ✓ Found 5 topics\nProcessing article 2056... ✓ Found 1 topics\nProcessing article 2057... ✓ Found 2 topics\nProcessing article 2058... ✓ Found 3 topics\nProcessing article 2059... ✓ Found 2 topics\nProcessing article 2060... ✓ Found 2 topics\nProcessing article 2061... ✓ Found 1 topics\nProcessing article 2062... ✓ Found 1 topics\nProcessing article 2063... ✓ Found 2 topics\nProcessing article 2064... ✓ Found 0 topics\nProcessing article 2065... ✓ Found 5 topics\n\n================================================================================\nCLASSIFICATION RESULTS SUMMARY\n================================================================================\n\nTotal articles: 2066\nArticles with topics: 1870 (90.5%)\nMulti-label articles: 1557 (75.4%)\n\n================================================================================\nTOP 10 MOST FREQUENT TOPICS\n================================================================================\n\n✓ Results saved to /kaggle/working/topic_classification_results.json\n✓ CSV exported to /kaggle/working/topic_classification_results.csv\n\n================================================================================\nSAMPLE RESULTS (First 3)\n================================================================================\n\nArticle 0:\n  Preview: 1. Health care reform\n\nHow could the health care reform legislation that President Barack Obama signed into law on March 23, 2010, not be the #1 story of the year? Whether you are for or against it, the Patient Protection and Affordable Care Act is nothing if not ambitious, and if implemented, it will fundamentally alter how American health care is financed and perhaps delivered. The law is designed to patch holes in the health insurance system and extend coverage to 32 million Americans by 2019 while also reining in health care spending, which now accounts for more than 17% of the country’s gross domestic product. The biggest changes aren’t scheduled to occur until 2014, when most people will be required to have health insurance or pay a penalty (the so-called individual mandate) and when state-level health insurance exchanges should be in place. The Medicaid program is also scheduled to be expanded that year so that it covers more people, and subsidized insurance will be available through the exchanges for people in lower- and middle-income brackets. But plenty is happening before 2014. The 1,000-page law contains hundreds of provisions, and they’re being rolled out in phases. This year, for example, the law created high-risk pools for people with pre-existing conditions, required health plans to extend coverage to adult children up to age 26, and imposed a 10% tax on indoor tanning salons. Next year, about 20 different provisions are scheduled to take effect, including the elimination of copayments for many preventive services for Medicare enrollees, the imposition of limits on non-medical spending by health plans, and the creation of a voluntary insurance that will help pay for home health care and other long-term care services received outside a nursing home. Getting a handle on the complicated law is difficult. If you’re looking for a short course, the Kaiser Family Foundation has created an excellent timeline of the law’s implementation (we depended on it for this post) and a short (nine minutes) animated video that’s one of the best (and most amusing) overviews available. The big question now is whether the sweeping health care law can survive various legal and political challenges. In December, a federal judge in Virginia ruled that the individual mandate was unconstitutional. Meanwhile, congressional Republicans have vowed to thwart the legislation, and if the party were to win the White House and control of the Senate in the 2012 election, Republicans would be in a position to follow through on their threats to repeal it.\n\n2. Smartphones, medical apps, and remote monitoring\n\nSmartphones and tablet computers are making it easier to get health care information, advice, and reminders on an anywhere-and-anytime basis. Hundreds of health and medical apps for smartphones like the iPhone became available this year. Some are just for fun. Others provide useful information (calorie counters, first aid and CPR instructions) or perform calculations. Even the federal government is getting into the act: the app store it opened this summer has several free health-related apps, including one called My Dietary Supplements for keeping track of vitamins and supplements and another one from the Environmental Protection Agency that allows you to check the UV index and air quality wherever you are. Smartphones are also being used with at-home monitoring devices; for example, glucose meters have been developed that send blood sugar readings wirelessly to an app on a smartphone. The number of doctors using apps and mobile devices is increasing, a trend that is likely to accelerate as electronic health records become more common. Check out iMedicalapps if you want to see the apps your doctor might be using or talking about. It has become a popular Web site for commentary and critiques of medical apps for doctors and medical students. Meanwhile, the FDA is wrestling with the issue of how tightly it should regulate medical apps. Some adverse events resulting from programming errors have been reported to the agency. Medical apps are part of a larger “e-health” trend toward delivering health care reminders and advice remotely with the help of computers and phones of all types. These phone services are being used in combination with increasingly sophisticated at-home monitoring devices. Research results have been mixed. Simple, low-cost text messages have been shown to be effective in getting people wear sunscreen. But one study published this year found that regular telephone contact and at-home monitoring of heart failure patients had no effect on hospitalizations of death from any cause over a six-month period. Another study found that remote monitoring did lower hospital readmission rates among heart failure patients, although the difference between remote monitoring and regular care didn’t reach statistical significance.\n\n3. New CPR guidelines\n\nThe American Heart Association issued new guidelines for cardiopulmonary resuscitation (CPR) this year that continue the trend toward simplifying CPR and emphasizing chest compressions. For trained rescuers, the guidelines change the CPR sequence from airway, breathing, and chest compressions and the A-B-C mnemonic to putting chest compressions first, followed by checks of the airway and breathing, or C-A-B. People who haven’t had CPR training are encouraged to do only chest compressions because they are easier and “more readily guided by dispatchers over the telephone.” The compressions should be fast (about 100 times a minute) and hard (so that the chest goes down by two inches or more). The American Heart Association produced a very good video about the guidelines that’s well worth watching. Fewer than half of those who suffer cardiac arrest receive CPR, so the hope is that more people will give CPR—and do so quickly— if it’s simpler and doesn’t involve mouth-to-mouth breathing. The guidelines note that the results for chest compression–only CPR are similar to those for traditional CPR for adults in cardiac arrest outside of the hospital. But conventional CPR is still better for children because cardiac arrest in children is usually preceded by a lack of breathing, so the mouth-to-mouth breaths are needed to restore oxygen levels in the blood. Research results reported this year in The Lancet, Journal of the American Medical Association, and The New England Journal of Medicine all suggested that in most cardiac arrest cases, chest compression–only CPR is as effective, if not more so, than conventional CPR.\n\n4. Making stem cells\n\nHeart attacks, strokes, and many other conditions destroy cells, and for years, scientists believed that it was impossible to make replacements. Then, four years ago, a Japanese researcher, Shinya Yamanaka, discovered a technique for reprogramming cells back into stem cells, so that they would function like a biological blank slate and be capable of turning into any other type of cell. Dr. Yamanka called his creations induced pluripotent stem cells, and a whole new frontier of stem cell research opened up. Scientists have since figured out ways to turn one cell type directly into another type: blood vessel cells have been turned into bone and fat cells, and skin cells have been turned into blood cells. And this year, stem cell research took another leap forward when a Harvard researcher, Derrick Rossi, reported results demonstrating a technique that may make the creation of induced pluripotent stem cells a lot easier and safer. Rossi and his colleagues at the Harvard Stem Cell Institute reprogrammed adult skin cells with synthetic messenger RNA that leaves DNA intact, instead of inserting genes into DNA. Research with embryonic stem cells remains important, and in October 2010, Geron, a California biotech company, began enrolling people in a trial to test the safety of using cells derived from embryonic stem cells to treat spinal cord injuries. But researchers are also making some remarkable progress toward turning readily available cells, such as skin or blood cells, into other types of cells. These new cells would be genetically identical to other cells in the body and therefore shouldn’t be rejected by the immune system when they’re transplanted to replace cells lost to disease.\n\n5. Heightened awareness of concussions\n\nConcern about sports-related concussions, especially in football, has been growing as evidence has increased that repeated concussions can cause permanent brain damage over the long term, even if the short-term effects are fairly mild (most concussions do not result in a loss of consciousness, for example) and CT and MRI scans are normal. Some researchers are calling concussion-related brain damage chronic traumatic encephalopathy (encephalopathy is a catchall term for any degenerative disease of the brain). There were several efforts in 2010 to reduce the number and severity of concussions. The National Football League started to fine players for illegal hits this season. The American Academy of Neurology came out with a position paper that says any athlete who might have suffered a concussion shouldn’t be allowed to partcipate again until he or she has been evaluated by a doctor with training in the evalulation and management of sports concussions. The American Academy of Pediatrics released a report about sports-related concussions in children and adolescents that says younger people often need more time (7 to 10 days or even longer) to recover from a concussion than college or professional athletes. Several states have passed laws requiring high schools to have concussion management programs. The concussion risk is greatest for football players, but girl basketball and soccer players also have relatively high rates. Meanwhile, research into concussions continues. Boston University researchers who have been prominent in the field caused a stir with a finding that linked concussions and chronic traumatic encephalopathy to amyotrophic lateral sclerosis (Lou Gherig’s disease).\n\n6. An anti-aging possibility\n\nResearchers at the Harvard-affliliated Dana-Farber Cancer Institute reported results this year that kindled hopes for altering the fundamental biology of aging. Their experiment involved mice that had been genetically engineered so that an enzyme called telomerase that is known to be important in the aging of cells could be turned on and off. When the enzyme was turned off, the mice aged prematurely. When they reached the chronological equivalent of adolescence, they appeared to be biologically very old: their brains and other organs had shrunk and were starting to fail. Then the scientists turned on the enzyme. Promptly, the brain and other shrunken organs started to grow with new cells, and organ failure ceased. The animals recovered their sense of smell. You might say the mice became adolescents again. Of course, what works in mice doesn’t always work in humans. There are concerns that the activation of telomerase could cause cancer, although that didn’t occur in this particular experiment. And this is very much an experimental finding; at this point, all those products making anti-aging claims are way ahead of the game and not to be trusted. Still, along with other research, this study hints at a future when it might be possible to slow down biological aging and possibly prevent some of the diseases associated with it.\n\n7. CT scans for lung cancer screening\n\nMore Americans die from lung cancer than from any other type of cancer, yet there’s no accepted screening test for the disease. Study results reported this year may change that situation. The National Cancer Institute (NCI) stopped the National Lung Screening Trial comparing CT scans to chest x-rays earlier than expected because the CT scans appeared to be so effective at reducing lung cancer deaths. The trial included 53,000 current and former heavy (30 pack years or more) smokers. Results released in October showed that over a five-year period, 354 of those screened with CT scans died from lung cancer (or about 1.4%) compared with 442 of those screened with chest x-rays (about 1.7%). Catching any cancer at an earlier, more treatable stage is an appealing idea, and especially lung cancer, because of its high mortality rate. But screening tests have become more controversial lately because of concerns that they lead to overdiagnosis and overtreatment. Almost one in every four people who were screened with CT scans in the National Lung Cancer Screening Trial had a false positive, the finding of an abnormality that turns out not to be cancer. There are also concerns about radiation exposure from CT scans and whether scans for lung cancer will add to that problem.\n\n8. New vitamin D guidelines\n\nAfter years of debate, discussion, and research, the Institute of Medicine (IOM) issued new vitamin D guidelines this year. The Recommended Dietary Allowance (RDA) is now 600 international units (IU) a day for people ages 1 to 70 and 800 IU a day for those 71 and older. The previous guidelines, set in 1997, recommended a daily intake of 200 IU through age 50, 400 IU between the ages of 51 and 70, and 600 IU for those 71 and older. The IOM panel also established a new safe upper limit of 4,000 IU a day, double the old limit of 2,000 IU. The new guidelines were criticized as being too conservative by many experts, who would have preferred an RDA closer to 1,000 IU a day and a blood level target of 30 ng/ml (75 nmol/l) for the vitamin, not the 20 ng/ml (50 nmol/l) set by the IOM panel. The difference of opinion stems, in part, from the fact that the IOM panel gave results from randomized clincial trials (RCTs) far more weight than results from other types of studies. As a result, the panel found evidence that vitamin D benefits bone and little else. If other kinds of studies are taken into account, a case can be made that blood levels of 30 ng/ml or even higher would result in optimal bone health and that the vitamin has a wide range of health benefits beyond bone, including protective effects against some cancers (especially colon cancer) and some autoimmune disorders. The debate about vitamin D is bound to continue. Soon after the IOM panel released its report, a different set of experts, the U.S. Preventive Services Task Force, came out with fall-prevention recommendations that include an endorsement of vitamin D.\n\n9. Alternatives to warfarin\n\nThe FDA approved one alternative to warfarin this year, a drug called dabigatran (sold as Pradaxa). Another alternative, rivaroxaban, seems to be waiting in the wings after largely favorable results were reported this year (here and here) from trials testing the drug in patients with deep-vein thrombosis and atrial fibrillation. A third drug, apixaban, which is related to rivaroxaban, is also looking promising. Warfarin (the brand-name version is called Coumadin) has been the mainstay for preventing blood clots for decades, but it’s a tricky, high-maintenance drug that requires frequent blood tests to make sure the dose is producing the desired results: enough anti-coagulation to prevent blood clots but not so much as to cause bleeding . Warfarin also interacts with many foods and drugs. In contrast, these warfarin alternatives seem simple as pie: they can be given in fixed doses, don’t require blood monitoring, and don’t seem to pose interteaction problems. Cost, however, will be a barrier. Drug companies set high prices for new brand-name drugs. Warfarin, widely available as a generic, is relatively cheap. And there’s always the possibility of unforeseen side effects once the new drugs are more widely used. Still, millions of people stand to benefit if good alternatives to difficult-to-use warfarin pan out.\n\n10. Concerns about bisphosphonates\n\nBisphosphonates are prescribed to prevent and treat osteoporosis, a decrease in bone density that makes fractures more likely. Well-known brands include Fosamax (alendronate) and Actonel (risedronate). Millions of people, most of them postmenopausal women, take bisphosphonates; for the most part, they are safe and effective medications that have been shown to cut the risk of fractures by 50%. But concerns about ill effects from long-term use have been growing. In October, the FDA issued a new warning about bisphosphonates increasing the risk of a rare kind of thighbone (femur) fracture. Two years ago, the agency issued a different warning about the bone drugs causing bone, joint, and muscle pain. There have also been reports about a small percentage of bisphosphonate users developing osteonecrosis in their jawbones, although most of those cases have occurred in cancer patients who have received high intravenous doses (bisphosphonates can relieve pain and strengthen bone if cancer has spread to the bone). Some doctors are now recommending “drug holidays” for people who take bisphosphontes for osteoporosis for extended periods. Other bone-building drugs, such as denosumab (sold as Prolia), which was approved by the FDA this year, may get a closer look because of concerns about the side effects of bisphosphonates. And perhaps the nonpharmacological ways to strengthen bones will gain some adherents. Pill-free bone builders include a regimen of regular weight-bearing exercise and adequte intake of calicum and vitamin D (see item #8)....\n  True: []\n  Predicted: []\n\nArticle 1:\n  Preview: My colleagues at Harvard Health Publishing and I have a mission: to provide accurate, reliable information that will help readers live healthier lives. We work hard to fulfill that mission, and the feedback we get from folks who read our newsletters, Special Health Reports, books, and online health information indicates we are on the right track. Every so often we hear something from a reader that makes me especially proud of the work we do.\n\nThis letter was recently sent to the editor of the Harvard Women’s Health Watch:\n\nOne of your mailings undoubtedly saved me a lot of grief. (My kids, anyway.) I was aware of a woman’s heart attack symptoms being different from a man’s, and your brochure contained a paragraph confirming that. Early in June I was packing for a trip to celebrate my brother’s 90th birthday, at the same time a ditching project was being done in my back lot. Trying to deal with several matters at the same time is a talent I’ve outgrown, at 88, so didn’t think too much of the sudden fatigue and vague aches I felt in jaw & arms. I crashed for a nap in my recliner, felt OK afterwards, and figured it was just stress. The next day I was ready to leave, but got to thinking of those symptoms, and the fact the brochure had arrived at just that time, and wondered if it was more than coincidence and maybe I should pay attention? Didn’t much like the idea of something happening out in the middle of nowhere, so took myself to the fire hall where an EMT was on duty. He ushered me into the ambulance, did an EKG, and soon I was being helicoptered on doctor’s orders to St. Joseph’s Hospital. There I had 3 stents installed, and they apparently are doing their job. Thank you!\n\nThis echoes what a reader of the Harvard Heart Letter told us a couple years ago:\n\nThis past Friday the 13th, in the a.m., I attended mass at a nearby church and when I returned home I felt aches across my chest, on my back, and down both arms. I also felt a bit nauseous. I tried to shrug it off thinking that a visit to the gym a few days before was my problem. I sat down in a chair and on a nearby table I spied a copy of the Harvard Heart Letter. We subscribe to this for my wife who has a heart condition. The cover story cited 10 symptoms of a heart attack. I had 4. We called 911. A few hours later a stent was implanted. I was released on Sunday—Father’s Day—and the surgeon’s parting words were, “Tim Russert had a 100% blockage—yours was 98%”. I believe very strongly in the Hand of God. Had I not sat down on that particular chair—and had that particular issue not been there—the outcome might have been decidedly different. My time had not yet come. Thank you.\n\nIn the New Year and beyond, the Harvard Health Publishing team will continue to bring you honest, practical, on-target information that we hope will make a difference in your life.\n\nFrom Harvard Medical School and Harvard Health Publishing, I wish you good health in 2011....\n  True: ['Heart Health', \"Women's Health\"]\n  Predicted: ['heart health']\n\nArticle 2:\n  Preview: Does echinacea, the popular natural cold remedy, really work?\n\nIt depends on what you mean by “work.” Results reported in today’s Annals of Internal Medicine found that echinacea may reduce the length of a weeklong cold by 7 to 10 hours and make symptoms a little less onerous. That can’t be characterized as a major effect, so many people may figure that the trouble and expense of echinacea just isn’t worth it (fortunately, side effects from echinacea don’t seem to be much of an issue).\n\nBut others may decide that some benefit is better than none, and these results do fit with others that have left the door slightly ajar for echinacea having some effect as a cold remedy—a modest effect, but an effect, nonetheless.\n\nA summary for patients published by the Annals summed up the situation nicely:\n\nPeople who take echinacea to treat colds may experience a decrease in the length and severity of their cold symptoms but to such a small degree that they may not care about the difference. Although many studies of echinacea have been performed, researchers still disagree about its benefits in treating the common cold. This study is unlikely to change minds about whether to take this remedy.\n\nHave you tried echinacea as a cold remedy? Has it worked? How do research findings, pro and con, affect your opinion of so-called alternative medicines?\n\nMany of the echinacea studies, especially early on, were sponsored by companies making or selling the product. This study was supported by a grant from the National Center for Complementary and Alternative Medicine, which is part of the National Institutes of Health....\n  True: []\n  Predicted: ['healthy eating']\n\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION: Compare Predicted vs True Concepts\n# ============================================================================\n\ndef evaluate_classification(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate accuracy metrics by comparing predicted topics with true concepts.\n    \"\"\"\n    total = len(results)\n    exact_match = 0  # All predicted topics match exactly with true concepts\n    partial_match = 0  # At least one topic matches\n    no_match = 0  # No topics match\n    no_true_labels = 0  # Articles with no true concepts\n    \n    all_precisions = []\n    all_recalls = []\n    all_f1s = []\n    \n    correct_predictions = []\n    incorrect_predictions = []\n    \n    for result in results:\n        true_set = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_set = set(result['predicted_topics'])\n        \n        # Skip articles with no true labels\n        if len(true_set) == 0:\n            no_true_labels += 1\n            continue\n        \n        # Find intersection\n        intersection = true_set & pred_set\n        \n        # Exact match\n        if true_set == pred_set and len(true_set) > 0:\n            exact_match += 1\n            correct_predictions.append(result)\n        elif len(intersection) > 0:\n            partial_match += 1\n        else:\n            no_match += 1\n            incorrect_predictions.append(result)\n        \n        # Calculate precision, recall, F1\n        if len(pred_set) > 0:\n            precision = len(intersection) / len(pred_set)\n            all_precisions.append(precision)\n        else:\n            all_precisions.append(0)\n        \n        if len(true_set) > 0:\n            recall = len(intersection) / len(true_set)\n            all_recalls.append(recall)\n        else:\n            all_recalls.append(0)\n        \n        if all_precisions[-1] + all_recalls[-1] > 0:\n            f1 = 2 * (all_precisions[-1] * all_recalls[-1]) / (all_precisions[-1] + all_recalls[-1])\n            all_f1s.append(f1)\n        else:\n            all_f1s.append(0)\n    \n    articles_with_true_labels = total - no_true_labels\n    \n    metrics = {\n        'total_articles': total,\n        'articles_with_true_labels': articles_with_true_labels,\n        'articles_without_true_labels': no_true_labels,\n        'exact_matches': exact_match,\n        'partial_matches': partial_match,\n        'no_matches': no_match,\n        'exact_match_rate': exact_match / articles_with_true_labels if articles_with_true_labels > 0 else 0,\n        'partial_match_rate': partial_match / articles_with_true_labels if articles_with_true_labels > 0 else 0,\n        'avg_precision': np.mean(all_precisions) if all_precisions else 0,\n        'avg_recall': np.mean(all_recalls) if all_recalls else 0,\n        'avg_f1': np.mean(all_f1s) if all_f1s else 0,\n        'correct_examples': correct_predictions[:3],\n        'incorrect_examples': incorrect_predictions[:3]\n    }\n    \n    # Print results\n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS\")\n    print(f\"{'='*80}\")\n    print(f\"\\nTotal articles evaluated: {articles_with_true_labels}\")\n    print(f\"Articles without labels (skipped): {no_true_labels}\")\n    print(f\"\\n📊 ACCURACY:\")\n    print(f\"  ✓ Exact matches: {exact_match} ({metrics['exact_match_rate']*100:.1f}%)\")\n    print(f\"  ~ Partial matches: {partial_match} ({metrics['partial_match_rate']*100:.1f}%)\")\n    print(f\"  ✗ No matches: {no_match} ({no_match/articles_with_true_labels*100:.1f}%)\")\n    print(f\"\\n📈 PERFORMANCE METRICS:\")\n    print(f\"  Precision: {metrics['avg_precision']:.3f} (how many predicted topics were correct)\")\n    print(f\"  Recall: {metrics['avg_recall']:.3f} (how many true topics were found)\")\n    print(f\"  F1 Score: {metrics['avg_f1']:.3f} (overall accuracy)\")\n    \n    return metrics\n\n\n# Run evaluation\nprint(\"\\nEvaluating classification accuracy...\")\neval_metrics = evaluate_classification(results)\n\n# Show correct examples\nprint(f\"\\n{'='*80}\")\nprint(\"✓ CORRECTLY CLASSIFIED EXAMPLES\")\nprint(f\"{'='*80}\")\nfor ex in eval_metrics['correct_examples']:\n    print(f\"\\nArticle {ex['article_id']}:\")\n    print(f\"  True: {ex['true_concepts']}\")\n    print(f\"  Predicted: {ex['predicted_topics']}\")\n    print(f\"  ✓ EXACT MATCH!\")\n\n# Show incorrect examples\nprint(f\"\\n{'='*80}\")\nprint(\"✗ INCORRECTLY CLASSIFIED EXAMPLES\")\nprint(f\"{'='*80}\")\nfor ex in eval_metrics['incorrect_examples']:\n    print(f\"\\nArticle {ex['article_id']}:\")\n    print(f\"  True: {ex['true_concepts']}\")\n    print(f\"  Predicted: {ex['predicted_topics']}\")\n    #print(f\"  Reasoning: {ex['reasoning'][:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:40:59.850461Z","iopub.execute_input":"2025-11-11T17:40:59.850735Z","iopub.status.idle":"2025-11-11T17:40:59.869855Z","shell.execute_reply.started":"2025-11-11T17:40:59.850714Z","shell.execute_reply":"2025-11-11T17:40:59.869012Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating classification accuracy...\n\n================================================================================\nEVALUATION METRICS\n================================================================================\n\nTotal articles evaluated: 1584\nArticles without labels (skipped): 482\n\n📊 ACCURACY:\n  ✓ Exact matches: 347 (21.9%)\n  ~ Partial matches: 1077 (68.0%)\n  ✗ No matches: 160 (10.1%)\n\n📈 PERFORMANCE METRICS:\n  Precision: 0.523 (how many predicted topics were correct)\n  Recall: 0.843 (how many true topics were found)\n  F1 Score: 0.598 (overall accuracy)\n\n================================================================================\n✓ CORRECTLY CLASSIFIED EXAMPLES\n================================================================================\n\nArticle 4:\n  True: ['Healthy Eating', 'Heart Health']\n  Predicted: ['healthy eating', 'heart health']\n  ✓ EXACT MATCH!\n\nArticle 6:\n  True: ['Mental Health']\n  Predicted: ['mental health']\n  ✓ EXACT MATCH!\n\nArticle 14:\n  True: ['Healthy Eating', 'Heart Health']\n  Predicted: ['healthy eating', 'heart health']\n  ✓ EXACT MATCH!\n\n================================================================================\n✗ INCORRECTLY CLASSIFIED EXAMPLES\n================================================================================\n\nArticle 44:\n  True: ['Prostate Knowledge']\n  Predicted: ['cancer', 'healthy eating', 'heart health']\n\nArticle 49:\n  True: ['Cancer', 'Prostate Knowledge']\n  Predicted: ['mental health', 'heart health', \"women's health\"]\n\nArticle 75:\n  True: ['Healthy Eating']\n  Predicted: []\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION METRICS (Following the Paper's Methodology)\n# ============================================================================\n\ndef calculate_metrics_paper_style(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate evaluation metrics following the paper's approach:\n    \"For each article, the model inferred topic(s) were compared against \n    the list of 'gold' topic(s) to compute the true positive, false positive, \n    and false negative statistics for that article. Then, all such statistics \n    for all the articles in a dataset were aggregated and used to compute \n    the final Precision, Recall, and micro-averaged F1 score.\"\n    \n    Reference: Section 5.4 of the paper\n    \"\"\"\n    # Aggregate statistics across all articles\n    total_tp = 0  # True Positives\n    total_fp = 0  # False Positives\n    total_fn = 0  # False Negatives\n    \n    articles_evaluated = 0\n    articles_skipped = 0\n    \n    for result in results:\n        # Get true and predicted topics (lowercased and stripped)\n        true_topics = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_topics = set([t.lower().strip() for t in result['predicted_topics']])\n        \n        # Skip articles with no ground truth labels\n        if len(true_topics) == 0:\n            articles_skipped += 1\n            continue\n        \n        articles_evaluated += 1\n        \n        # Calculate TP, FP, FN for this article\n        tp = len(true_topics & pred_topics)  # Intersection\n        fp = len(pred_topics - true_topics)  # Predicted but not true\n        fn = len(true_topics - pred_topics)  # True but not predicted\n        \n        # Aggregate\n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n    \n    # Calculate micro-averaged metrics\n    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    metrics = {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'total_tp': total_tp,\n        'total_fp': total_fp,\n        'total_fn': total_fn,\n        'articles_evaluated': articles_evaluated,\n        'articles_skipped': articles_skipped\n    }\n    \n    # Print results\n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS (Paper's Methodology)\")\n    print(f\"{'='*80}\")\n    print(f\"\\nArticles evaluated: {articles_evaluated}\")\n    print(f\"Articles skipped (no ground truth): {articles_skipped}\")\n    print(f\"\\nAggregated Statistics:\")\n    print(f\"  True Positives (TP): {total_tp}\")\n    print(f\"  False Positives (FP): {total_fp}\")\n    print(f\"  False Negatives (FN): {total_fn}\")\n    print(f\"\\n📊 MICRO-AVERAGED METRICS:\")\n    print(f\"  Precision: {precision:.3f}\")\n    print(f\"  Recall: {recall:.3f}\")\n    print(f\"  F1 Score: {f1_score:.3f}\")\n    \n    return metrics\n\n\n# Run evaluation using the paper's methodology\nprint(\"\\nEvaluating using paper's methodology...\")\npaper_metrics = calculate_metrics_paper_style(results)\n\n# Compare with their baselines (from Table 6 in the paper)\nprint(f\"\\n{'='*80}\")\nprint(\"COMPARISON WITH PAPER'S BASELINES\")\nprint(f\"{'='*80}\")\nprint(f\"This model's F1 Score: {paper_metrics['f1_score']:.3f}\")\nprint(f\"\\nPaper's Results:\")\nprint(f\"  GFLM-S baseline: 0.532\")\nprint(f\"  GFLM-W baseline: 0.530\")\nprint(f\"  SBERT (best mid encoder): 0.594\")\nprint(f\"  ChatGPT-3.5 (best overall): 0.606\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:41:07.012315Z","iopub.execute_input":"2025-11-11T17:41:07.012577Z","iopub.status.idle":"2025-11-11T17:41:07.028527Z","shell.execute_reply.started":"2025-11-11T17:41:07.012558Z","shell.execute_reply":"2025-11-11T17:41:07.027777Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating using paper's methodology...\n\n================================================================================\nEVALUATION METRICS (Paper's Methodology)\n================================================================================\n\nArticles evaluated: 1584\nArticles skipped (no ground truth): 482\n\nAggregated Statistics:\n  True Positives (TP): 1922\n  False Positives (FP): 2357\n  False Negatives (FN): 407\n\n📊 MICRO-AVERAGED METRICS:\n  Precision: 0.449\n  Recall: 0.825\n  F1 Score: 0.582\n\n================================================================================\nCOMPARISON WITH PAPER'S BASELINES\n================================================================================\nThis model's F1 Score: 0.582\n\nPaper's Results:\n  GFLM-S baseline: 0.532\n  GFLM-W baseline: 0.530\n  SBERT (best mid encoder): 0.594\n  ChatGPT-3.5 (best overall): 0.606\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: LOAD KEYWORDS\n# ============================================================================\n\n# Load the keyword file\nwith open('/kaggle/input/keywords/Keyword_Medical.json', 'r') as f:\n    keyword_data = json.load(f)\n\n# Parse into dictionary: {topic_name: [keywords]}\nconcept_keywords = {}\n\nfor item in keyword_data:\n    keywords_list = item['Keyword']\n    topic_name = keywords_list[0].lower()  # First item is topic name\n    related_keywords = [kw.lower() for kw in keywords_list[1:]]  # Rest are keywords\n    concept_keywords[topic_name] = related_keywords\n\nprint(f\"\\n{'='*80}\")\nprint(\"LOADED CONCEPT KEYWORDS\")\nprint(f\"{'='*80}\")\nprint(f\"Total concepts: {len(concept_keywords)}\\n\")\n\n# Show first 3 examples\nfor i, (topic, keywords) in enumerate(list(concept_keywords.items())[:3]):\n    print(f\"{i+1}. {topic}:\")\n    print(f\"   Keywords: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:41:13.398255Z","iopub.execute_input":"2025-11-11T17:41:13.398518Z","iopub.status.idle":"2025-11-11T17:41:13.426181Z","shell.execute_reply.started":"2025-11-11T17:41:13.398498Z","shell.execute_reply":"2025-11-11T17:41:13.425456Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nLOADED CONCEPT KEYWORDS\n================================================================================\nTotal concepts: 18\n\n1. addiction:\n   Keywords: opioids, alcohol, drug, overdose, smoking...\n2. alcohol:\n   Keywords: wine, consumption, addiction, abstinence, sud...\n3. arthritis:\n   Keywords: pain, knee, osteoarthritis, joint, nsaids...\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: BUILD CONCEPT KNOWLEDGE GRAPHS\n# ============================================================================\n\nclass ConceptKGBuilder:\n    \"\"\"Build knowledge graphs for concepts using topic names and keywords.\"\"\"\n    \n    def __init__(self):\n        self.concept_kgs = {}\n    \n    def build_concept_kg(self, topic_name: str, keywords: List[str]) -> nx.DiGraph:\n        \"\"\"Build a KG for a single concept.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add topic as central node\n        G.add_node(topic_name, node_type='topic')\n        \n        # Add keywords and connect to topic\n        for keyword in keywords:\n            G.add_node(keyword, node_type='keyword')\n            G.add_edge(topic_name, keyword, relation='has_keyword')\n        \n        # Connect keywords to each other (co-occurrence)\n        for i, kw1 in enumerate(keywords):\n            for kw2 in keywords[i+1:]:\n                G.add_edge(kw1, kw2, relation='co_occurs')\n        \n        return G\n    \n    def build_all_concept_kgs(self, concept_keywords: Dict[str, List[str]]) -> Dict[str, nx.DiGraph]:\n        \"\"\"Build KGs for all concepts.\"\"\"\n        for topic, keywords in concept_keywords.items():\n            self.concept_kgs[topic] = self.build_concept_kg(topic, keywords)\n        \n        print(f\"\\n{'='*80}\")\n        print(\"BUILT CONCEPT KNOWLEDGE GRAPHS\")\n        print(f\"{'='*80}\")\n        print(f\"Total concept KGs: {len(self.concept_kgs)}\")\n        \n        # Show stats\n        for topic, kg in list(self.concept_kgs.items())[:3]:\n            print(f\"  {topic}: {kg.number_of_nodes()} nodes, {kg.number_of_edges()} edges\")\n        \n        return self.concept_kgs\n\n# Build concept KGs\nkg_builder = ConceptKGBuilder()\nconcept_kgs = kg_builder.build_all_concept_kgs(concept_keywords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:41:16.618439Z","iopub.execute_input":"2025-11-11T17:41:16.619107Z","iopub.status.idle":"2025-11-11T17:41:16.627511Z","shell.execute_reply.started":"2025-11-11T17:41:16.619084Z","shell.execute_reply":"2025-11-11T17:41:16.626597Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nBUILT CONCEPT KNOWLEDGE GRAPHS\n================================================================================\nTotal concept KGs: 18\n  addiction: 11 nodes, 55 edges\n  alcohol: 7 nodes, 21 edges\n  arthritis: 9 nodes, 36 edges\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: GRAPH EMBEDDER\n# ============================================================================\n\nclass GraphEmbedder:\n    \"\"\"Embed graphs using Graph2Vec.\"\"\"\n    \n    def __init__(self, dimensions=128, wl_iterations=3):\n        \"\"\"\n        Args:\n            dimensions: Embedding dimension\n            wl_iterations: Weisfeiler-Lehman iterations\n        \"\"\"\n        self.dimensions = dimensions\n        self.model = Graph2Vec(\n            dimensions=dimensions,\n            wl_iterations=wl_iterations,\n            epochs=100,\n            min_count=1\n        )\n        print(f\"\\n✓ Initialized Graph2Vec (dim={dimensions}, WL={wl_iterations})\")\n    \n    def embed_graphs(self, graphs: List[nx.Graph]) -> np.ndarray:\n        \"\"\"\n        Embed multiple graphs.\n        \n        Args:\n            graphs: List of NetworkX graphs\n        \n        Returns:\n            Embedding matrix (n_graphs × dimensions)\n        \"\"\"\n        # Convert to undirected (Graph2Vec requires undirected)\n        undirected = [G.to_undirected() if G.is_directed() else G for G in graphs]\n        \n        # Fit and get embeddings\n        self.model.fit(undirected)\n        embeddings = self.model.get_embedding()\n        \n        return embeddings\n    \n    def embed_single_graph(self, G: nx.Graph, reference_graphs: List[nx.Graph]) -> np.ndarray:\n        \"\"\"\n        Embed a single new graph using reference graphs.\n        \n        Args:\n            G: Graph to embed\n            reference_graphs: Graphs to fit model on\n        \n        Returns:\n            Single embedding vector\n        \"\"\"\n        # Combine reference graphs with new graph\n        all_graphs = reference_graphs + [G]\n        embeddings = self.embed_graphs(all_graphs)\n        \n        # Return embedding of the last graph (the new one)\n        return embeddings[-1]\n\nprint(\"✓ GraphEmbedder class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:41:20.538367Z","iopub.execute_input":"2025-11-11T17:41:20.539014Z","iopub.status.idle":"2025-11-11T17:41:20.545844Z","shell.execute_reply.started":"2025-11-11T17:41:20.538989Z","shell.execute_reply":"2025-11-11T17:41:20.544933Z"}},"outputs":[{"name":"stdout","text":"✓ GraphEmbedder class defined\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: GRAPH-BASED TOPIC CLASSIFIER\n# ============================================================================\n\nclass GraphTopicClassifier:\n    \"\"\"Classify articles using graph embeddings.\"\"\"\n    \n    def __init__(self, \n                 concept_kgs: Dict[str, nx.DiGraph],\n                 embedding_dim: int = 128,\n                 threshold: float = 0.3):\n        \"\"\"\n        Initialize classifier.\n        \n        Args:\n            concept_kgs: Dictionary of concept KGs\n            embedding_dim: Embedding dimension\n            threshold: Similarity threshold for topic assignment\n        \"\"\"\n        self.concept_kgs = concept_kgs\n        self.threshold = threshold\n        \n        print(f\"\\n{'='*80}\")\n        print(\"INITIALIZING GRAPH-BASED CLASSIFIER\")\n        print(f\"{'='*80}\")\n        \n        # Initialize embedder\n        self.embedder = GraphEmbedder(dimensions=embedding_dim)\n        \n        # Prepare concept graphs\n        self.concept_names = list(concept_kgs.keys())\n        concept_graphs = [concept_kgs[name] for name in self.concept_names]\n        \n        # Embed all concept graphs\n        print(\"\\nEmbedding concept knowledge graphs...\")\n        self.concept_embeddings = self.embedder.embed_graphs(concept_graphs)\n        \n        print(f\"✓ Embedded {len(self.concept_names)} concept graphs\")\n        print(f\"  Embedding shape: {self.concept_embeddings.shape}\")\n        print(f\"  Similarity threshold: {threshold}\")\n        \n        # Store as dictionary\n        self.concept_embedding_dict = {\n            name: self.concept_embeddings[i] \n            for i, name in enumerate(self.concept_names)\n        }\n    \n    def _dict_to_graph(self, kg_dict: Dict) -> nx.DiGraph:\n        \"\"\"Convert KG dictionary to NetworkX graph.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add nodes from entities\n        entities = kg_dict.get('entities', [])\n        for entity in entities:\n            G.add_node(entity)\n        \n        # Add edges from relations\n        relations = kg_dict.get('relations', [])\n        for relation in relations:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                G.add_edge(source, target, relation=rel_type)\n        \n        return G\n    \n    def classify_article(self, article_kg: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        # Convert to graph\n        G_article = self._dict_to_graph(article_kg)\n        \n        # Skip if empty\n        if G_article.number_of_nodes() == 0:\n            return {\n                'predicted_topics': [],\n                'similarities': {},\n                'max_similarity': 0.0\n            }\n        \n        # Embed article graph\n        reference_graphs = [self.concept_kgs[name] for name in self.concept_names]\n        article_embedding = self.embedder.embed_single_graph(\n            G_article, \n            reference_graphs=reference_graphs\n        )\n        \n        # Compute similarities with all concepts\n        similarities = {}\n        for topic_name in self.concept_names:\n            concept_emb = self.concept_embedding_dict[topic_name]\n            sim = cosine_similarity([article_embedding], [concept_emb])[0][0]\n            similarities[topic_name] = float(sim)\n        \n        # Assign topics above threshold\n        predicted_topics = [\n            topic for topic, sim in similarities.items() \n            if sim >= self.threshold\n        ]\n        \n        # Sort by similarity\n        predicted_topics = sorted(\n            predicted_topics, \n            key=lambda t: similarities[t], \n            reverse=True\n        )\n        \n        return {\n            'predicted_topics': predicted_topics,\n            'similarities': similarities,\n            'max_similarity': max(similarities.values()) if similarities else 0.0\n        }\n    \n    def classify_dataset(self, \n                        articles: List[Dict],\n                        knowledge_graphs: List[Dict]) -> List[Dict]:\n        \"\"\"Classify all articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(knowledge_graphs)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx in range(len(knowledge_graphs)):\n            if idx % 20 == 0:\n                print(f\"Processing articles {idx}-{min(idx+20, len(knowledge_graphs))}...\")\n            \n            article = articles[idx] if idx < len(articles) else {}\n            article_kg = kg_dict.get(idx, {'entities': [], 'relations': []})\n            \n            # Skip if no KG\n            if article_kg.get('num_edges', 0) == 0:\n                results.append({\n                    'article_id': idx,\n                    'predicted_topics': [],\n                    'similarities': {},\n                    'max_similarity': 0.0,\n                    'true_concepts': article.get('Concept', [])\n                })\n                continue\n            \n            # Classify\n            classification = self.classify_article(article_kg)\n            \n            result = {\n                'article_id': idx,\n                'predicted_topics': classification['predicted_topics'],\n                'similarities': classification['similarities'],\n                'max_similarity': classification['max_similarity'],\n                'true_concepts': article.get('Concept', [])\n            }\n            \n            results.append(result)\n        \n        print(\"\\n✓ Classification complete!\")\n        return results\n\nprint(\"✓ GraphTopicClassifier class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:41:24.648468Z","iopub.execute_input":"2025-11-11T17:41:24.648724Z","iopub.status.idle":"2025-11-11T17:41:24.663301Z","shell.execute_reply.started":"2025-11-11T17:41:24.648706Z","shell.execute_reply":"2025-11-11T17:41:24.662421Z"}},"outputs":[{"name":"stdout","text":"✓ GraphTopicClassifier class defined\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: GRAPH-BASED TOPIC CLASSIFIER\n# ============================================================================\n\nclass GraphTopicClassifier:\n    \"\"\"Classify articles using graph embeddings.\"\"\"\n    \n    def __init__(self, \n                 concept_kgs: Dict[str, nx.DiGraph],\n                 embedding_dim: int = 128,\n                 threshold: float = 0.3):\n        \"\"\"\n        Initialize classifier.\n        \n        Args:\n            concept_kgs: Dictionary of concept KGs\n            embedding_dim: Embedding dimension\n            threshold: Similarity threshold for topic assignment\n        \"\"\"\n        self.concept_kgs = concept_kgs\n        self.threshold = threshold\n        \n        print(f\"\\n{'='*80}\")\n        print(\"INITIALIZING GRAPH-BASED CLASSIFIER\")\n        print(f\"{'='*80}\")\n        \n        # Initialize embedder\n        self.embedder = GraphEmbedder(dimensions=embedding_dim)\n        \n        # Prepare concept graphs\n        self.concept_names = list(concept_kgs.keys())\n        concept_graphs = [concept_kgs[name] for name in self.concept_names]\n        \n        # Embed all concept graphs\n        print(\"\\nEmbedding concept knowledge graphs...\")\n        self.concept_embeddings = self.embedder.embed_graphs(concept_graphs)\n        \n        print(f\"✓ Embedded {len(self.concept_names)} concept graphs\")\n        print(f\"  Embedding shape: {self.concept_embeddings.shape}\")\n        print(f\"  Similarity threshold: {threshold}\")\n        \n        # Store as dictionary\n        self.concept_embedding_dict = {\n            name: self.concept_embeddings[i] \n            for i, name in enumerate(self.concept_names)\n        }\n    \n    def _dict_to_graph(self, kg_dict: Dict) -> nx.DiGraph:\n        \"\"\"Convert KG dictionary to NetworkX graph.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add nodes from entities\n        entities = kg_dict.get('entities', [])\n        for entity in entities:\n            G.add_node(entity)\n        \n        # Add edges from relations\n        relations = kg_dict.get('relations', [])\n        for relation in relations:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                G.add_edge(source, target, relation=rel_type)\n        \n        return G\n    \n    def classify_article(self, article_kg: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        # Convert to graph\n        G_article = self._dict_to_graph(article_kg)\n        \n        # Skip if empty\n        if G_article.number_of_nodes() == 0:\n            return {\n                'predicted_topics': [],\n                'similarities': {},\n                'max_similarity': 0.0\n            }\n        \n        # Embed article graph\n        reference_graphs = [self.concept_kgs[name] for name in self.concept_names]\n        article_embedding = self.embedder.embed_single_graph(\n            G_article, \n            reference_graphs=reference_graphs\n        )\n        \n        # Compute similarities with all concepts\n        similarities = {}\n        for topic_name in self.concept_names:\n            concept_emb = self.concept_embedding_dict[topic_name]\n            sim = cosine_similarity([article_embedding], [concept_emb])[0][0]\n            similarities[topic_name] = float(sim)\n        \n        # Assign topics above threshold\n        predicted_topics = [\n            topic for topic, sim in similarities.items() \n            if sim >= self.threshold\n        ]\n        \n        # Sort by similarity\n        predicted_topics = sorted(\n            predicted_topics, \n            key=lambda t: similarities[t], \n            reverse=True\n        )\n        \n        return {\n            'predicted_topics': predicted_topics,\n            'similarities': similarities,\n            'max_similarity': max(similarities.values()) if similarities else 0.0\n        }\n    \n    def classify_dataset(self, \n                        articles: List[Dict],\n                        knowledge_graphs: List[Dict]) -> List[Dict]:\n        \"\"\"Classify all articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(knowledge_graphs)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx in range(len(knowledge_graphs)):\n            if idx % 20 == 0:\n                print(f\"Processing articles {idx}-{min(idx+20, len(knowledge_graphs))}...\")\n            \n            article = articles[idx] if idx < len(articles) else {}\n            article_kg = kg_dict.get(idx, {'entities': [], 'relations': []})\n            \n            # Skip if no KG\n            if article_kg.get('num_edges', 0) == 0:\n                results.append({\n                    'article_id': idx,\n                    'predicted_topics': [],\n                    'similarities': {},\n                    'max_similarity': 0.0,\n                    'true_concepts': article.get('Concept', [])\n                })\n                continue\n            \n            # Classify\n            classification = self.classify_article(article_kg)\n            \n            result = {\n                'article_id': idx,\n                'predicted_topics': classification['predicted_topics'],\n                'similarities': classification['similarities'],\n                'max_similarity': classification['max_similarity'],\n                'true_concepts': article.get('Concept', [])\n            }\n            \n            results.append(result)\n        \n        print(\"\\n✓ Classification complete!\")\n        return results\n\nprint(\"✓ GraphTopicClassifier class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:41:28.916235Z","iopub.execute_input":"2025-11-11T17:41:28.916910Z","iopub.status.idle":"2025-11-11T17:41:28.934011Z","shell.execute_reply.started":"2025-11-11T17:41:28.916882Z","shell.execute_reply":"2025-11-11T17:41:28.933072Z"}},"outputs":[{"name":"stdout","text":"✓ GraphTopicClassifier class defined\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y node2vec gensim smart_open\n!pip install smart_open==5.2.1 --force-reinstall\n!pip install gensim==4.3.2 --force-reinstall\n!pip install node2vec==0.4.6 --force-reinstall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:24:03.439043Z","iopub.status.idle":"2025-11-11T15:24:03.439327Z","shell.execute_reply.started":"2025-11-11T15:24:03.439204Z","shell.execute_reply":"2025-11-11T15:24:03.439217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install smart_open==5.2.1 --force-reinstall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:24:03.440226Z","iopub.status.idle":"2025-11-11T15:24:03.440519Z","shell.execute_reply.started":"2025-11-11T15:24:03.440371Z","shell.execute_reply":"2025-11-11T15:24:03.440383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import os, sys\n#os._exit(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:24:03.441913Z","iopub.status.idle":"2025-11-11T15:24:03.442165Z","shell.execute_reply.started":"2025-11-11T15:24:03.442055Z","shell.execute_reply":"2025-11-11T15:24:03.442069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nimport networkx as nx\nfrom typing import List, Dict\nfrom node2vec import Node2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\n\nprint(\"✓ Starting Multi-Label Article Classifier\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:41:34.350545Z","iopub.execute_input":"2025-11-11T17:41:34.351228Z","iopub.status.idle":"2025-11-11T17:41:34.355799Z","shell.execute_reply.started":"2025-11-11T17:41:34.351202Z","shell.execute_reply":"2025-11-11T17:41:34.354918Z"}},"outputs":[{"name":"stdout","text":"✓ Starting Multi-Label Article Classifier\n\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: GRAPH EMBEDDER\n# ============================================================================\n\nclass GraphEmbedder:\n    \"\"\"Embed graphs using Node2Vec.\"\"\"\n    \n    def __init__(self, dimensions=128, walk_length=30, num_walks=200):\n        self.dimensions = dimensions\n        self.walk_length = walk_length\n        self.num_walks = num_walks\n        print(f\"✓ Initialized Node2Vec embedder (dim={dimensions})\")\n    \n    def embed_graph(self, G: nx.Graph) -> np.ndarray:\n        \"\"\"Embed a single graph.\"\"\"\n        # Convert to undirected\n        G_undirected = G.to_undirected() if G.is_directed() else G\n        \n        # Skip if too small\n        if G_undirected.number_of_nodes() < 2:\n            return np.zeros(self.dimensions)\n        \n        try:\n            # Fit Node2Vec\n            node2vec = Node2Vec(\n                G_undirected, \n                dimensions=self.dimensions,\n                walk_length=self.walk_length,\n                num_walks=self.num_walks,\n                workers=1,\n                quiet=True\n            )\n            \n            model = node2vec.fit(window=10, min_count=1, batch_words=4)\n            \n            # Get embeddings for all nodes\n            node_embeddings = []\n            for node in G_undirected.nodes():\n                node_embeddings.append(model.wv[str(node)])\n            \n            # Average all node embeddings to get graph embedding\n            graph_embedding = np.mean(node_embeddings, axis=0)\n            return graph_embedding\n        except:\n            return np.zeros(self.dimensions)\n    \n    def embed_graphs(self, graphs: List[nx.Graph]) -> np.ndarray:\n        \"\"\"Embed multiple graphs.\"\"\"\n        embeddings = []\n        for i, G in enumerate(graphs):\n            if i % 5 == 0:\n                print(f\"  Embedding graph {i+1}/{len(graphs)}...\")\n            emb = self.embed_graph(G)\n            embeddings.append(emb)\n        return np.array(embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:41:36.733555Z","iopub.execute_input":"2025-11-11T17:41:36.733810Z","iopub.status.idle":"2025-11-11T17:41:36.741085Z","shell.execute_reply.started":"2025-11-11T17:41:36.733790Z","shell.execute_reply":"2025-11-11T17:41:36.740396Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: MULTI-LABEL GRAPH CLASSIFIER\n# ============================================================================\n\nclass MultiLabelGraphClassifier:\n    \"\"\"Multi-label classifier using graph embeddings.\"\"\"\n    \n    def __init__(self, \n                 concept_kgs: Dict[str, nx.DiGraph],\n                 embedding_dim: int = 128,\n                 threshold: float = 0.5,  # Increased from 0.3\n                 top_k: int = 3):  # Max labels per article\n        \"\"\"\n        Initialize classifier.\n        \n        Args:\n            concept_kgs: Dictionary of concept knowledge graphs\n            embedding_dim: Embedding dimension\n            threshold: Minimum similarity threshold (increased to 0.5)\n            top_k: Maximum number of topics to assign per article\n        \"\"\"\n        self.concept_kgs = concept_kgs\n        self.threshold = threshold\n        self.top_k = top_k\n        \n        print(f\"\\n{'='*80}\")\n        print(\"INITIALIZING MULTI-LABEL GRAPH CLASSIFIER\")\n        print(f\"{'='*80}\")\n        \n        # Initialize embedder\n        self.embedder = GraphEmbedder(dimensions=embedding_dim)\n        \n        # Prepare concept graphs\n        self.concept_names = list(concept_kgs.keys())\n        concept_graphs = [concept_kgs[name] for name in self.concept_names]\n        \n        # Embed all concept graphs\n        print(\"\\nEmbedding concept knowledge graphs...\")\n        self.concept_embeddings = self.embedder.embed_graphs(concept_graphs)\n        \n        print(f\"\\n✓ Embedded {len(self.concept_names)} concept graphs\")\n        print(f\"  Embedding shape: {self.concept_embeddings.shape}\")\n        print(f\"  Absolute threshold: {threshold}\")\n        print(f\"  Top-K limit: {self.top_k}\")\n        print(f\"  Relative gap threshold: 0.15\")\n        \n        # Store as dictionary\n        self.concept_embedding_dict = {\n            name: self.concept_embeddings[i] \n            for i, name in enumerate(self.concept_names)\n        }\n    \n    def _dict_to_graph(self, kg_dict: Dict) -> nx.DiGraph:\n        \"\"\"Convert KG dictionary to NetworkX graph.\"\"\"\n        G = nx.DiGraph()\n        \n        # Add nodes from entities\n        entities = kg_dict.get('entities', [])\n        for entity in entities:\n            G.add_node(entity)\n        \n        # Add edges from relations\n        relations = kg_dict.get('relations', [])\n        for relation in relations:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                G.add_edge(source, target, relation=rel_type)\n        \n        return G\n    \n    def classify_article(self, article_kg: Dict, top_k: int = 3) -> Dict:\n        \"\"\"\n        Classify a single article (multi-label).\n        \n        Strategy: Use adaptive thresholding to prevent overprediction.\n        - Apply absolute threshold\n        - Take top-K only\n        - Use relative threshold (gap from max)\n        \n        Args:\n            article_kg: Article knowledge graph dictionary\n            top_k: Maximum number of labels to assign\n        \"\"\"\n        # Convert to graph\n        G_article = self._dict_to_graph(article_kg)\n        \n        # Skip if empty\n        if G_article.number_of_nodes() == 0:\n            return {\n                'predicted_topics': [],\n                'similarities': {},\n                'max_similarity': 0.0\n            }\n        \n        # Embed article graph\n        article_embedding = self.embedder.embed_graph(G_article)\n        \n        # Compute similarities with all concepts\n        similarities = {}\n        for topic_name in self.concept_names:\n            concept_emb = self.concept_embedding_dict[topic_name]\n            sim = cosine_similarity([article_embedding], [concept_emb])[0][0]\n            similarities[topic_name] = float(sim)\n        \n        # Get sorted topics by similarity\n        sorted_topics = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n        \n        # Strategy 1: Absolute threshold\n        candidates = [(topic, sim) for topic, sim in sorted_topics if sim >= self.threshold]\n        \n        if len(candidates) == 0:\n            return {\n                'predicted_topics': [],\n                'similarities': similarities,\n                'max_similarity': max(similarities.values()) if similarities else 0.0\n            }\n        \n        # Strategy 2: Top-K limit (prevent overprediction)\n        candidates = candidates[:top_k]\n        \n        # Strategy 3: Relative threshold (gap detection)\n        # Only keep topics within 0.15 similarity of the best match\n        max_sim = candidates[0][1]\n        relative_threshold = max_sim - 0.35\n        \n        predicted_topics = [\n            topic for topic, sim in candidates\n            if sim >= relative_threshold\n        ]\n        \n        return {\n            'predicted_topics': predicted_topics,\n            'similarities': similarities,\n            'max_similarity': max(similarities.values()) if similarities else 0.0\n        }\n    \n    def classify_dataset(self, \n                        articles: List[Dict],\n                        knowledge_graphs: List[Dict]) -> List[Dict]:\n        \"\"\"Classify all articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(knowledge_graphs)} ARTICLES (MULTI-LABEL)\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx in range(len(knowledge_graphs)):\n            if idx % 20 == 0:\n                print(f\"Processing articles {idx}-{min(idx+20, len(knowledge_graphs))}...\")\n            \n            article = articles[idx] if idx < len(articles) else {}\n            article_kg = kg_dict.get(idx, {'entities': [], 'relations': []})\n            \n            # Skip if no KG\n            if article_kg.get('num_edges', 0) == 0:\n                results.append({\n                    'article_id': idx,\n                    'predicted_topics': [],\n                    'similarities': {},\n                    'max_similarity': 0.0,\n                    'true_concepts': article.get('Concept', [])\n                })\n                continue\n            \n            # Classify (multi-label)\n            classification = self.classify_article(article_kg)\n            \n            result = {\n                'article_id': idx,\n                'predicted_topics': classification['predicted_topics'],\n                'num_predicted': len(classification['predicted_topics']),\n                'similarities': classification['similarities'],\n                'max_similarity': classification['max_similarity'],\n                'true_concepts': article.get('Concept', [])\n            }\n            \n            results.append(result)\n        \n        print(\"\\n✓ Classification complete!\")\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:58:59.037230Z","iopub.execute_input":"2025-11-11T17:58:59.037538Z","iopub.status.idle":"2025-11-11T17:58:59.086231Z","shell.execute_reply.started":"2025-11-11T17:58:59.037506Z","shell.execute_reply":"2025-11-11T17:58:59.085346Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_39/3012426908.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMultiLabelGraphClassifier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"\"\"Multi-label classifier using graph embeddings.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_39/3012426908.py\u001b[0m in \u001b[0;36mMultiLabelGraphClassifier\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     def __init__(self, \n\u001b[0;32m----> 9\u001b[0;31m                  \u001b[0mconcept_kgs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                  \u001b[0membedding_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                  \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Increased from 0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"],"ename":"NameError","evalue":"name 'nx' is not defined","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: EVALUATION METRICS\n# ============================================================================\n\ndef calculate_metrics(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate multi-label classification metrics.\n    Uses micro-averaged precision, recall, F1.\n    \"\"\"\n    total_tp = 0\n    total_fp = 0\n    total_fn = 0\n    \n    articles_evaluated = 0\n    articles_skipped = 0\n    \n    multi_label_count = 0\n    single_label_count = 0\n    no_label_count = 0\n    \n    for result in results:\n        true_topics = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_topics = set([t.lower().strip() for t in result['predicted_topics']])\n        \n        if len(true_topics) == 0:\n            articles_skipped += 1\n            continue\n        \n        articles_evaluated += 1\n        \n        # Count label distribution\n        if len(pred_topics) == 0:\n            no_label_count += 1\n        elif len(pred_topics) == 1:\n            single_label_count += 1\n        else:\n            multi_label_count += 1\n        \n        tp = len(true_topics & pred_topics)\n        fp = len(pred_topics - true_topics)\n        fn = len(true_topics - pred_topics)\n        \n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n    \n    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    metrics = {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'total_tp': total_tp,\n        'total_fp': total_fp,\n        'total_fn': total_fn,\n        'articles_evaluated': articles_evaluated,\n        'articles_skipped': articles_skipped,\n        'multi_label_count': multi_label_count,\n        'single_label_count': single_label_count,\n        'no_label_count': no_label_count\n    }\n    \n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS (Micro-Averaged)\")\n    print(f\"{'='*80}\")\n    print(f\"\\nArticles evaluated: {articles_evaluated}\")\n    print(f\"Articles skipped (no ground truth): {articles_skipped}\")\n    print(f\"\\nLabel Distribution:\")\n    print(f\"  No labels: {no_label_count} ({no_label_count/articles_evaluated*100:.1f}%)\")\n    print(f\"  Single label: {single_label_count} ({single_label_count/articles_evaluated*100:.1f}%)\")\n    print(f\"  Multi-label: {multi_label_count} ({multi_label_count/articles_evaluated*100:.1f}%)\")\n    print(f\"\\nAggregated Statistics:\")\n    print(f\"  True Positives (TP): {total_tp}\")\n    print(f\"  False Positives (FP): {total_fp}\")\n    print(f\"  False Negatives (FN): {total_fn}\")\n    print(f\"\\n📊 PERFORMANCE:\")\n    print(f\"  Precision: {precision:.3f}\")\n    print(f\"  Recall: {recall:.3f}\")\n    print(f\"  F1 Score: {f1_score:.3f}\")\n    \n    # Comparison with baselines\n    print(f\"\\n{'='*80}\")\n    print(\"COMPARISON WITH PAPER BASELINES (Medical Dataset)\")\n    print(f\"{'='*80}\")\n    print(f\"  This model:           F1 = {f1_score:.3f}\")\n    print(f\"  GFLM-S baseline:      F1 = 0.532\")\n    print(f\"  GFLM-W baseline:      F1 = 0.530\")\n    print(f\"  SBERT:                F1 = 0.594\")\n    print(f\"  ChatGPT-3.5:          F1 = 0.606\")\n    \n    return metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:41:45.756222Z","iopub.execute_input":"2025-11-11T17:41:45.756496Z","iopub.status.idle":"2025-11-11T17:41:45.765783Z","shell.execute_reply.started":"2025-11-11T17:41:45.756475Z","shell.execute_reply":"2025-11-11T17:41:45.764959Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: RUN CLASSIFICATION\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"RUNNING MULTI-LABEL CLASSIFICATION\")\nprint(\"=\"*80)\n\n# Initialize classifier with concept KGs\nclassifier = MultiLabelGraphClassifier(\n    concept_kgs=concept_kgs,\n    embedding_dim=128,\n    threshold=0.5,  # Higher threshold = fewer predictions\n    top_k=5  # Max 3 labels per article\n)\n\n# Classify all articles\nresults = classifier.classify_dataset(\n    articles=articles,\n    knowledge_graphs=knowledge_graphs\n)\n\n# Calculate metrics\nmetrics = calculate_metrics(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:58:47.876927Z","iopub.execute_input":"2025-11-11T17:58:47.877163Z","iopub.status.idle":"2025-11-11T17:58:47.907463Z","shell.execute_reply.started":"2025-11-11T17:58:47.877147Z","shell.execute_reply":"2025-11-11T17:58:47.906514Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nRUNNING MULTI-LABEL CLASSIFICATION\n================================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_39/3731974826.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Initialize classifier with concept KGs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m classifier = MultiLabelGraphClassifier(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mconcept_kgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcept_kgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MultiLabelGraphClassifier' is not defined"],"ename":"NameError","evalue":"name 'MultiLabelGraphClassifier' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# THREE APPROACHES TO TOPIC EMBEDDING (FIXED VERSION)\n# ============================================================================\n\n!pip install sentence-transformers -q\n\nimport json\nimport numpy as np\nfrom typing import List, Dict\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\n\nprint(\"✓ Imports complete\\n\")\n\n\nclass SimpleKGClassifier:\n    \"\"\"Classify articles by comparing KG embeddings to topic embeddings.\"\"\"\n    \n    def __init__(self, \n                 topic_names: List[str],\n                 topic_keywords: Dict[str, List[str]] = None,\n                 topic_embedding_method: str = 'name_only',\n                 threshold: float = 0.3):\n        \"\"\"\n        Initialize classifier.\n        \n        Args:\n            topic_names: List of topic names\n            topic_keywords: Dictionary mapping topics to keywords\n            topic_embedding_method: 'name_only', 'name_plus_keywords', or 'average_embeddings'\n            threshold: Similarity threshold\n        \"\"\"\n        self.topic_names = [t.lower() for t in topic_names]\n        self.topic_keywords = topic_keywords\n        self.method = topic_embedding_method\n        self.threshold = threshold\n        \n        # Initialize SBERT model\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"INITIALIZING CLASSIFIER - METHOD: {topic_embedding_method.upper()}\")\n        print(f\"{'='*80}\")\n        print(f\"Total topics: {len(self.topic_names)}\")\n        print(f\"Threshold: {threshold}\\n\")\n        \n        # Embed topics based on method\n        print(f\"Embedding topics using method: {topic_embedding_method}...\")\n        \n        if topic_embedding_method == 'name_only':\n            self.topic_embeddings = self._embed_method1_name_only()\n        elif topic_embedding_method == 'name_plus_keywords':\n            self.topic_embeddings = self._embed_method2_name_plus_keywords()\n        elif topic_embedding_method == 'average_embeddings':\n            self.topic_embeddings = self._embed_method3_average_embeddings()\n        else:\n            raise ValueError(f\"Unknown method: {topic_embedding_method}\")\n        \n        print(f\"✓ Embedded {len(self.topic_names)} topics\")\n        print(f\"  Embedding dimension: {self.topic_embeddings.shape[1]}\")\n    \n    def _embed_method1_name_only(self) -> np.ndarray:\n        \"\"\"Method 1: Embed only topic names.\"\"\"\n        print(\"  Method 1: Topic name only\")\n        print(f\"  Example: 'phone' → embedding\")\n        \n        embeddings = self.model.encode(self.topic_names)\n        return embeddings\n    \n    def _embed_method2_name_plus_keywords(self) -> np.ndarray:\n        \"\"\"Method 2: Embed topic + keywords as concatenated text.\"\"\"\n        print(\"  Method 2: Topic + keywords as text\")\n        \n        topic_texts = []\n        for topic in self.topic_names:\n            if self.topic_keywords and topic in self.topic_keywords and self.topic_keywords[topic]:\n                keywords = \", \".join(self.topic_keywords[topic][:10])\n                topic_text = f\"{topic}: {keywords}\"\n            else:\n                topic_text = topic\n            topic_texts.append(topic_text)\n        \n        print(f\"  Example: '{topic_texts[0][:80]}...'\")\n        \n        embeddings = self.model.encode(topic_texts)\n        return embeddings\n    \n    def _embed_method3_average_embeddings(self) -> np.ndarray:\n        \"\"\"Method 3: Average embeddings of topic name + keywords.\"\"\"\n        print(\"  Method 3: Average of topic and keyword embeddings\")\n        \n        all_embeddings = []\n        \n        for topic in self.topic_names:\n            # Embed topic name\n            topic_emb = self.model.encode(topic)\n            \n            # Embed keywords if available\n            if self.topic_keywords and topic in self.topic_keywords and self.topic_keywords[topic]:\n                keywords = self.topic_keywords[topic][:10]\n                keyword_embs = self.model.encode(keywords)\n                \n                # Average topic + keyword embeddings\n                all_embs = np.vstack([topic_emb.reshape(1, -1), keyword_embs])\n                avg_emb = np.mean(all_embs, axis=0)\n            else:\n                # No keywords - just use topic embedding\n                avg_emb = topic_emb\n            \n            all_embeddings.append(avg_emb)\n        \n        print(f\"  Example: avg(embed('phone'), embed('nokia'), embed('quality'))\")\n        \n        return np.array(all_embeddings)\n    \n    def kg_to_text(self, kg_dict: Dict) -> str:\n        \"\"\"Convert KG to text representation.\"\"\"\n        entities = kg_dict.get('entities', [])\n        \n        if not entities:\n            return \"\"\n        \n        # Just list entities\n        text = \", \".join(entities[:50])\n        return text\n    \n    def classify_article(self, article_kg: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        kg_text = self.kg_to_text(article_kg)\n        \n        if not kg_text:\n            return {\n                'predicted_topics': [],\n                'similarities': {},\n                'max_similarity': 0.0\n            }\n        \n        # Embed KG\n        kg_embedding = self.model.encode(kg_text)\n        \n        # Compute similarities\n        similarities = {}\n        for i, topic_name in enumerate(self.topic_names):\n            sim = cosine_similarity(\n                [kg_embedding], \n                [self.topic_embeddings[i]]\n            )[0][0]\n            similarities[topic_name] = float(sim)\n        \n        # Assign topics above threshold\n        predicted_topics = [\n            topic for topic, sim in similarities.items() \n            if sim >= self.threshold\n        ]\n        \n        predicted_topics = sorted(\n            predicted_topics, \n            key=lambda t: similarities[t], \n            reverse=True\n        )\n        \n        return {\n            'predicted_topics': predicted_topics,\n            'similarities': similarities,\n            'max_similarity': max(similarities.values()) if similarities else 0.0\n        }\n    \n    def classify_dataset(self, \n                        articles: List[Dict],\n                        knowledge_graphs: List[Dict]) -> List[Dict]:\n        \"\"\"Classify all articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(knowledge_graphs)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx in range(len(knowledge_graphs)):\n            if idx % 20 == 0:\n                print(f\"Processing articles {idx}-{min(idx+20, len(knowledge_graphs))}...\")\n            \n            article = articles[idx] if idx < len(articles) else {}\n            article_kg = kg_dict.get(idx, {'entities': [], 'relations': []})\n            \n            classification = self.classify_article(article_kg)\n            \n            result = {\n                'article_id': idx,\n                'predicted_topics': classification['predicted_topics'],\n                'similarities': classification['similarities'],\n                'max_similarity': classification['max_similarity'],\n                'true_concepts': article.get('Concept', [])\n            }\n            \n            results.append(result)\n        \n        print(\"\\n✓ Classification complete!\")\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:52:11.168408Z","iopub.execute_input":"2025-11-11T17:52:11.168619Z","iopub.status.idle":"2025-11-11T17:54:09.792037Z","shell.execute_reply.started":"2025-11-11T17:52:11.168601Z","shell.execute_reply":"2025-11-11T17:54:09.790645Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m525.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-11 17:53:55.078854: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762883635.257179      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762883635.312943      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✓ Imports complete\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================================\n# TEST ALL THREE METHODS\n# ============================================================================\n\n# Get topic names\nall_topic_names = list(concept_keywords.keys())\n\nmethods = {\n    'name_only': 'Method 1: Topic Name Only',\n    'name_plus_keywords': 'Method 2: Topic + Keywords as Text',\n    'average_embeddings': 'Method 3: Average Embeddings'\n}\n\nresults_comparison = {}\n\nfor method_key, method_name in methods.items():\n    print(f\"\\n\\n{'#'*80}\")\n    print(f\"TESTING: {method_name}\")\n    print(f\"{'#'*80}\")\n    \n    # Initialize classifier\n    classifier = SimpleKGClassifier(\n        topic_names=all_topic_names,\n        topic_keywords=concept_keywords,\n        topic_embedding_method=method_key,\n        threshold=0.3\n    )\n    \n    # Classify\n    results = classifier.classify_dataset(\n        articles=articles,\n        knowledge_graphs=knowledge_graphs\n    )\n    \n    # Evaluate\n    metrics = calculate_metrics_paper_style(results)\n    \n    # Store results\n    results_comparison[method_key] = {\n        'metrics': metrics,\n        'results': results\n    }\n    \n    # Save\n    save_results(results, f'/kaggle/working/{method_key}_results.json')\n    export_to_csv(results, f'/kaggle/working/{method_key}_results.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T17:58:07.715248Z","iopub.execute_input":"2025-11-11T17:58:07.715550Z","iopub.status.idle":"2025-11-11T17:58:07.747462Z","shell.execute_reply.started":"2025-11-11T17:58:07.715532Z","shell.execute_reply":"2025-11-11T17:58:07.746195Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_39/3925412041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get topic names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mall_topic_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m methods = {\n","\u001b[0;31mNameError\u001b[0m: name 'concept_keywords' is not defined"],"ename":"NameError","evalue":"name 'concept_keywords' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# FINAL COMPARISON OF ALL METHODS\n# ============================================================================\n\nprint(f\"\\n\\n{'='*80}\")\nprint(\"FINAL COMPARISON: ALL METHODS\")\nprint(f\"{'='*80}\\n\")\n\ncomparison_df = pd.DataFrame({\n    'Method': [\n        'Method 1: Name Only',\n        'Method 2: Name + Keywords Text',\n        'Method 3: Average Embeddings',\n        'LLM Reasoning (baseline)',\n        'Paper ChatGPT (baseline)'\n    ],\n    'F1 Score': [\n        results_comparison['name_only']['metrics']['f1_score'],\n        results_comparison['name_plus_keywords']['metrics']['f1_score'],\n        results_comparison['average_embeddings']['metrics']['f1_score'],\n        0.818,\n        0.606\n    ],\n    'Precision': [\n        results_comparison['name_only']['metrics']['precision'],\n        results_comparison['name_plus_keywords']['metrics']['precision'],\n        results_comparison['average_embeddings']['metrics']['precision'],\n        0.719,\n        '-'\n    ],\n    'Recall': [\n        results_comparison['name_only']['metrics']['recall'],\n        results_comparison['name_plus_keywords']['metrics']['recall'],\n        results_comparison['average_embeddings']['metrics']['recall'],\n        0.948,\n        '-'\n    ]\n})\n\nprint(comparison_df.to_string(index=False))\n\n# Find best method\nbest_method = max(results_comparison.items(), key=lambda x: x[1]['metrics']['f1_score'])\nprint(f\"\\n{'='*80}\")\nprint(f\"🏆 BEST METHOD: {methods[best_method[0]]}\")\nprint(f\"   F1 Score: {best_method[1]['metrics']['f1_score']:.3f}\")\nprint(f\"{'='*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:24:03.451295Z","iopub.status.idle":"2025-11-11T15:24:03.451592Z","shell.execute_reply.started":"2025-11-11T15:24:03.451433Z","shell.execute_reply":"2025-11-11T15:24:03.451446Z"}},"outputs":[],"execution_count":null}]}