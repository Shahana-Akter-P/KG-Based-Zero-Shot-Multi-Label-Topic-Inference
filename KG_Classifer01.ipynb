{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13468504,"sourceType":"datasetVersion","datasetId":8549797},{"sourceId":13524188,"sourceType":"datasetVersion","datasetId":8587369}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:08:09.637786Z","iopub.execute_input":"2025-11-06T00:08:09.637939Z","iopub.status.idle":"2025-11-06T00:08:11.398327Z","shell.execute_reply.started":"2025-11-06T00:08:09.637925Z","shell.execute_reply":"2025-11-06T00:08:11.397686Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/keywords/Keyword_Medical.json\n/kaggle/input/keywords/Keyword_Canon.json\n/kaggle/input/keywords/Keyword_Creative.json\n/kaggle/input/keywords/Keyword_Apex.json\n/kaggle/input/keywords/Keyword_Apex 1.json\n/kaggle/input/keywords/Keyword_Nokia.json\n/kaggle/input/keywords/Keyword_News.json\n/kaggle/input/keywords/Keyword_Nikon.json\n/kaggle/input/zstikg/NewsConcept Data-set.json\n/kaggle/input/zstikg/DVD playerData-set.json\n/kaggle/input/zstikg/MedicalConcept Data-set.json\n/kaggle/input/zstikg/Cellular phone Data-set.json\n/kaggle/input/zstikg/Digital camera2 Data-set.json\n/kaggle/input/zstikg/Mp3 playerData-set.json\n/kaggle/input/zstikg/Digital camera1 Data-set.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================================\n# PART 1: INSTALLATION AND SETUP\n# ============================================================================\n\n# Install required packages\n!pip install dspy-ai huggingface_hub networkx sentence-transformers pandas --quiet\n\nprint(\"✓ Packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:08:11.399840Z","iopub.execute_input":"2025-11-06T00:08:11.400096Z","iopub.status.idle":"2025-11-06T00:09:44.828677Z","shell.execute_reply.started":"2025-11-06T00:08:11.400082Z","shell.execute_reply":"2025-11-06T00:09:44.827318Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✓ Packages installed successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# PART 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom collections import defaultdict\nimport pandas as pd\nimport os\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:09:44.830547Z","iopub.execute_input":"2025-11-06T00:09:44.831516Z","iopub.status.idle":"2025-11-06T00:10:18.864430Z","shell.execute_reply.started":"2025-11-06T00:09:44.831468Z","shell.execute_reply":"2025-11-06T00:10:18.863428Z"}},"outputs":[{"name":"stderr","text":"2025-11-06 00:10:04.631186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762387804.833973      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762387804.889140      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: INSTALL AND IMPORT (if not done already)\n# ============================================================================\n\nimport os\nfrom huggingface_hub import InferenceClient\nimport dspy\nimport json\nfrom typing import List, Tuple, Dict\n\nprint(\"✓ Imports complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:10:18.865425Z","iopub.execute_input":"2025-11-06T00:10:18.866304Z","iopub.status.idle":"2025-11-06T00:10:18.940851Z","shell.execute_reply.started":"2025-11-06T00:10:18.866282Z","shell.execute_reply":"2025-11-06T00:10:18.939952Z"}},"outputs":[{"name":"stdout","text":"✓ Imports complete\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: CONFIGURE HUGGING FACE WITH DSPY.LM (PROPER WAY)\n# ============================================================================\nimport os\nimport dspy\n\nos.environ['HUGGINGFACE_API_KEY'] = 'KEY'\n\n# Use DSPy's built-in LM class with the correct prefix\nlm = dspy.LM(\n    model='huggingface/meta-llama/Llama-3.1-8B-Instruct',\n    api_key=os.environ['HUGGINGFACE_API_KEY'],\n    max_tokens=12000,\n    temperature=0.3\n)\n\ndspy.settings.configure(lm=lm)\nprint(\"✓ Llama-3.1-8B-Instruct configured successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:24:09.668134Z","iopub.execute_input":"2025-11-06T00:24:09.668757Z","iopub.status.idle":"2025-11-06T00:24:09.673741Z","shell.execute_reply.started":"2025-11-06T00:24:09.668741Z","shell.execute_reply":"2025-11-06T00:24:09.672976Z"}},"outputs":[{"name":"stdout","text":"✓ Llama-3.1-8B-Instruct configured successfully!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ============================================================================\n# PART 1: DATA LOADING FUNCTION\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"\n    Load JSON data from file\n    \n    Expected format:\n    [\n        {\n            \"Article Title\": [],\n            \"Article Text\": \"text here...\",\n            \"Concept\": [\"concept1\", \"concept2\"]\n        },\n        ...\n    ]\n    \"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\n# Test with sample data\nsample_data = [\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"excellent phone, excellent service.\",\n        \"Concept\": []\n    },\n    {\n        \"Article Title\": [],\n        \"Article Text\": \"i am a business user who heavily depend on mobile service.\",\n        \"Concept\": [\"service\"]\n    }\n]\n\nprint(\"✓ Sample data ready for testing\")\nprint(f\"  Sample has {len(sample_data)} documents\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:10:18.948856Z","iopub.execute_input":"2025-11-06T00:10:18.949124Z","iopub.status.idle":"2025-11-06T00:10:18.966903Z","shell.execute_reply.started":"2025-11-06T00:10:18.949106Z","shell.execute_reply":"2025-11-06T00:10:18.965865Z"}},"outputs":[{"name":"stdout","text":"✓ Sample data ready for testing\n  Sample has 2 documents\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# LOAD ONE SPECIFIC DATASET\n# ============================================================================\n\n# Choose which dataset you want to work with:\n# Option 1: Cellular phone\nfilepath = '/kaggle/input/zstikg/Cellular phone Data-set.json'\n\n# Option 2: News\n# filepath = '/kaggle/input/zsltikg/NewsConcept Data-set.json'\n\n# Option 3: Medical\n# filepath = '/kaggle/input/zsltikg/MedicalConcept Data-set.json'\n\n# Load the data\ndata = load_json_data(filepath)\n\nprint(f\"✓ Loaded {len(data)} documents\")\nprint(f\"\\nFirst document preview:\")\nprint(f\"  Keys: {list(data[0].keys())}\")\nprint(f\"  Text: {data[0].get('Article Text', '')}...\")\nprint(f\"  Concepts: {data[0].get('Concept', [])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:10:18.969756Z","iopub.execute_input":"2025-11-06T00:10:18.970036Z","iopub.status.idle":"2025-11-06T00:10:19.005244Z","shell.execute_reply.started":"2025-11-06T00:10:18.970019Z","shell.execute_reply":"2025-11-06T00:10:19.004448Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 587 documents from /kaggle/input/zstikg/Cellular phone Data-set.json\n✓ Loaded 587 documents\n\nFirst document preview:\n  Keys: ['Article Title', 'Article Text', 'Concept']\n  Text: excellent phone , excellent service . \n...\n  Concepts: []\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# PROCESS YOUR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"PROCESSING ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\n# data is now correctly loaded as a list of dicts\nindividual_articles = []\n\nfor idx, item in enumerate(data):\n    article = {\n        'id': idx,\n        'Article Text': item['Article Text'],\n        #'Concept': item['Concept'] if item['Concept'] else []\n    }\n    individual_articles.append(article)\n\nprint(f\"\\n✓ Created list of {len(individual_articles)} individual articles\")\n\n# Show first 3\nprint(f\"\\nFirst 3 articles:\")\nprint(\"-\" * 80)\nfor i in range(min(3, len(individual_articles))):\n    article = individual_articles[i]\n    print(f\"\\nArticle {article['id']}:\")\n    print(f\"  Text: {article['Article Text'][:80]}...\")\n    #print(f\"  Concepts: {article['Concept']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:10:19.005970Z","iopub.execute_input":"2025-11-06T00:10:19.006162Z","iopub.status.idle":"2025-11-06T00:10:19.012727Z","shell.execute_reply.started":"2025-11-06T00:10:19.006145Z","shell.execute_reply":"2025-11-06T00:10:19.011913Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPROCESSING ALL THE ARTICLES\n================================================================================\n\n✓ Created list of 587 individual articles\n\nFirst 3 articles:\n--------------------------------------------------------------------------------\n\nArticle 0:\n  Text: excellent phone , excellent service . \n...\n\nArticle 1:\n  Text: i am a business user who heavily depend on mobile service . \n...\n\nArticle 2:\n  Text: there is much which has been said in other reviews about the features of this ph...\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: DEFINE SIGNATURES\n# ============================================================================\n\nclass EntityExtractor(dspy.Signature):\n    \"\"\"Extract key entities from the given text. Extracted entities are nouns, \n    verbs, or adjectives, particularly regarding sentiment. This is for an \n    extraction task, please be thorough and accurate to the reference text.\n    \n    Return ONLY a valid JSON list format: [\"entity1\", \"entity2\", \"entity3\"]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract entities from\")\n    entities = dspy.OutputField(desc=\"List of extracted entities in JSON format\")\n\nclass RelationExtractor(dspy.Signature):\n    \"\"\"Extract subject-predicate-object triples from the assistant message. \n    A predicate (1-3 words) defines the relationship between the subject and \n    object. Relationship may be fact or sentiment based on assistant's message. \n    Subject and object are entities. Entities provided are from the assistant \n    message and prior conversation history, though you may not need all of them. \n    This is for an extraction task, please be thorough, accurate, and faithful \n    to the reference text.\n    \n    Return ONLY valid JSON format: [[\"subject1\", \"predicate1\", \"object1\"], [\"subject2\", \"predicate2\", \"object2\"]]\n    \"\"\"\n    \n    text = dspy.InputField(desc=\"The text to extract relations from\")\n    entities = dspy.InputField(desc=\"List of available entities\")\n    triples = dspy.OutputField(desc=\"List of [subject, predicate, object] triples in JSON format\")\n\nprint(\"✓ Signatures defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:10:19.013455Z","iopub.execute_input":"2025-11-06T00:10:19.014382Z","iopub.status.idle":"2025-11-06T00:10:19.034658Z","shell.execute_reply.started":"2025-11-06T00:10:19.014355Z","shell.execute_reply":"2025-11-06T00:10:19.033859Z"}},"outputs":[{"name":"stdout","text":"✓ Signatures defined\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: CREATE ENTITY EXTRACTOR\n# ============================================================================\n\nclass ExtractEntities(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(EntityExtractor)\n    \n    def forward(self, text: str) -> List[str]:\n        if not text or len(text.strip()) < 3:\n            return []\n            \n        result = self.extract(text=text)\n        \n        try:\n            entities_text = result.entities.strip()\n            \n            if '[' in entities_text and ']' in entities_text:\n                start = entities_text.find('[')\n                end = entities_text.rfind(']') + 1\n                entities_text = entities_text[start:end]\n            \n            entities = json.loads(entities_text)\n            \n            if isinstance(entities, list):\n                return [str(e).lower().strip() for e in entities if e and len(str(e).strip()) > 1]\n            return []\n            \n        except:\n            try:\n                entities_text = result.entities.strip()\n                if entities_text.startswith('['):\n                    entities_text = entities_text[1:]\n                if entities_text.endswith(']'):\n                    entities_text = entities_text[:-1]\n                \n                entities = []\n                for item in entities_text.split(','):\n                    item = item.strip(' \"\\'\\n\\t')\n                    if item and len(item) > 1:\n                        entities.append(item.lower())\n                \n                return entities[:50]\n            except:\n                return []\n\nentity_extractor = ExtractEntities()\nprint(\"✓ Entity Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:10:19.035496Z","iopub.execute_input":"2025-11-06T00:10:19.035752Z","iopub.status.idle":"2025-11-06T00:10:19.057251Z","shell.execute_reply.started":"2025-11-06T00:10:19.035733Z","shell.execute_reply":"2025-11-06T00:10:19.056481Z"}},"outputs":[{"name":"stdout","text":"✓ Entity Extractor created\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: CREATE RELATION EXTRACTOR\n# ============================================================================\n\nclass ExtractRelations(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extract = dspy.ChainOfThought(RelationExtractor)\n    \n    def forward(self, text: str, entities: List[str]) -> List[Tuple[str, str, str]]:\n        if not entities or not text:\n            return []\n        \n        entities_subset = entities[:30]\n        entities_str = json.dumps(entities_subset)\n        \n        result = self.extract(text=text, entities=entities_str)\n        \n        try:\n            triples_text = result.triples.strip()\n            \n            if '[' in triples_text and ']' in triples_text:\n                start = triples_text.find('[')\n                end = triples_text.rfind(']') + 1\n                triples_text = triples_text[start:end]\n            \n            triples = json.loads(triples_text)\n            \n            normalized_triples = []\n            for triple in triples:\n                if isinstance(triple, (list, tuple)) and len(triple) == 3:\n                    s, p, o = triple\n                    s = str(s).lower().strip()\n                    p = str(p).lower().strip()\n                    o = str(o).lower().strip()\n                    \n                    if s and p and o and s != o:\n                        normalized_triples.append((s, p, o))\n            \n            return normalized_triples\n            \n        except Exception as e:\n            return []\n\nrelation_extractor = ExtractRelations()\nprint(\"✓ Relation Extractor created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:10:19.058115Z","iopub.execute_input":"2025-11-06T00:10:19.058320Z","iopub.status.idle":"2025-11-06T00:10:19.082919Z","shell.execute_reply.started":"2025-11-06T00:10:19.058306Z","shell.execute_reply":"2025-11-06T00:10:19.082163Z"}},"outputs":[{"name":"stdout","text":"✓ Relation Extractor created\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# CLUSTERING SIGNATURES (DEFINE FIRST!)\n# ============================================================================\n\nclass ClusterValidator(dspy.Signature):\n    \"\"\"Verify if these entities belong in the same cluster.\n    A cluster should contain entities that are the same in meaning, with different:\n    - tenses, plural forms, stem forms, upper/lower cases\n    Or entities with close semantic meanings.\n    \n    Return ONLY valid JSON format: [\"entity1\", \"entity2\", \"entity3\"]\n    Return only entities you are confident belong together.\n    If not confident, return empty list [].\n    \"\"\"\n    \n    entities = dspy.InputField(desc=\"Entities to validate\")\n    valid_cluster = dspy.OutputField(desc=\"Validated cluster in JSON format\")\n\nprint(\"✓ ClusterValidator Signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:10:19.083795Z","iopub.execute_input":"2025-11-06T00:10:19.084079Z","iopub.status.idle":"2025-11-06T00:10:19.104542Z","shell.execute_reply.started":"2025-11-06T00:10:19.084060Z","shell.execute_reply":"2025-11-06T00:10:19.103647Z"}},"outputs":[{"name":"stdout","text":"✓ ClusterValidator Signature defined\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================================\n# SEMANTIC SIMILARITY CLUSTERING (FROM PAPER)\n# ============================================================================\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nclass SemanticEntityClustering(dspy.Module):\n    def __init__(self, similarity_threshold=0.75):\n        super().__init__()\n        self.validator = dspy.ChainOfThought(ClusterValidator)\n        \n        # Load embedding model (same as paper)\n        print(\"Loading sentence transformer model...\")\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        print(\"✓ Model loaded\")\n        \n        self.similarity_threshold = similarity_threshold\n    \n    def _parse_cluster(self, text: str) -> List[str]:\n        \"\"\"Parse cluster from LLM response\"\"\"\n        try:\n            text = text.strip()\n            if '[' in text and ']' in text:\n                start = text.find('[')\n                end = text.rfind(']') + 1\n                text = text[start:end]\n            \n            cluster = json.loads(text)\n            if isinstance(cluster, list):\n                return [str(e).lower().strip() for e in cluster if e]\n            return []\n        except:\n            return []\n    \n    def _get_semantic_clusters(self, entities: List[str]) -> List[List[str]]:\n        \"\"\"Group entities by semantic similarity using embeddings\"\"\"\n        \n        if len(entities) == 0:\n            return []\n        \n        # Get embeddings for all entities\n        embeddings = self.model.encode(entities)\n        \n        # Compute pairwise cosine similarity\n        similarity_matrix = cosine_similarity(embeddings)\n        \n        # Find clusters using similarity threshold\n        clusters = []\n        remaining = set(range(len(entities)))\n        \n        for i in range(len(entities)):\n            if i not in remaining:\n                continue\n            \n            # Find all entities similar to this one\n            cluster_indices = [i]\n            remaining.discard(i)\n            \n            for j in range(i + 1, len(entities)):\n                if j not in remaining:\n                    continue\n                \n                # Check if similar enough\n                if similarity_matrix[i][j] >= self.similarity_threshold:\n                    cluster_indices.append(j)\n                    remaining.discard(j)\n            \n            # Convert indices to entity names\n            cluster = [entities[idx] for idx in cluster_indices]\n            \n            # Only keep clusters with 2-4 entities\n            if 2 <= len(cluster) <= 4:\n                clusters.append(cluster)\n            elif len(cluster) == 1:\n                # Keep singletons for later\n                pass\n        \n        return clusters\n    \n    def forward(self, entities: List[str]) -> Dict[str, List[str]]:\n        \"\"\"Semantic clustering with LLM validation\"\"\"\n        \n        print(f\"Starting semantic clustering with {len(entities)} entities...\")\n        print(f\"  Similarity threshold: {self.similarity_threshold}\")\n        \n        # Remove duplicates\n        unique_entities = list(set(entities))\n        \n        # Step 1: Find semantic clusters using embeddings\n        print(\"  Computing semantic similarities...\")\n        potential_clusters = self._get_semantic_clusters(unique_entities)\n        \n        print(f\"  Found {len(potential_clusters)} potential clusters\")\n        \n        # Step 2: Validate with LLM\n        validated_clusters = {}\n        cluster_id = 0\n        clustered_entities = set()\n        \n        for cluster in potential_clusters:\n            try:\n                # Ask LLM to validate\n                validation = self.validator(entities=json.dumps(cluster))\n                validated = self._parse_cluster(validation.valid_cluster)\n                \n                if validated and len(validated) >= 2:\n                    cluster_label = validated[0]\n                    validated_clusters[cluster_label] = validated\n                    \n                    for entity in validated:\n                        clustered_entities.add(entity)\n                    \n                    print(f\"  ✓ Cluster {cluster_id}: {validated}\")\n                    cluster_id += 1\n                else:\n                    # LLM rejected - add as singletons\n                    for entity in cluster:\n                        if entity not in clustered_entities:\n                            validated_clusters[entity] = [entity]\n                            clustered_entities.add(entity)\n            except:\n                # Error - add as singletons\n                for entity in cluster:\n                    if entity not in clustered_entities:\n                        validated_clusters[entity] = [entity]\n                        clustered_entities.add(entity)\n        \n        # Step 3: Add all remaining entities as singletons\n        for entity in unique_entities:\n            if entity not in clustered_entities:\n                validated_clusters[entity] = [entity]\n        \n        multi = sum(1 for v in validated_clusters.values() if len(v) > 1)\n        print(f\"✓ Semantic clustering complete: {len(validated_clusters)} total clusters\")\n        print(f\"  Multi-entity clusters: {multi}\")\n        print(f\"  Singleton entities: {len(validated_clusters) - multi}\")\n        \n        return validated_clusters\n\n# Create semantic clusterer with different thresholds\nentity_clusterer_semantic = SemanticEntityClustering(similarity_threshold=0.75)\nprint(\"\\n✓ Semantic Entity Clustering Module created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:10:19.105556Z","iopub.execute_input":"2025-11-06T00:10:19.106091Z","iopub.status.idle":"2025-11-06T00:10:25.733319Z","shell.execute_reply.started":"2025-11-06T00:10:19.106073Z","shell.execute_reply":"2025-11-06T00:10:25.732601Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06d8674f05b4d0c9f15d51fad3fec50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fb181d73a0248319b4f30dba2c88ca0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f78cec0f82e24734906aa8116490e6d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eba647b3994b4ff49ab41a4ab1e5c1c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7a91cc313674ad1aac647015cc9fdc5"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f024f3229d04656aa8427b3678face8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b1d595e23004e38b0c9bcdd930de1d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7ab4109a7164475b8995b584bf0d2af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"710d7de17b3342a19c5b45f4ed9dd1ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8584b7cfeb6b409e96608bbcc3595c8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2539a6cb8c2f416f9033f574d75bb105"}},"metadata":{}},{"name":"stdout","text":"✓ Model loaded\n\n✓ Semantic Entity Clustering Module created\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================================\n# FINAL KGGEN WITH SEMANTIC CLUSTERING\n# ============================================================================\n\nclass KGGenSemantic:\n    def __init__(self, similarity_threshold=0.75):\n        self.entity_extractor = entity_extractor\n        self.relation_extractor = relation_extractor\n        self.entity_clusterer = SemanticEntityClustering(similarity_threshold=similarity_threshold)\n        self.graph = nx.DiGraph()\n        self.entity_clusters = {}\n        \n    def generate_from_json(self, json_data: List[Dict], max_docs: int = None) -> nx.DiGraph:\n        \"\"\"Generate KG from JSON dataset\"\"\"\n        all_entities = set()\n        all_relations = []\n        \n        if max_docs:\n            json_data = json_data[:max_docs]\n        \n        print(f\"Processing {len(json_data)} documents...\")\n        print(\"=\" * 80)\n        \n        for idx, item in enumerate(json_data):\n            text = item.get('Article Text', '')\n            concepts = item.get('Concept', [])\n            \n            if not text or len(text.strip()) < 5:\n                continue\n            \n            try:\n                # Extract entities\n                entities = self.entity_extractor(text)\n                all_entities.update(entities)\n                \n                # Add concepts\n                #for concept in concepts:\n                    #if concept and isinstance(concept, str):\n                        #all_entities.add(concept.lower().strip())\n                \n                # Extract relations\n                relations = self.relation_extractor(text, list(all_entities))\n                all_relations.extend(relations)\n                \n                if (idx + 1) % 20 == 0:\n                    print(f\"  {idx + 1}/{len(json_data)} docs | {len(all_entities)} entities | {len(all_relations)} relations\")\n                    \n            except Exception as e:\n                continue\n        \n        print(f\"\\n✓ Extraction complete!\")\n        print(f\"  Total entities: {len(all_entities)}\")\n        print(f\"  Total relations: {len(all_relations)}\")\n        \n        # Build graph\n        for subj, pred, obj in all_relations:\n            self.graph.add_edge(subj, obj, relation=pred)\n        \n        print(f\"  Graph nodes: {len(self.graph.nodes())}\")\n        print(f\"  Graph edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def cluster_entities(self):\n        \"\"\"Semantic clustering with embeddings\"\"\"\n        nodes = list(self.graph.nodes())\n        \n        if len(nodes) == 0:\n            print(\"No nodes to cluster!\")\n            return self.graph\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"SEMANTIC CLUSTERING: {len(nodes)} ENTITIES\")\n        print(f\"{'='*80}\")\n        \n        self.entity_clusters = self.entity_clusterer(nodes)\n        \n        # Map entities\n        entity_mapping = {}\n        for cluster_label, cluster_entities in self.entity_clusters.items():\n            for entity in cluster_entities:\n                entity_mapping[entity] = cluster_label\n        \n        # Rebuild graph\n        new_graph = nx.DiGraph()\n        for u, v, data in self.graph.edges(data=True):\n            new_u = entity_mapping.get(u, u)\n            new_v = entity_mapping.get(v, v)\n            relation = data.get('relation', 'related_to')\n            \n            if new_u == new_v:\n                continue\n            \n            if not new_graph.has_edge(new_u, new_v):\n                new_graph.add_edge(new_u, new_v, relation=relation)\n        \n        self.graph = new_graph\n        \n        print(f\"\\n✓ Clustering complete!\")\n        print(f\"  Final nodes: {len(self.graph.nodes())}\")\n        print(f\"  Final edges: {len(self.graph.edges())}\")\n        \n        return self.graph\n    \n    def save_graph(self, filepath: str):\n        data = nx.node_link_data(self.graph)\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n        print(f\"✓ Saved to {filepath}\")\n    \n    def export_triples(self, filepath: str):\n        triples = []\n        for u, v, data in self.graph.edges(data=True):\n            triples.append({\n                'subject': u,\n                'predicate': data.get('relation', 'related_to'),\n                'object': v\n            })\n        import pandas as pd\n        df = pd.DataFrame(triples)\n        df.to_csv(filepath, index=False)\n        print(f\"✓ Exported to {filepath}\")\n\n# Initialize semantic KGGen (threshold 0.75 = balanced)\nkg_gen_semantic = KGGenSemantic(similarity_threshold=0.75)\nprint(\"\\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:10:25.734205Z","iopub.execute_input":"2025-11-06T00:10:25.734424Z","iopub.status.idle":"2025-11-06T00:10:26.424128Z","shell.execute_reply.started":"2025-11-06T00:10:25.734410Z","shell.execute_reply":"2025-11-06T00:10:26.423446Z"}},"outputs":[{"name":"stdout","text":"Loading sentence transformer model...\n✓ Model loaded\n\n✓ KGGen with Semantic Clustering (Embeddings + LLM) created!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================================================\n# GENERATE KG FOR ALL THE ARTICLES\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\")\nprint(\"=\" * 80)\n\nall_article_kgs = []\n\ntotal = len(individual_articles)\nprint(f\"\\nProcessing {total} articles...\\n\")\n\nfor idx, article in enumerate(individual_articles):\n    article_id = article['id']\n    text = article['Article Text']\n    #concepts = article['Concept']\n    \n    # Skip empty articles\n    if not text or len(text.strip()) < 5:\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n        continue\n    \n    try:\n        # Extract entities for THIS article\n        entities = entity_extractor(text)\n        \n        # Add original concepts as entities\n        #for concept in concepts:\n            #if concept and isinstance(concept, str):\n                #entities.append(concept.lower().strip())\n        \n        entities = list(set(entities))  # Remove duplicates\n        \n        # Extract relations for THIS article\n        if entities:\n            relations = relation_extractor(text, entities)\n        else:\n            relations = []\n        \n        # Build graph for THIS article\n        graph = nx.DiGraph()\n        for subj, pred, obj in relations:\n            graph.add_edge(subj, obj, relation=pred)\n        \n        # Store everything for this article\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': graph,\n            'entities': entities,\n            'relations': relations,\n            'num_nodes': len(graph.nodes()),\n            'num_edges': len(graph.edges())\n        })\n        \n        # Progress update\n        if (idx + 1) % 50 == 0:\n            print(f\"✓ Processed {idx + 1}/{total} articles...\")\n        \n    except Exception as e:\n        print(f\"✗ Article {article_id}: Error - {str(e)}\")\n        all_article_kgs.append({\n            'id': article_id,\n            'text': text,\n            #'concepts': concepts,\n            'graph': nx.DiGraph(),\n            'entities': [],\n            'relations': [],\n            'num_nodes': 0,\n            'num_edges': 0\n        })\n\nprint(f\"\\n\" + \"=\" * 80)\nprint(\"✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\")\nprint(\"=\" * 80)\nprint(f\"Total articles processed: {len(all_article_kgs)}\")\nprint(f\"Articles with graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] > 0)}\")\nprint(f\"Articles without graphs: {sum(1 for kg in all_article_kgs if kg['num_edges'] == 0)}\")\n\n# Statistics\ntotal_entities = sum(len(kg['entities']) for kg in all_article_kgs)\ntotal_relations = sum(len(kg['relations']) for kg in all_article_kgs)\n\nprint(f\"\\nTotal entities extracted: {total_entities}\")\nprint(f\"Total relations extracted: {total_relations}\")\n\nwith_graphs = [kg for kg in all_article_kgs if kg['num_edges'] > 0]\nif with_graphs:\n    avg_nodes = sum(kg['num_nodes'] for kg in with_graphs) / len(with_graphs)\n    avg_edges = sum(kg['num_edges'] for kg in with_graphs) / len(with_graphs)\n    #print(f\"\\nAverage nodes per KG: {avg_nodes:.2f}\")\n    #print(f\"Average edges per KG: {avg_edges:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:24:20.451234Z","iopub.execute_input":"2025-11-06T00:24:20.452075Z","iopub.status.idle":"2025-11-06T01:10:12.837550Z","shell.execute_reply.started":"2025-11-06T00:24:20.452053Z","shell.execute_reply":"2025-11-06T01:10:12.836628Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nGENERATING KNOWLEDGE GRAPHS FOR ALL THE ARTICLES\n================================================================================\n\nProcessing 587 articles...\n\n✓ Processed 50/587 articles...\n✓ Processed 100/587 articles...\n✓ Processed 150/587 articles...\n✓ Processed 200/587 articles...\n✓ Processed 250/587 articles...\n✓ Processed 300/587 articles...\n✓ Processed 350/587 articles...\n✓ Processed 400/587 articles...\n✓ Processed 450/587 articles...\n✓ Processed 500/587 articles...\n✓ Processed 550/587 articles...\n\n================================================================================\n✓ KNOWLEDGE GRAPH GENERATION COMPLETE!\n================================================================================\nTotal articles processed: 587\nArticles with graphs: 533\nArticles without graphs: 54\n\nTotal entities extracted: 1845\nTotal relations extracted: 1476\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: IMPORTS\n# ============================================================================\n\nimport json\nimport networkx as nx\nfrom typing import List, Dict, Set, Tuple\nimport dspy\nfrom dspy.signatures import Signature\nfrom dspy import OutputField, InputField\nimport pandas as pd\nimport os\nfrom collections import Counter\nimport numpy as np\n\nprint(\"✓ All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:11:38.763083Z","iopub.execute_input":"2025-11-06T01:11:38.763397Z","iopub.status.idle":"2025-11-06T01:11:38.768590Z","shell.execute_reply.started":"2025-11-06T01:11:38.763378Z","shell.execute_reply":"2025-11-06T01:11:38.767950Z"}},"outputs":[{"name":"stdout","text":"✓ All imports successful!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ============================================================================\n# SAVE KNOWLEDGE GRAPHS TO JSON FILE\n# ============================================================================\n\nimport json\nimport networkx as nx\n\ndef save_knowledge_graphs(all_article_kgs, filepath='/kaggle/working/article_knowledge_graphs.json'):\n    \"\"\"\n    Save knowledge graphs to JSON file.\n    \n    Args:\n        all_article_kgs: List of dictionaries containing KG information\n        filepath: Path to save the JSON file\n    \"\"\"\n    # Convert NetworkX graphs to serializable format\n    serializable_kgs = []\n    \n    for kg_data in all_article_kgs:\n        # Convert NetworkX graph to node-link format\n        graph = kg_data['graph']\n        graph_dict = nx.node_link_data(graph) if graph.number_of_nodes() > 0 else None\n        \n        serializable_kg = {\n            'id': kg_data['id'],\n            'text': kg_data['text'],\n            'entities': kg_data['entities'],\n            'relations': kg_data['relations'],\n            'num_nodes': kg_data['num_nodes'],\n            'num_edges': kg_data['num_edges'],\n            'graph_data': graph_dict  # Serialized graph\n        }\n        \n        serializable_kgs.append(serializable_kg)\n    \n    # Save to JSON\n    with open(filepath, 'w', encoding='utf-8') as f:\n        json.dump(serializable_kgs, f, indent=2, ensure_ascii=False)\n    \n    print(f\"\\n{'='*80}\")\n    print(\"KNOWLEDGE GRAPHS SAVED\")\n    print(f\"{'='*80}\")\n    print(f\"Saved {len(serializable_kgs)} knowledge graphs to:\")\n    print(f\"  {filepath}\")\n    print(f\"\\nFile size: {len(json.dumps(serializable_kgs)) / (1024*1024):.2f} MB\")\n    print(f\"Articles with graphs: {sum(1 for kg in serializable_kgs if kg['num_edges'] > 0)}\")\n    print(f\"Articles without graphs: {sum(1 for kg in serializable_kgs if kg['num_edges'] == 0)}\")\n\n# Save the knowledge graphs\nsave_knowledge_graphs(all_article_kgs, '/kaggle/working/article_knowledge_graphs.json')\n\nprint(\"\\n✓ Knowledge graphs saved successfully!\")\nprint(\"You can now proceed to the topic classification step.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:17:36.654449Z","iopub.execute_input":"2025-11-06T01:17:36.654749Z","iopub.status.idle":"2025-11-06T01:17:36.715710Z","shell.execute_reply.started":"2025-11-06T01:17:36.654733Z","shell.execute_reply":"2025-11-06T01:17:36.714901Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nKNOWLEDGE GRAPHS SAVED\n================================================================================\nSaved 587 knowledge graphs to:\n  /kaggle/working/article_knowledge_graphs.json\n\nFile size: 0.38 MB\nArticles with graphs: 533\nArticles without graphs: 54\n\n✓ Knowledge graphs saved successfully!\nYou can now proceed to the topic classification step.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/networkx/readwrite/json_graph/node_link.py:145: FutureWarning: \nThe default value will be `edges=\"edges\" in NetworkX 3.6.\n\nTo make this warning go away, explicitly set the edges kwarg, e.g.:\n\n  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: LOAD DATA AND EXISTING KNOWLEDGE GRAPHS\n# ============================================================================\n\ndef load_json_data(filepath: str) -> List[Dict]:\n    \"\"\"Load JSON data from file\"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    print(f\"✓ Loaded {len(data)} documents from {filepath}\")\n    return data\n\ndef load_knowledge_graphs(filepath: str) -> List[Dict]:\n    \"\"\"Load previously created knowledge graphs\"\"\"\n    if not os.path.exists(filepath):\n        print(f\"\\n⚠️  WARNING: Knowledge graph file not found at {filepath}\")\n        return None\n    \n    with open(filepath, 'r', encoding='utf-8') as f:\n        kgs = json.load(f)\n    print(f\"✓ Loaded {len(kgs)} knowledge graphs from {filepath}\")\n    return kgs\n\n# Load your data\narticles = load_json_data('/kaggle/input/zstikg/Cellular phone Data-set.json')\nknowledge_graphs = load_knowledge_graphs('/kaggle/working/article_knowledge_graphs.json')\n\n# Only classify articles that have KGs\narticles_with_kgs = articles[:len(knowledge_graphs)] \n\nif knowledge_graphs is None:\n    print(\"\\n❌ Cannot proceed without knowledge graphs. Please save them first.\")\nelse:\n    print(f\"\\n✓ Successfully loaded {len(articles)} articles and {len(knowledge_graphs)} KGs\")\n    print(\"✓ Ready to proceed with topic classification!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:17:44.275371Z","iopub.execute_input":"2025-11-06T01:17:44.275661Z","iopub.status.idle":"2025-11-06T01:17:44.293836Z","shell.execute_reply.started":"2025-11-06T01:17:44.275620Z","shell.execute_reply":"2025-11-06T01:17:44.293048Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 587 documents from /kaggle/input/zstikg/Cellular phone Data-set.json\n✓ Loaded 587 knowledge graphs from /kaggle/working/article_knowledge_graphs.json\n\n✓ Successfully loaded 587 articles and 587 KGs\n✓ Ready to proceed with topic classification!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ============================================================================\n# STEP 4: EXTRACT ALL UNIQUE CONCEPTS FROM DATASET\n# ============================================================================\n\ndef extract_unique_concepts(articles: List[Dict]) -> List[str]:\n    \"\"\"\n    Extract all unique concepts from the dataset's 'Concept' field.\n    \n    Args:\n        articles: List of article dictionaries\n    \n    Returns:\n        Sorted list of unique concepts\n    \"\"\"\n    all_concepts = set()\n    \n    for article in articles:\n        # Get concepts from the Concept field\n        if 'Concept' in article and article['Concept']:\n            if isinstance(article['Concept'], list):\n                all_concepts.update([c.lower().strip() for c in article['Concept'] if c])\n            elif isinstance(article['Concept'], str):\n                all_concepts.add(article['Concept'].lower().strip())\n    \n    # Convert to sorted list\n    unique_concepts = sorted(list(all_concepts))\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"UNIQUE CONCEPTS EXTRACTED FROM DATASET\")\n    print(f\"{'='*80}\")\n    print(f\"Total unique concepts: {len(unique_concepts)}\")\n    print(f\"\\nFirst 20 concepts: {unique_concepts[:20]}\")\n    print(f\"\\nLast 20 concepts: {unique_concepts[-20:]}\")\n    \n    return unique_concepts\n\n\n# Extract all unique concepts from the dataset\nall_concepts = extract_unique_concepts(articles)\n\n# Save concept list for reference\nwith open('/kaggle/working/all_concepts.json', 'w') as f:\n    json.dump(all_concepts, f, indent=2)\n\nprint(f\"\\n✓ Saved concept list to /kaggle/working/all_concepts.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:17:52.124124Z","iopub.execute_input":"2025-11-06T01:17:52.124405Z","iopub.status.idle":"2025-11-06T01:17:52.132844Z","shell.execute_reply.started":"2025-11-06T01:17:52.124388Z","shell.execute_reply":"2025-11-06T01:17:52.132053Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nUNIQUE CONCEPTS EXTRACTED FROM DATASET\n================================================================================\nTotal unique concepts: 24\n\nFirst 20 concepts: ['***', 'backlight', 'battery', 'button', 'camera', 'design', 'feature', 'game', 'interface', 'internet', 'message', 'network', 'phone', 'price', 'radio', 'ringtones', 'screen', 'service', 'size', 'software']\n\nLast 20 concepts: ['camera', 'design', 'feature', 'game', 'interface', 'internet', 'message', 'network', 'phone', 'price', 'radio', 'ringtones', 'screen', 'service', 'size', 'software', 'sound', 'tmobile', 'vibration', 'weight']\n\n✓ Saved concept list to /kaggle/working/all_concepts.json\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: KG FORMATTING FUNCTION\n# ============================================================================\n\ndef format_kg_for_llm(kg_data: Dict) -> str:\n    \"\"\"Format knowledge graph into readable text for the LLM.\"\"\"\n    if not kg_data or kg_data.get('num_edges', 0) == 0:\n        return \"No knowledge graph available.\"\n    \n    # Format entities\n    entities = kg_data.get('entities', [])\n    entities_str = \", \".join(entities[:30]) if entities else \"None\"\n    if len(entities) > 30:\n        entities_str += f\"... ({len(entities) - 30} more)\"\n    \n    # Format relationships\n    relations = kg_data.get('relations', [])\n    if relations:\n        relationships = []\n        for relation in relations[:20]:\n            if isinstance(relation, (list, tuple)) and len(relation) >= 3:\n                source, rel_type, target = relation[0], relation[1], relation[2]\n                relationships.append(f\"{source} --[{rel_type}]--> {target}\")\n        relationships_str = \"\\n\".join(relationships)\n        if len(relations) > 20:\n            relationships_str += f\"\\n... ({len(relations) - 20} more relationships)\"\n    else:\n        relationships_str = \"None\"\n    \n    kg_summary = f\"\"\"Knowledge Graph Summary:\nEntities: {entities_str}\n\nKey Relationships:\n{relationships_str}\"\"\"\n    \n    return kg_summary\n\nprint(\"✓ KG formatting function ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:18:05.954735Z","iopub.execute_input":"2025-11-06T01:18:05.954983Z","iopub.status.idle":"2025-11-06T01:18:05.962884Z","shell.execute_reply.started":"2025-11-06T01:18:05.954968Z","shell.execute_reply":"2025-11-06T01:18:05.962159Z"}},"outputs":[{"name":"stdout","text":"✓ KG formatting function ready\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: DEFINE DSPY SIGNATURE FOR TOPIC CLASSIFICATION\n# ============================================================================\n\nclass TopicClassification(Signature):\n    \"\"\"Given an article text, its knowledge graph, and a list of possible topics,\n    determine which topics (can be multiple) are most relevant to this article.\n    Return only topic names that are actually present in the available_topics list.\"\"\"\n    \n    article_text: str = InputField(\n        desc=\"The text content of the article\"\n    )\n    knowledge_graph: str = InputField(\n        desc=\"Knowledge graph extracted from the article showing entities and relationships\"\n    )\n    available_topics: str = InputField(\n        desc=\"Comma-separated list of all possible topic/concept names\"\n    )\n    \n    predicted_topics: str = OutputField(\n        desc=\"Comma-separated list of relevant topics from the available_topics list. \"\n             \"Only return topics that actually appear in the available_topics list. \"\n             \"If multiple topics apply, list them all. If no topics match well, return 'none'.\"\n    )\n    #confidence: str = OutputField(\n        #desc=\"Confidence level: high, medium, or low\"\n    #)\n    #reasoning: str = OutputField(\n        #desc=\"Brief explanation of why these topics were chosen based on the article and knowledge graph\"\n    #)\n\nprint(\"✓ Topic classification signature defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:18:09.985168Z","iopub.execute_input":"2025-11-06T01:18:09.985424Z","iopub.status.idle":"2025-11-06T01:18:09.992271Z","shell.execute_reply.started":"2025-11-06T01:18:09.985410Z","shell.execute_reply":"2025-11-06T01:18:09.991574Z"}},"outputs":[{"name":"stdout","text":"✓ Topic classification signature defined\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ============================================================================\n# STEP 7: CREATE TOPIC CLASSIFIER CLASS\n# ============================================================================\n\nclass ArticleTopicClassifier:\n    \"\"\"Multi-label topic classifier using article text, KG, and available topics.\"\"\"\n    \n    def __init__(self, available_topics: List[str]):\n        self.available_topics = available_topics\n        self.available_topics_str = \", \".join(available_topics)\n        self.classifier = dspy.ChainOfThought(TopicClassification)\n        print(f\"✓ Classifier initialized with {len(available_topics)} possible topics\")\n    \n    def classify_article(self, article_text: str, kg_data: Dict) -> Dict:\n        \"\"\"Classify a single article.\"\"\"\n        kg_summary = format_kg_for_llm(kg_data)\n        \n        # Truncate article if too long\n        #max_text_length = 500\n        #if len(article_text) > max_text_length:\n            #article_text = article_text[:max_text_length] + \"...\"\n        \n        try:\n            result = self.classifier(\n                article_text=article_text,\n                knowledge_graph=kg_summary,\n                available_topics=self.available_topics_str\n            )\n            \n            predicted_topics_raw = result.predicted_topics\n            \n            if predicted_topics_raw.lower() == 'none':\n                predicted_topics = []\n            else:\n                predicted_topics = [\n                    t.strip().lower() \n                    for t in predicted_topics_raw.split(',')\n                    if t.strip()\n                ]\n                predicted_topics = [\n                    t for t in predicted_topics \n                    if t in [at.lower() for at in self.available_topics]\n                ]\n            \n            return {\n                'predicted_topics': predicted_topics,\n                'num_topics': len(predicted_topics),\n                #'confidence': result.confidence,\n                #'reasoning': result.reasoning\n            }\n            \n        except Exception as e:\n            print(f\"  Error: {str(e)}\")\n            return {\n                'predicted_topics': [],\n                'num_topics': 0,\n                #'confidence': 'error',\n                #'reasoning': f'Classification failed: {str(e)}'\n            }\n    \n    def classify_dataset(self, articles: List[Dict], knowledge_graphs: List[Dict],\n                        max_articles: int = None) -> List[Dict]:\n        \"\"\"Classify multiple articles.\"\"\"\n        results = []\n        kg_dict = {kg['id']: kg for kg in knowledge_graphs}\n        \n        if max_articles:\n            articles = articles[:max_articles]\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"CLASSIFYING {len(articles)} ARTICLES\")\n        print(f\"{'='*80}\\n\")\n        \n        for idx, article in enumerate(articles):\n            print(f\"Processing article {idx}...\", end=\" \")\n            \n            article_text = article.get('Article Text', '')\n            kg_data = kg_dict.get(idx, {'num_edges': 0})\n            classification = self.classify_article(article_text, kg_data)\n            \n            result = {\n                'article_id': idx,\n                'article_text': article_text + \"...\",\n                'true_concepts': article.get('Concept', []),\n                'predicted_topics': classification['predicted_topics'],\n                'num_predicted': classification['num_topics'],\n                #'confidence': classification['confidence'],\n                #'reasoning': classification['reasoning']\n            }\n            \n            results.append(result)\n            print(f\"✓ Found {len(classification['predicted_topics'])} topics\")\n        \n        return results\n\nprint(\"✓ Classifier class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:18:15.260020Z","iopub.execute_input":"2025-11-06T01:18:15.260699Z","iopub.status.idle":"2025-11-06T01:18:15.275664Z","shell.execute_reply.started":"2025-11-06T01:18:15.260605Z","shell.execute_reply":"2025-11-06T01:18:15.274880Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier class defined\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ============================================================================\n# STEP 8: ANALYSIS FUNCTIONS\n# ============================================================================\n\ndef analyze_classification_results(results: List[Dict], available_topics: List[str]) -> Dict:\n    \"\"\"Analyze classification results and generate statistics.\"\"\"\n    from collections import Counter\n    \n    topic_counts = Counter()\n    #confidence_dist = Counter()\n    \n    multi_label_count = 0\n    no_label_count = 0\n    \n    for result in results:\n        predicted = result['predicted_topics']\n        \n        if len(predicted) == 0:\n            no_label_count += 1\n        elif len(predicted) > 1:\n            multi_label_count += 1\n        \n        for topic in predicted:\n            topic_counts[topic] += 1\n        \n        #confidence_dist[result['confidence']] += 1\n    \n    stats = {\n        'total_articles': len(results),\n        'articles_with_topics': len(results) - no_label_count,\n        'articles_without_topics': no_label_count,\n        'multi_label_articles': multi_label_count,\n        #'avg_topics_per_article': sum(len(r['predicted_topics']) for r in results) / len(results),\n        #'most_common_topics': topic_counts.most_common(10)\n    }\n    \n    print(f\"\\n{'='*80}\")\n    print(\"CLASSIFICATION RESULTS SUMMARY\")\n    print(f\"{'='*80}\")\n    print(f\"\\nTotal articles: {stats['total_articles']}\")\n    print(f\"Articles with topics: {stats['articles_with_topics']} ({stats['articles_with_topics']/stats['total_articles']*100:.1f}%)\")\n    print(f\"Multi-label articles: {stats['multi_label_articles']} ({stats['multi_label_articles']/stats['total_articles']*100:.1f}%)\")\n    #print(f\"Average topics per article: {stats['avg_topics_per_article']:.2f}\")\n    \n    print(f\"\\n{'='*80}\")\n    print(\"TOP 10 MOST FREQUENT TOPICS\")\n    print(f\"{'='*80}\")\n    #for topic, count in stats['most_common_topics']:\n        #print(f\"  {topic}: {count} articles ({count/stats['total_articles']*100:.1f}%)\")\n    \n    return stats\n\ndef save_results(results: List[Dict], output_path: str):\n    \"\"\"Save classification results to JSON file.\"\"\"\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(results, f, indent=2, ensure_ascii=False)\n    print(f\"\\n✓ Results saved to {output_path}\")\n\ndef export_to_csv(results: List[Dict], output_path: str):\n    \"\"\"Export results to CSV for easy analysis.\"\"\"\n    rows = []\n    for r in results:\n        rows.append({\n            'article_id': r['article_id'],\n            'article_preview': r['article_text'],\n            'true_concepts': '|'.join(r['true_concepts']) if r['true_concepts'] else '',\n            'predicted_topics': '|'.join(r['predicted_topics']),\n            'num_predicted': r['num_predicted'],\n            #'confidence': r['confidence'],\n            #'reasoning': r['reasoning']\n        })\n    \n    df = pd.DataFrame(rows)\n    df.to_csv(output_path, index=False, encoding='utf-8')\n    print(f\"✓ CSV exported to {output_path}\")\n\nprint(\"✓ Analysis functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:18:19.412163Z","iopub.execute_input":"2025-11-06T01:18:19.412423Z","iopub.status.idle":"2025-11-06T01:18:19.422004Z","shell.execute_reply.started":"2025-11-06T01:18:19.412407Z","shell.execute_reply":"2025-11-06T01:18:19.421423Z"}},"outputs":[{"name":"stdout","text":"✓ Analysis functions defined\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ============================================================================\n# RUN CLASSIFICATION ON ALL THE ARTICLES\n# ============================================================================\n\n# Initialize classifier\nclassifier = ArticleTopicClassifier(all_concepts)\n\n# Classify all the articles with KGs\nresults = classifier.classify_dataset(\n    articles=articles,\n    knowledge_graphs=knowledge_graphs,\n    max_articles=10000\n)\n\n# Analyze results\nstats = analyze_classification_results(results, all_concepts)\n\n# Save results\nsave_results(results, '/kaggle/working/topic_classification_results.json')\nexport_to_csv(results, '/kaggle/working/topic_classification_results.csv')\n\n# Show sample results\nprint(f\"\\n{'='*80}\")\nprint(\"SAMPLE RESULTS (First 3)\")\nprint(f\"{'='*80}\\n\")\n\nfor result in results[:3]:\n    print(f\"Article {result['article_id']}:\")\n    print(f\"  Preview: {result['article_text']}\")\n    print(f\"  True: {result['true_concepts']}\")\n    print(f\"  Predicted: {result['predicted_topics']}\")\n    #print(f\"  Confidence: {result['confidence']}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:18:23.263922Z","iopub.execute_input":"2025-11-06T01:18:23.264162Z","iopub.status.idle":"2025-11-06T01:46:45.824690Z","shell.execute_reply.started":"2025-11-06T01:18:23.264147Z","shell.execute_reply":"2025-11-06T01:46:45.823836Z"}},"outputs":[{"name":"stdout","text":"✓ Classifier initialized with 24 possible topics\n\n================================================================================\nCLASSIFYING 587 ARTICLES\n================================================================================\n\nProcessing article 0... ✓ Found 2 topics\nProcessing article 1... ✓ Found 1 topics\nProcessing article 2... ✓ Found 2 topics\nProcessing article 3... ✓ Found 1 topics\nProcessing article 4... ✓ Found 2 topics\nProcessing article 5... ✓ Found 3 topics\nProcessing article 6... ✓ Found 2 topics\nProcessing article 7... ✓ Found 2 topics\nProcessing article 8... ✓ Found 2 topics\nProcessing article 9... ✓ Found 0 topics\nProcessing article 10... ✓ Found 3 topics\nProcessing article 11... ✓ Found 2 topics\nProcessing article 12... ✓ Found 6 topics\nProcessing article 13... ✓ Found 2 topics\nProcessing article 14... ✓ Found 4 topics\nProcessing article 15... ✓ Found 3 topics\nProcessing article 16... ✓ Found 2 topics\nProcessing article 17... ✓ Found 4 topics\nProcessing article 18... ✓ Found 1 topics\nProcessing article 19... ✓ Found 1 topics\nProcessing article 20... ✓ Found 2 topics\nProcessing article 21... ✓ Found 1 topics\nProcessing article 22... ✓ Found 3 topics\nProcessing article 23... ✓ Found 2 topics\nProcessing article 24... ✓ Found 4 topics\nProcessing article 25... ✓ Found 2 topics\nProcessing article 26... ✓ Found 4 topics\nProcessing article 27... ✓ Found 1 topics\nProcessing article 28... ✓ Found 2 topics\nProcessing article 29... ✓ Found 9 topics\nProcessing article 30... ✓ Found 2 topics\nProcessing article 31... ✓ Found 0 topics\nProcessing article 32... ✓ Found 2 topics\nProcessing article 33... ✓ Found 1 topics\nProcessing article 34... ✓ Found 2 topics\nProcessing article 35... ✓ Found 1 topics\nProcessing article 36... ✓ Found 18 topics\nProcessing article 37... ✓ Found 1 topics\nProcessing article 38... ✓ Found 1 topics\nProcessing article 39... ✓ Found 3 topics\nProcessing article 40... ✓ Found 1 topics\nProcessing article 41... ✓ Found 0 topics\nProcessing article 42... ✓ Found 0 topics\nProcessing article 43... ✓ Found 2 topics\nProcessing article 44... ✓ Found 1 topics\nProcessing article 45... ✓ Found 1 topics\nProcessing article 46... ✓ Found 9 topics\nProcessing article 47... ✓ Found 0 topics\nProcessing article 48... ✓ Found 3 topics\nProcessing article 49... ✓ Found 0 topics\nProcessing article 50... ✓ Found 1 topics\nProcessing article 51... ✓ Found 2 topics\nProcessing article 52... ✓ Found 2 topics\nProcessing article 53... ✓ Found 2 topics\nProcessing article 54... ✓ Found 4 topics\nProcessing article 55... ✓ Found 0 topics\nProcessing article 56... ✓ Found 5 topics\nProcessing article 57... ✓ Found 3 topics\nProcessing article 58... ✓ Found 1 topics\nProcessing article 59... ✓ Found 0 topics\nProcessing article 60... ✓ Found 8 topics\nProcessing article 61... ✓ Found 1 topics\nProcessing article 62... ✓ Found 5 topics\nProcessing article 63... ✓ Found 3 topics\nProcessing article 64... ✓ Found 6 topics\nProcessing article 65... ✓ Found 4 topics\nProcessing article 66... ✓ Found 1 topics\nProcessing article 67... ✓ Found 1 topics\nProcessing article 68... ✓ Found 4 topics\nProcessing article 69... ✓ Found 13 topics\nProcessing article 70... ✓ Found 2 topics\nProcessing article 71... ✓ Found 2 topics\nProcessing article 72... ✓ Found 2 topics\nProcessing article 73... ✓ Found 1 topics\nProcessing article 74... ✓ Found 2 topics\nProcessing article 75... ✓ Found 0 topics\nProcessing article 76... ✓ Found 1 topics\nProcessing article 77... ✓ Found 2 topics\nProcessing article 78... ✓ Found 2 topics\nProcessing article 79... ✓ Found 2 topics\nProcessing article 80... ✓ Found 3 topics\nProcessing article 81... ✓ Found 2 topics\nProcessing article 82... ✓ Found 1 topics\nProcessing article 83... ✓ Found 6 topics\nProcessing article 84... ✓ Found 1 topics\nProcessing article 85... ✓ Found 2 topics\nProcessing article 86... ✓ Found 2 topics\nProcessing article 87... ✓ Found 1 topics\nProcessing article 88... ✓ Found 2 topics\nProcessing article 89... ✓ Found 1 topics\nProcessing article 90... ✓ Found 2 topics\nProcessing article 91... ✓ Found 8 topics\nProcessing article 92... ✓ Found 6 topics\nProcessing article 93... ✓ Found 3 topics\nProcessing article 94... ✓ Found 7 topics\nProcessing article 95... ✓ Found 2 topics\nProcessing article 96... ✓ Found 0 topics\nProcessing article 97... ✓ Found 10 topics\nProcessing article 98... ✓ Found 1 topics\nProcessing article 99... ✓ Found 1 topics\nProcessing article 100... ✓ Found 6 topics\nProcessing article 101... ✓ Found 2 topics\nProcessing article 102... ✓ Found 1 topics\nProcessing article 103... ✓ Found 6 topics\nProcessing article 104... ✓ Found 10 topics\nProcessing article 105... ✓ Found 2 topics\nProcessing article 106... ✓ Found 1 topics\nProcessing article 107... ✓ Found 1 topics\nProcessing article 108... ✓ Found 1 topics\nProcessing article 109... ✓ Found 4 topics\nProcessing article 110... ✓ Found 5 topics\nProcessing article 111... ✓ Found 1 topics\nProcessing article 112... ✓ Found 2 topics\nProcessing article 113... ✓ Found 6 topics\nProcessing article 114... ✓ Found 1 topics\nProcessing article 115... ✓ Found 1 topics\nProcessing article 116... ✓ Found 3 topics\nProcessing article 117... ✓ Found 2 topics\nProcessing article 118... ✓ Found 8 topics\nProcessing article 119... ✓ Found 2 topics\nProcessing article 120... ✓ Found 3 topics\nProcessing article 121... ✓ Found 3 topics\nProcessing article 122... ✓ Found 2 topics\nProcessing article 123... ✓ Found 6 topics\nProcessing article 124... ✓ Found 16 topics\nProcessing article 125... ✓ Found 3 topics\nProcessing article 126... ✓ Found 1 topics\nProcessing article 127... ✓ Found 1 topics\nProcessing article 128... ✓ Found 1 topics\nProcessing article 129... ✓ Found 8 topics\nProcessing article 130... ✓ Found 1 topics\nProcessing article 131... ✓ Found 2 topics\nProcessing article 132... ✓ Found 14 topics\nProcessing article 133... ✓ Found 2 topics\nProcessing article 134... ✓ Found 2 topics\nProcessing article 135... ✓ Found 1 topics\nProcessing article 136... ✓ Found 4 topics\nProcessing article 137... ✓ Found 1 topics\nProcessing article 138... ✓ Found 2 topics\nProcessing article 139... ✓ Found 1 topics\nProcessing article 140... ✓ Found 3 topics\nProcessing article 141... ✓ Found 2 topics\nProcessing article 142... ✓ Found 2 topics\nProcessing article 143... ✓ Found 3 topics\nProcessing article 144... ✓ Found 1 topics\nProcessing article 145... ✓ Found 1 topics\nProcessing article 146... ✓ Found 4 topics\nProcessing article 147... ✓ Found 9 topics\nProcessing article 148... ✓ Found 16 topics\nProcessing article 149... ✓ Found 1 topics\nProcessing article 150... ✓ Found 1 topics\nProcessing article 151... ✓ Found 1 topics\nProcessing article 152... ✓ Found 2 topics\nProcessing article 153... ✓ Found 1 topics\nProcessing article 154... ✓ Found 1 topics\nProcessing article 155... ✓ Found 7 topics\nProcessing article 156... ✓ Found 2 topics\nProcessing article 157... ✓ Found 2 topics\nProcessing article 158... ✓ Found 4 topics\nProcessing article 159... ✓ Found 4 topics\nProcessing article 160... ✓ Found 2 topics\nProcessing article 161... ✓ Found 2 topics\nProcessing article 162... ✓ Found 2 topics\nProcessing article 163... ✓ Found 4 topics\nProcessing article 164... ✓ Found 2 topics\nProcessing article 165... ✓ Found 3 topics\nProcessing article 166... ✓ Found 1 topics\nProcessing article 167... ✓ Found 1 topics\nProcessing article 168... ✓ Found 8 topics\nProcessing article 169... ✓ Found 13 topics\nProcessing article 170... ✓ Found 14 topics\nProcessing article 171... ✓ Found 1 topics\nProcessing article 172... ✓ Found 2 topics\nProcessing article 173... ✓ Found 5 topics\nProcessing article 174... ✓ Found 2 topics\nProcessing article 175... ✓ Found 20 topics\nProcessing article 176... ✓ Found 2 topics\nProcessing article 177... ✓ Found 1 topics\nProcessing article 178... ✓ Found 1 topics\nProcessing article 179... ✓ Found 8 topics\nProcessing article 180... ✓ Found 3 topics\nProcessing article 181... ✓ Found 15 topics\nProcessing article 182... ✓ Found 2 topics\nProcessing article 183... ✓ Found 1 topics\nProcessing article 184... ✓ Found 0 topics\nProcessing article 185... ✓ Found 1 topics\nProcessing article 186... ✓ Found 11 topics\nProcessing article 187... ✓ Found 3 topics\nProcessing article 188... ✓ Found 1 topics\nProcessing article 189... ✓ Found 3 topics\nProcessing article 190... ✓ Found 2 topics\nProcessing article 191... ✓ Found 4 topics\nProcessing article 192... ✓ Found 3 topics\nProcessing article 193... ✓ Found 4 topics\nProcessing article 194... ✓ Found 0 topics\nProcessing article 195... ✓ Found 1 topics\nProcessing article 196... ✓ Found 4 topics\nProcessing article 197... ✓ Found 3 topics\nProcessing article 198... ✓ Found 5 topics\nProcessing article 199... ✓ Found 11 topics\nProcessing article 200... ✓ Found 1 topics\nProcessing article 201... ✓ Found 3 topics\nProcessing article 202... ✓ Found 2 topics\nProcessing article 203... ✓ Found 1 topics\nProcessing article 204... ✓ Found 4 topics\nProcessing article 205... ✓ Found 1 topics\nProcessing article 206... ✓ Found 14 topics\nProcessing article 207... ✓ Found 1 topics\nProcessing article 208... ✓ Found 5 topics\nProcessing article 209... ✓ Found 5 topics\nProcessing article 210... ✓ Found 3 topics\nProcessing article 211... ✓ Found 1 topics\nProcessing article 212... ✓ Found 3 topics\nProcessing article 213... ✓ Found 2 topics\nProcessing article 214... ✓ Found 1 topics\nProcessing article 215... ✓ Found 20 topics\nProcessing article 216... ✓ Found 11 topics\nProcessing article 217... ✓ Found 6 topics\nProcessing article 218... ✓ Found 1 topics\nProcessing article 219... ✓ Found 9 topics\nProcessing article 220... ✓ Found 8 topics\nProcessing article 221... ✓ Found 5 topics\nProcessing article 222... ✓ Found 1 topics\nProcessing article 223... ✓ Found 6 topics\nProcessing article 224... ✓ Found 2 topics\nProcessing article 225... ✓ Found 0 topics\nProcessing article 226... ✓ Found 5 topics\nProcessing article 227... ✓ Found 1 topics\nProcessing article 228... ✓ Found 1 topics\nProcessing article 229... ✓ Found 5 topics\nProcessing article 230... ✓ Found 1 topics\nProcessing article 231... ✓ Found 9 topics\nProcessing article 232... ✓ Found 6 topics\nProcessing article 233... ✓ Found 2 topics\nProcessing article 234... ✓ Found 2 topics\nProcessing article 235... ✓ Found 1 topics\nProcessing article 236... ✓ Found 1 topics\nProcessing article 237... ✓ Found 3 topics\nProcessing article 238... ✓ Found 1 topics\nProcessing article 239... ✓ Found 1 topics\nProcessing article 240... ✓ Found 3 topics\nProcessing article 241... ✓ Found 13 topics\nProcessing article 242... ✓ Found 4 topics\nProcessing article 243... ✓ Found 5 topics\nProcessing article 244... ✓ Found 1 topics\nProcessing article 245... ✓ Found 3 topics\nProcessing article 246... ✓ Found 1 topics\nProcessing article 247... ✓ Found 2 topics\nProcessing article 248... ✓ Found 3 topics\nProcessing article 249... ✓ Found 1 topics\nProcessing article 250... ✓ Found 0 topics\nProcessing article 251... ✓ Found 1 topics\nProcessing article 252... ✓ Found 19 topics\nProcessing article 253... ✓ Found 2 topics\nProcessing article 254... ✓ Found 7 topics\nProcessing article 255... ✓ Found 14 topics\nProcessing article 256... ✓ Found 2 topics\nProcessing article 257... ✓ Found 0 topics\nProcessing article 258... ✓ Found 0 topics\nProcessing article 259... ✓ Found 5 topics\nProcessing article 260... ✓ Found 1 topics\nProcessing article 261... ✓ Found 4 topics\nProcessing article 262... ✓ Found 1 topics\nProcessing article 263... ✓ Found 1 topics\nProcessing article 264... ✓ Found 20 topics\nProcessing article 265... ✓ Found 0 topics\nProcessing article 266... ✓ Found 6 topics\nProcessing article 267... ✓ Found 1 topics\nProcessing article 268... ✓ Found 17 topics\nProcessing article 269... ✓ Found 1 topics\nProcessing article 270... ✓ Found 1 topics\nProcessing article 271... ✓ Found 5 topics\nProcessing article 272... ✓ Found 1 topics\nProcessing article 273... ✓ Found 2 topics\nProcessing article 274... ✓ Found 1 topics\nProcessing article 275... ✓ Found 1 topics\nProcessing article 276... ✓ Found 2 topics\nProcessing article 277... ✓ Found 2 topics\nProcessing article 278... ✓ Found 3 topics\nProcessing article 279... ✓ Found 2 topics\nProcessing article 280... ✓ Found 2 topics\nProcessing article 281... ✓ Found 1 topics\nProcessing article 282... ✓ Found 3 topics\nProcessing article 283... ✓ Found 10 topics\nProcessing article 284... ✓ Found 2 topics\nProcessing article 285... ✓ Found 2 topics\nProcessing article 286... ✓ Found 6 topics\nProcessing article 287... ✓ Found 1 topics\nProcessing article 288... ✓ Found 18 topics\nProcessing article 289... ✓ Found 2 topics\nProcessing article 290... ✓ Found 1 topics\nProcessing article 291... ✓ Found 1 topics\nProcessing article 292... ✓ Found 2 topics\nProcessing article 293... ✓ Found 23 topics\nProcessing article 294... ✓ Found 5 topics\nProcessing article 295... ✓ Found 2 topics\nProcessing article 296... ✓ Found 1 topics\nProcessing article 297... ✓ Found 1 topics\nProcessing article 298... ✓ Found 3 topics\nProcessing article 299... ✓ Found 1 topics\nProcessing article 300... ✓ Found 4 topics\nProcessing article 301... ✓ Found 5 topics\nProcessing article 302... ✓ Found 3 topics\nProcessing article 303... ✓ Found 2 topics\nProcessing article 304... ✓ Found 3 topics\nProcessing article 305... ✓ Found 0 topics\nProcessing article 306... ✓ Found 1 topics\nProcessing article 307... ✓ Found 2 topics\nProcessing article 308... ✓ Found 10 topics\nProcessing article 309... ✓ Found 7 topics\nProcessing article 310... ✓ Found 2 topics\nProcessing article 311... ✓ Found 1 topics\nProcessing article 312... ✓ Found 8 topics\nProcessing article 313... ✓ Found 1 topics\nProcessing article 314... ✓ Found 8 topics\nProcessing article 315... ✓ Found 1 topics\nProcessing article 316... ✓ Found 3 topics\nProcessing article 317... ✓ Found 3 topics\nProcessing article 318... ✓ Found 3 topics\nProcessing article 319... ✓ Found 1 topics\nProcessing article 320... ✓ Found 3 topics\nProcessing article 321... ✓ Found 2 topics\nProcessing article 322... ✓ Found 2 topics\nProcessing article 323... ✓ Found 1 topics\nProcessing article 324... ✓ Found 2 topics\nProcessing article 325... ✓ Found 19 topics\nProcessing article 326... ✓ Found 3 topics\nProcessing article 327... ✓ Found 2 topics\nProcessing article 328... ✓ Found 1 topics\nProcessing article 329... ✓ Found 1 topics\nProcessing article 330... ✓ Found 1 topics\nProcessing article 331... ✓ Found 3 topics\nProcessing article 332... ✓ Found 2 topics\nProcessing article 333... ✓ Found 1 topics\nProcessing article 334... ✓ Found 0 topics\nProcessing article 335... ✓ Found 3 topics\nProcessing article 336... ✓ Found 2 topics\nProcessing article 337... ✓ Found 2 topics\nProcessing article 338... ✓ Found 1 topics\nProcessing article 339... ✓ Found 0 topics\nProcessing article 340... ✓ Found 2 topics\nProcessing article 341... ✓ Found 17 topics\nProcessing article 342... ✓ Found 4 topics\nProcessing article 343... ✓ Found 1 topics\nProcessing article 344... ✓ Found 2 topics\nProcessing article 345... ✓ Found 3 topics\nProcessing article 346... ✓ Found 0 topics\nProcessing article 347... ✓ Found 1 topics\nProcessing article 348... ✓ Found 0 topics\nProcessing article 349... ✓ Found 0 topics\nProcessing article 350... ✓ Found 0 topics\nProcessing article 351... ✓ Found 1 topics\nProcessing article 352... ✓ Found 1 topics\nProcessing article 353... ✓ Found 2 topics\nProcessing article 354... ✓ Found 2 topics\nProcessing article 355... ✓ Found 1 topics\nProcessing article 356... ✓ Found 1 topics\nProcessing article 357... ✓ Found 3 topics\nProcessing article 358... ✓ Found 2 topics\nProcessing article 359... ✓ Found 3 topics\nProcessing article 360... ✓ Found 1 topics\nProcessing article 361... ✓ Found 2 topics\nProcessing article 362... ✓ Found 2 topics\nProcessing article 363... ✓ Found 1 topics\nProcessing article 364... ✓ Found 2 topics\nProcessing article 365... ✓ Found 8 topics\nProcessing article 366... ✓ Found 3 topics\nProcessing article 367... ✓ Found 1 topics\nProcessing article 368... ✓ Found 10 topics\nProcessing article 369... ✓ Found 1 topics\nProcessing article 370... ✓ Found 1 topics\nProcessing article 371... ✓ Found 2 topics\nProcessing article 372... ✓ Found 1 topics\nProcessing article 373... ✓ Found 9 topics\nProcessing article 374... ✓ Found 2 topics\nProcessing article 375... ✓ Found 7 topics\nProcessing article 376... ✓ Found 2 topics\nProcessing article 377... ✓ Found 1 topics\nProcessing article 378... ✓ Found 3 topics\nProcessing article 379... ✓ Found 1 topics\nProcessing article 380... ✓ Found 4 topics\nProcessing article 381... ✓ Found 2 topics\nProcessing article 382... ✓ Found 1 topics\nProcessing article 383... ✓ Found 0 topics\nProcessing article 384... ✓ Found 2 topics\nProcessing article 385... ✓ Found 5 topics\nProcessing article 386... ✓ Found 1 topics\nProcessing article 387... ✓ Found 1 topics\nProcessing article 388... ✓ Found 2 topics\nProcessing article 389... ✓ Found 2 topics\nProcessing article 390... ✓ Found 2 topics\nProcessing article 391... ✓ Found 2 topics\nProcessing article 392... ✓ Found 1 topics\nProcessing article 393... ✓ Found 1 topics\nProcessing article 394... ✓ Found 2 topics\nProcessing article 395... ✓ Found 4 topics\nProcessing article 396... ✓ Found 1 topics\nProcessing article 397... ✓ Found 3 topics\nProcessing article 398... ✓ Found 1 topics\nProcessing article 399... ✓ Found 1 topics\nProcessing article 400... ✓ Found 2 topics\nProcessing article 401... ✓ Found 2 topics\nProcessing article 402... ✓ Found 1 topics\nProcessing article 403... ✓ Found 1 topics\nProcessing article 404... ✓ Found 1 topics\nProcessing article 405... ✓ Found 2 topics\nProcessing article 406... ✓ Found 4 topics\nProcessing article 407... ✓ Found 2 topics\nProcessing article 408... ✓ Found 0 topics\nProcessing article 409... ✓ Found 1 topics\nProcessing article 410... ✓ Found 3 topics\nProcessing article 411... ✓ Found 5 topics\nProcessing article 412... ✓ Found 9 topics\nProcessing article 413... ✓ Found 2 topics\nProcessing article 414... ✓ Found 3 topics\nProcessing article 415... ✓ Found 1 topics\nProcessing article 416... ✓ Found 7 topics\nProcessing article 417... ✓ Found 0 topics\nProcessing article 418... ✓ Found 3 topics\nProcessing article 419... ✓ Found 1 topics\nProcessing article 420... ✓ Found 1 topics\nProcessing article 421... ✓ Found 1 topics\nProcessing article 422... ✓ Found 5 topics\nProcessing article 423... ✓ Found 5 topics\nProcessing article 424... ✓ Found 2 topics\nProcessing article 425... ✓ Found 23 topics\nProcessing article 426... ✓ Found 3 topics\nProcessing article 427... ✓ Found 10 topics\nProcessing article 428... ✓ Found 23 topics\nProcessing article 429... ✓ Found 0 topics\nProcessing article 430... ✓ Found 2 topics\nProcessing article 431... ✓ Found 2 topics\nProcessing article 432... ✓ Found 4 topics\nProcessing article 433... ✓ Found 2 topics\nProcessing article 434... ✓ Found 4 topics\nProcessing article 435... ✓ Found 1 topics\nProcessing article 436... ✓ Found 2 topics\nProcessing article 437... ✓ Found 2 topics\nProcessing article 438... ✓ Found 1 topics\nProcessing article 439... ✓ Found 3 topics\nProcessing article 440... ✓ Found 1 topics\nProcessing article 441... ✓ Found 1 topics\nProcessing article 442... ✓ Found 2 topics\nProcessing article 443... ✓ Found 1 topics\nProcessing article 444... ✓ Found 1 topics\nProcessing article 445... ✓ Found 1 topics\nProcessing article 446... ✓ Found 1 topics\nProcessing article 447... ✓ Found 2 topics\nProcessing article 448... ✓ Found 8 topics\nProcessing article 449... ✓ Found 2 topics\nProcessing article 450... ✓ Found 4 topics\nProcessing article 451... ✓ Found 2 topics\nProcessing article 452... ✓ Found 7 topics\nProcessing article 453... ✓ Found 2 topics\nProcessing article 454... ✓ Found 4 topics\nProcessing article 455... ✓ Found 3 topics\nProcessing article 456... ✓ Found 2 topics\nProcessing article 457... ✓ Found 5 topics\nProcessing article 458... ✓ Found 1 topics\nProcessing article 459... ✓ Found 1 topics\nProcessing article 460... ✓ Found 16 topics\nProcessing article 461... ✓ Found 2 topics\nProcessing article 462... ✓ Found 4 topics\nProcessing article 463... ✓ Found 1 topics\nProcessing article 464... ✓ Found 5 topics\nProcessing article 465... ✓ Found 1 topics\nProcessing article 466... ✓ Found 1 topics\nProcessing article 467... ✓ Found 9 topics\nProcessing article 468... ✓ Found 4 topics\nProcessing article 469... ✓ Found 2 topics\nProcessing article 470... ✓ Found 5 topics\nProcessing article 471... ✓ Found 5 topics\nProcessing article 472... ✓ Found 1 topics\nProcessing article 473... ✓ Found 2 topics\nProcessing article 474... ✓ Found 4 topics\nProcessing article 475... ✓ Found 3 topics\nProcessing article 476... ✓ Found 2 topics\nProcessing article 477... ✓ Found 2 topics\nProcessing article 478... ✓ Found 4 topics\nProcessing article 479... ✓ Found 5 topics\nProcessing article 480... ✓ Found 1 topics\nProcessing article 481... ✓ Found 1 topics\nProcessing article 482... ✓ Found 3 topics\nProcessing article 483... ✓ Found 1 topics\nProcessing article 484... ✓ Found 3 topics\nProcessing article 485... ✓ Found 4 topics\nProcessing article 486... ✓ Found 2 topics\nProcessing article 487... ✓ Found 4 topics\nProcessing article 488... ✓ Found 2 topics\nProcessing article 489... ✓ Found 4 topics\nProcessing article 490... ✓ Found 3 topics\nProcessing article 491... ✓ Found 4 topics\nProcessing article 492... ✓ Found 1 topics\nProcessing article 493... ✓ Found 4 topics\nProcessing article 494... ✓ Found 0 topics\nProcessing article 495... ✓ Found 1 topics\nProcessing article 496... ✓ Found 2 topics\nProcessing article 497... ✓ Found 3 topics\nProcessing article 498... ✓ Found 11 topics\nProcessing article 499... ✓ Found 1 topics\nProcessing article 500... ✓ Found 0 topics\nProcessing article 501... ✓ Found 1 topics\nProcessing article 502... ✓ Found 1 topics\nProcessing article 503... ✓ Found 2 topics\nProcessing article 504... ✓ Found 2 topics\nProcessing article 505... ✓ Found 3 topics\nProcessing article 506... ✓ Found 2 topics\nProcessing article 507... ✓ Found 2 topics\nProcessing article 508... ✓ Found 2 topics\nProcessing article 509... ✓ Found 2 topics\nProcessing article 510... ✓ Found 1 topics\nProcessing article 511... ✓ Found 2 topics\nProcessing article 512... ✓ Found 2 topics\nProcessing article 513... ✓ Found 1 topics\nProcessing article 514... ✓ Found 3 topics\nProcessing article 515... ✓ Found 2 topics\nProcessing article 516... ✓ Found 1 topics\nProcessing article 517... ✓ Found 1 topics\nProcessing article 518... ✓ Found 4 topics\nProcessing article 519... ✓ Found 1 topics\nProcessing article 520... ✓ Found 1 topics\nProcessing article 521... ✓ Found 3 topics\nProcessing article 522... ✓ Found 2 topics\nProcessing article 523... ✓ Found 2 topics\nProcessing article 524... ✓ Found 2 topics\nProcessing article 525... ✓ Found 1 topics\nProcessing article 526... ✓ Found 1 topics\nProcessing article 527... ✓ Found 1 topics\nProcessing article 528... ✓ Found 4 topics\nProcessing article 529... ✓ Found 6 topics\nProcessing article 530... ✓ Found 3 topics\nProcessing article 531... ✓ Found 2 topics\nProcessing article 532... ✓ Found 5 topics\nProcessing article 533... ✓ Found 5 topics\nProcessing article 534... ✓ Found 2 topics\nProcessing article 535... ✓ Found 2 topics\nProcessing article 536... ✓ Found 1 topics\nProcessing article 537... ✓ Found 1 topics\nProcessing article 538... ✓ Found 2 topics\nProcessing article 539... ✓ Found 3 topics\nProcessing article 540... ✓ Found 1 topics\nProcessing article 541... ✓ Found 4 topics\nProcessing article 542... ✓ Found 5 topics\nProcessing article 543... ✓ Found 2 topics\nProcessing article 544... ✓ Found 2 topics\nProcessing article 545... ✓ Found 2 topics\nProcessing article 546... ✓ Found 3 topics\nProcessing article 547... ✓ Found 5 topics\nProcessing article 548... ✓ Found 1 topics\nProcessing article 549... ✓ Found 2 topics\nProcessing article 550... ✓ Found 1 topics\nProcessing article 551... ✓ Found 7 topics\nProcessing article 552... ✓ Found 2 topics\nProcessing article 553... ✓ Found 1 topics\nProcessing article 554... ✓ Found 2 topics\nProcessing article 555... ✓ Found 3 topics\nProcessing article 556... ✓ Found 3 topics\nProcessing article 557... ✓ Found 3 topics\nProcessing article 558... ✓ Found 1 topics\nProcessing article 559... ✓ Found 2 topics\nProcessing article 560... ✓ Found 1 topics\nProcessing article 561... ✓ Found 2 topics\nProcessing article 562... ✓ Found 5 topics\nProcessing article 563... ✓ Found 3 topics\nProcessing article 564... ✓ Found 1 topics\nProcessing article 565... ✓ Found 5 topics\nProcessing article 566... ✓ Found 4 topics\nProcessing article 567... ✓ Found 2 topics\nProcessing article 568... ✓ Found 8 topics\nProcessing article 569... ✓ Found 1 topics\nProcessing article 570... ✓ Found 1 topics\nProcessing article 571... ✓ Found 2 topics\nProcessing article 572... ✓ Found 1 topics\nProcessing article 573... ✓ Found 1 topics\nProcessing article 574... ✓ Found 2 topics\nProcessing article 575... ✓ Found 2 topics\nProcessing article 576... ✓ Found 1 topics\nProcessing article 577... ✓ Found 1 topics\nProcessing article 578... ✓ Found 1 topics\nProcessing article 579... ✓ Found 7 topics\nProcessing article 580... ✓ Found 8 topics\nProcessing article 581... ✓ Found 1 topics\nProcessing article 582... ✓ Found 1 topics\nProcessing article 583... ✓ Found 1 topics\nProcessing article 584... ✓ Found 2 topics\nProcessing article 585... ✓ Found 20 topics\nProcessing article 586... ✓ Found 2 topics\n\n================================================================================\nCLASSIFICATION RESULTS SUMMARY\n================================================================================\n\nTotal articles: 587\nArticles with topics: 557 (94.9%)\nMulti-label articles: 375 (63.9%)\n\n================================================================================\nTOP 10 MOST FREQUENT TOPICS\n================================================================================\n\n✓ Results saved to /kaggle/working/topic_classification_results.json\n✓ CSV exported to /kaggle/working/topic_classification_results.csv\n\n================================================================================\nSAMPLE RESULTS (First 3)\n================================================================================\n\nArticle 0:\n  Preview: excellent phone , excellent service . \n...\n  True: []\n  Predicted: ['phone', 'service']\n\nArticle 1:\n  Preview: i am a business user who heavily depend on mobile service . \n...\n  True: ['service']\n  Predicted: ['service']\n\nArticle 2:\n  Preview: there is much which has been said in other reviews about the features of this phone , it is a great phone , mine worked without any problems right out of the box . \n...\n  True: ['phone']\n  Predicted: ['feature', 'phone']\n\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION: Compare Predicted vs True Concepts\n# ============================================================================\n\ndef evaluate_classification(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate accuracy metrics by comparing predicted topics with true concepts.\n    \"\"\"\n    total = len(results)\n    exact_match = 0  # All predicted topics match exactly with true concepts\n    partial_match = 0  # At least one topic matches\n    no_match = 0  # No topics match\n    no_true_labels = 0  # Articles with no true concepts\n    \n    all_precisions = []\n    all_recalls = []\n    all_f1s = []\n    \n    correct_predictions = []\n    incorrect_predictions = []\n    \n    for result in results:\n        true_set = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_set = set(result['predicted_topics'])\n        \n        # Skip articles with no true labels\n        if len(true_set) == 0:\n            no_true_labels += 1\n            continue\n        \n        # Find intersection\n        intersection = true_set & pred_set\n        \n        # Exact match\n        if true_set == pred_set and len(true_set) > 0:\n            exact_match += 1\n            correct_predictions.append(result)\n        elif len(intersection) > 0:\n            partial_match += 1\n        else:\n            no_match += 1\n            incorrect_predictions.append(result)\n        \n        # Calculate precision, recall, F1\n        if len(pred_set) > 0:\n            precision = len(intersection) / len(pred_set)\n            all_precisions.append(precision)\n        else:\n            all_precisions.append(0)\n        \n        if len(true_set) > 0:\n            recall = len(intersection) / len(true_set)\n            all_recalls.append(recall)\n        else:\n            all_recalls.append(0)\n        \n        if all_precisions[-1] + all_recalls[-1] > 0:\n            f1 = 2 * (all_precisions[-1] * all_recalls[-1]) / (all_precisions[-1] + all_recalls[-1])\n            all_f1s.append(f1)\n        else:\n            all_f1s.append(0)\n    \n    articles_with_true_labels = total - no_true_labels\n    \n    metrics = {\n        'total_articles': total,\n        'articles_with_true_labels': articles_with_true_labels,\n        'articles_without_true_labels': no_true_labels,\n        'exact_matches': exact_match,\n        'partial_matches': partial_match,\n        'no_matches': no_match,\n        'exact_match_rate': exact_match / articles_with_true_labels if articles_with_true_labels > 0 else 0,\n        'partial_match_rate': partial_match / articles_with_true_labels if articles_with_true_labels > 0 else 0,\n        'avg_precision': np.mean(all_precisions) if all_precisions else 0,\n        'avg_recall': np.mean(all_recalls) if all_recalls else 0,\n        'avg_f1': np.mean(all_f1s) if all_f1s else 0,\n        'correct_examples': correct_predictions[:3],\n        'incorrect_examples': incorrect_predictions[:3]\n    }\n    \n    # Print results\n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS\")\n    print(f\"{'='*80}\")\n    print(f\"\\nTotal articles evaluated: {articles_with_true_labels}\")\n    print(f\"Articles without labels (skipped): {no_true_labels}\")\n    print(f\"\\n📊 ACCURACY:\")\n    print(f\"  ✓ Exact matches: {exact_match} ({metrics['exact_match_rate']*100:.1f}%)\")\n    print(f\"  ~ Partial matches: {partial_match} ({metrics['partial_match_rate']*100:.1f}%)\")\n    print(f\"  ✗ No matches: {no_match} ({no_match/articles_with_true_labels*100:.1f}%)\")\n    print(f\"\\n📈 PERFORMANCE METRICS:\")\n    print(f\"  Precision: {metrics['avg_precision']:.3f} (how many predicted topics were correct)\")\n    print(f\"  Recall: {metrics['avg_recall']:.3f} (how many true topics were found)\")\n    print(f\"  F1 Score: {metrics['avg_f1']:.3f} (overall accuracy)\")\n    \n    return metrics\n\n\n# Run evaluation\nprint(\"\\nEvaluating classification accuracy...\")\neval_metrics = evaluate_classification(results)\n\n# Show correct examples\nprint(f\"\\n{'='*80}\")\nprint(\"✓ CORRECTLY CLASSIFIED EXAMPLES\")\nprint(f\"{'='*80}\")\nfor ex in eval_metrics['correct_examples']:\n    print(f\"\\nArticle {ex['article_id']}:\")\n    print(f\"  True: {ex['true_concepts']}\")\n    print(f\"  Predicted: {ex['predicted_topics']}\")\n    print(f\"  ✓ EXACT MATCH!\")\n\n# Show incorrect examples\nprint(f\"\\n{'='*80}\")\nprint(\"✗ INCORRECTLY CLASSIFIED EXAMPLES\")\nprint(f\"{'='*80}\")\nfor ex in eval_metrics['incorrect_examples']:\n    print(f\"\\nArticle {ex['article_id']}:\")\n    print(f\"  True: {ex['true_concepts']}\")\n    print(f\"  Predicted: {ex['predicted_topics']}\")\n    #print(f\"  Reasoning: {ex['reasoning'][:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:47:04.831522Z","iopub.execute_input":"2025-11-06T01:47:04.831801Z","iopub.status.idle":"2025-11-06T01:47:04.846347Z","shell.execute_reply.started":"2025-11-06T01:47:04.831784Z","shell.execute_reply":"2025-11-06T01:47:04.845151Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating classification accuracy...\n\n================================================================================\nEVALUATION METRICS\n================================================================================\n\nTotal articles evaluated: 417\nArticles without labels (skipped): 170\n\n📊 ACCURACY:\n  ✓ Exact matches: 152 (36.5%)\n  ~ Partial matches: 219 (52.5%)\n  ✗ No matches: 46 (11.0%)\n\n📈 PERFORMANCE METRICS:\n  Precision: 0.643 (how many predicted topics were correct)\n  Recall: 0.812 (how many true topics were found)\n  F1 Score: 0.664 (overall accuracy)\n\n================================================================================\n✓ CORRECTLY CLASSIFIED EXAMPLES\n================================================================================\n\nArticle 1:\n  True: ['service']\n  Predicted: ['service']\n  ✓ EXACT MATCH!\n\nArticle 3:\n  True: ['service']\n  Predicted: ['service']\n  ✓ EXACT MATCH!\n\nArticle 15:\n  True: ['phone', 'size', 'design']\n  Predicted: ['phone', 'design', 'size']\n  ✓ EXACT MATCH!\n\n================================================================================\n✗ INCORRECTLY CLASSIFIED EXAMPLES\n================================================================================\n\nArticle 9:\n  True: ['phone']\n  Predicted: []\n\nArticle 28:\n  True: ['sound']\n  Predicted: ['size', 'weight']\n\nArticle 31:\n  True: ['sound']\n  Predicted: []\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# ============================================================================\n# EVALUATION METRICS (Following the Paper's Methodology)\n# ============================================================================\n\ndef calculate_metrics_paper_style(results: List[Dict]) -> Dict:\n    \"\"\"\n    Calculate evaluation metrics following the paper's approach:\n    \"For each article, the model inferred topic(s) were compared against \n    the list of 'gold' topic(s) to compute the true positive, false positive, \n    and false negative statistics for that article. Then, all such statistics \n    for all the articles in a dataset were aggregated and used to compute \n    the final Precision, Recall, and micro-averaged F1 score.\"\n    \n    Reference: Section 5.4 of the paper\n    \"\"\"\n    # Aggregate statistics across all articles\n    total_tp = 0  # True Positives\n    total_fp = 0  # False Positives\n    total_fn = 0  # False Negatives\n    \n    articles_evaluated = 0\n    articles_skipped = 0\n    \n    for result in results:\n        # Get true and predicted topics (lowercased and stripped)\n        true_topics = set([c.lower().strip() for c in result['true_concepts']]) if result['true_concepts'] else set()\n        pred_topics = set([t.lower().strip() for t in result['predicted_topics']])\n        \n        # Skip articles with no ground truth labels\n        if len(true_topics) == 0:\n            articles_skipped += 1\n            continue\n        \n        articles_evaluated += 1\n        \n        # Calculate TP, FP, FN for this article\n        tp = len(true_topics & pred_topics)  # Intersection\n        fp = len(pred_topics - true_topics)  # Predicted but not true\n        fn = len(true_topics - pred_topics)  # True but not predicted\n        \n        # Aggregate\n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n    \n    # Calculate micro-averaged metrics\n    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    metrics = {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'total_tp': total_tp,\n        'total_fp': total_fp,\n        'total_fn': total_fn,\n        'articles_evaluated': articles_evaluated,\n        'articles_skipped': articles_skipped\n    }\n    \n    # Print results\n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION METRICS (Paper's Methodology)\")\n    print(f\"{'='*80}\")\n    print(f\"\\nArticles evaluated: {articles_evaluated}\")\n    print(f\"Articles skipped (no ground truth): {articles_skipped}\")\n    print(f\"\\nAggregated Statistics:\")\n    print(f\"  True Positives (TP): {total_tp}\")\n    print(f\"  False Positives (FP): {total_fp}\")\n    print(f\"  False Negatives (FN): {total_fn}\")\n    print(f\"\\n📊 MICRO-AVERAGED METRICS:\")\n    print(f\"  Precision: {precision:.3f}\")\n    print(f\"  Recall: {recall:.3f}\")\n    print(f\"  F1 Score: {f1_score:.3f}\")\n    \n    return metrics\n\n\n# Run evaluation using the paper's methodology\nprint(\"\\nEvaluating using paper's methodology...\")\npaper_metrics = calculate_metrics_paper_style(results)\n\n# Compare with their baselines (from Table 6 in the paper)\nprint(f\"\\n{'='*80}\")\nprint(\"COMPARISON WITH PAPER'S BASELINES\")\nprint(f\"{'='*80}\")\nprint(f\"This model's F1 Score: {paper_metrics['f1_score']:.3f}\")\nprint(f\"\\nPaper's Results:\")\nprint(f\"  GFLM-S baseline: 0.532\")\nprint(f\"  GFLM-W baseline: 0.530\")\nprint(f\"  SBERT (best mid encoder): 0.594\")\nprint(f\"  ChatGPT-3.5 (best overall): 0.606\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T01:49:29.813141Z","iopub.execute_input":"2025-11-06T01:49:29.813424Z","iopub.status.idle":"2025-11-06T01:49:29.825204Z","shell.execute_reply.started":"2025-11-06T01:49:29.813406Z","shell.execute_reply":"2025-11-06T01:49:29.824297Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating using paper's methodology...\n\n================================================================================\nEVALUATION METRICS (Paper's Methodology)\n================================================================================\n\nArticles evaluated: 417\nArticles skipped (no ground truth): 170\n\nAggregated Statistics:\n  True Positives (TP): 489\n  False Positives (FP): 505\n  False Negatives (FN): 132\n\n📊 MICRO-AVERAGED METRICS:\n  Precision: 0.492\n  Recall: 0.787\n  F1 Score: 0.606\n\n================================================================================\nCOMPARISON WITH PAPER'S BASELINES\n================================================================================\nThis model's F1 Score: 0.606\n\nPaper's Results:\n  GFLM-S baseline: 0.532\n  GFLM-W baseline: 0.530\n  SBERT (best mid encoder): 0.594\n  ChatGPT-3.5 (best overall): 0.606\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}